#!/usr/bin/env python3
"""
ğŸš€ Quick Train and Save - ØªØ¯Ø±ÙŠØ¨ Ø³Ø±ÙŠØ¹ ÙˆØ­ÙØ¸ Ù…Ø¨Ø§Ø´Ø±
ğŸ“Š ÙŠØªØ®Ø·Ù‰ ÙƒÙ„ Ø§Ù„ÙØ­ÙˆØµØ§Øª ÙˆÙŠØ­ÙØ¸ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ ÙÙˆØ±Ø§Ù‹
"""

import os
import sys
import sqlite3
import pandas as pd
import numpy as np
import joblib
import logging
import warnings
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import RobustScaler
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier
from sklearn.neural_network import MLPClassifier

warnings.filterwarnings('ignore')

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s'
)
logger = logging.getLogger(__name__)

# Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ù…Ø­Ø³Ù†
sys.path.append(os.path.dirname(os.path.abspath(__file__)))
from enhanced_ml_server import EnhancedMLTradingSystem

def force_save_models():
    """Ø­ÙØ¸ Ù†Ù…Ø§Ø°Ø¬ Ø¨Ø´ÙƒÙ„ Ù…Ø¨Ø§Ø´Ø±"""
    logger.info("="*60)
    logger.info("ğŸš€ Quick Model Training & Saving")
    logger.info("="*60)
    
    # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù†Ø¸Ø§Ù…
    system = EnhancedMLTradingSystem()
    
    # Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªØ§Ø­Ø©
    conn = sqlite3.connect('./data/forex_ml.db')
    
    # Ø¬Ù„Ø¨ Ø§Ù„Ø£Ø²ÙˆØ§Ø¬ Ø§Ù„Ù…ØªØ§Ø­Ø©
    query = """
    SELECT DISTINCT symbol, timeframe, COUNT(*) as count
    FROM price_data
    GROUP BY symbol, timeframe
    HAVING count > 1000
    ORDER BY count DESC
    LIMIT 20
    """
    
    available = pd.read_sql_query(query, conn)
    logger.info(f"\nğŸ“Š Found {len(available)} symbol/timeframe combinations")
    
    models_saved = 0
    
    for _, row in available.iterrows():
        symbol = row['symbol']
        timeframe = row['timeframe']
        count = row['count']
        
        # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø±Ù…Ø²
        clean_symbol = symbol.replace('m', '').replace('.ecn', '').upper()
        
        logger.info(f"\n{'='*40}")
        logger.info(f"Processing {clean_symbol} {timeframe} ({count:,} records)")
        
        try:
            # Ø¬Ù„Ø¨ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
            data_query = f"""
            SELECT time, open, high, low, close, volume
            FROM price_data
            WHERE symbol = '{symbol}' AND timeframe = '{timeframe}'
            ORDER BY time
            LIMIT 5000
            """
            
            df = pd.read_sql_query(data_query, conn)
            
            if len(df) < 500:
                logger.warning(f"Skipping - insufficient data: {len(df)}")
                continue
            
            # ØªØ­Ø¶ÙŠØ± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
            df['time'] = pd.to_datetime(df['time'])
            df.set_index('time', inplace=True)
            
            # Ø­Ø³Ø§Ø¨ Ù…ÙŠØ²Ø§Øª Ø¨Ø³ÙŠØ·Ø©
            features = pd.DataFrame(index=df.index)
            
            # Returns
            features['returns'] = df['close'].pct_change()
            
            # Moving averages
            for period in [5, 10, 20]:
                features[f'sma_{period}'] = df['close'].rolling(period).mean()
                features[f'ratio_sma_{period}'] = df['close'] / features[f'sma_{period}']
            
            # RSI
            delta = df['close'].diff()
            gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()
            loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()
            rs = gain / loss
            features['rsi'] = 100 - (100 / (1 + rs))
            
            # Bollinger Bands
            sma = df['close'].rolling(20).mean()
            std = df['close'].rolling(20).std()
            features['bb_upper'] = sma + (std * 2)
            features['bb_lower'] = sma - (std * 2)
            features['bb_position'] = (df['close'] - features['bb_lower']) / (features['bb_upper'] - features['bb_lower'])
            
            # Volume
            features['volume_ratio'] = df['volume'] / df['volume'].rolling(20).mean()
            
            # Target
            features['target'] = (df['close'].shift(-1) > df['close']).astype(int)
            
            # Ø­Ø°Ù NaN
            features = features.dropna()
            
            if len(features) < 200:
                logger.warning(f"Skipping - insufficient features: {len(features)}")
                continue
            
            # ØªØ­Ø¶ÙŠØ± X Ùˆ y
            X = features.drop('target', axis=1)
            y = features['target']
            
            # ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
            
            # Scaling
            scaler = RobustScaler()
            X_train_scaled = scaler.fit_transform(X_train)
            X_test_scaled = scaler.transform(X_test)
            
            # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…Ø¬Ù„Ø¯
            os.makedirs('./trained_models', exist_ok=True)
            
            # ØªØ¯Ø±ÙŠØ¨ ÙˆØ­ÙØ¸ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬
            models = {
                'random_forest_enhanced': RandomForestClassifier(n_estimators=50, max_depth=10, random_state=42),
                'gradient_boosting_enhanced': GradientBoostingClassifier(n_estimators=50, max_depth=5, random_state=42),
                'extra_trees_enhanced': ExtraTreesClassifier(n_estimators=50, max_depth=10, random_state=42)
            }
            
            for model_name, model in models.items():
                try:
                    # ØªØ¯Ø±ÙŠØ¨
                    model.fit(X_train_scaled, y_train)
                    
                    # Ø­ÙØ¸
                    model_path = f'./trained_models/{clean_symbol}_{timeframe}_{model_name}.pkl'
                    joblib.dump(model, model_path)
                    
                    # ØªÙ‚ÙŠÙŠÙ…
                    score = model.score(X_test_scaled, y_test)
                    logger.info(f"   âœ… {model_name}: {score:.2%} accuracy")
                    models_saved += 1
                    
                except Exception as e:
                    logger.error(f"   âŒ Failed {model_name}: {e}")
            
            # Ø­ÙØ¸ Scaler
            scaler_path = f'./trained_models/{clean_symbol}_{timeframe}_scaler_enhanced.pkl'
            joblib.dump(scaler, scaler_path)
            
        except Exception as e:
            logger.error(f"Error processing {symbol} {timeframe}: {e}")
            continue
        
        # ØªÙˆÙ‚Ù Ø¨Ø¹Ø¯ 5 Ø£Ø²ÙˆØ§Ø¬ Ù„Ù„Ø§Ø®ØªØ¨Ø§Ø±
        if models_saved >= 15:
            logger.info("\nâœ… Saved 15 models for testing - stopping here")
            break
    
    conn.close()
    
    # Ø§Ù„ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ
    logger.info("\n" + "="*60)
    logger.info("ğŸ Training Complete")
    logger.info(f"âœ… Models saved: {models_saved}")
    
    # Ø¹Ø±Ø¶ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…Ø­ÙÙˆØ¸Ø©
    if os.path.exists('./trained_models'):
        all_files = os.listdir('./trained_models')
        model_files = [f for f in all_files if f.endswith('.pkl') and 'scaler' not in f]
        scaler_files = [f for f in all_files if 'scaler' in f]
        
        logger.info(f"\nğŸ“ Files in trained_models/:")
        logger.info(f"   ğŸ¤– Models: {len(model_files)}")
        logger.info(f"   ğŸ“ Scalers: {len(scaler_files)}")
        
        if model_files:
            logger.info("\nâœ… Sample models:")
            for model in model_files[:5]:
                logger.info(f"   - {model}")

if __name__ == "__main__":
    force_save_models()