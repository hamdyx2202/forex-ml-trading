#!/usr/bin/env python3
"""
Advanced Learner with Unified Standards
Ù†Ø¸Ø§Ù… Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ù…ØªÙ‚Ø¯Ù… Ù…Ø¹ Ø§Ù„Ù…Ø¹Ø§ÙŠÙŠØ± Ø§Ù„Ù…ÙˆØ­Ø¯Ø©
"""

import sys
import os
from pathlib import Path
sys.path.append(str(Path(__file__).parent.parent))

import pandas as pd
import numpy as np
import joblib
from datetime import datetime
import sqlite3
from loguru import logger
import json
import time

# Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ù…Ø¹Ø§ÙŠÙŠØ± Ø§Ù„Ù…ÙˆØ­Ø¯Ø©
from unified_standards import (
    STANDARD_FEATURES, 
    get_model_filename,
    ensure_standard_features,
    TRAINING_STANDARDS,
    SAVING_STANDARDS
)

# Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø£Ø¯ÙˆØ§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨
from feature_engineer_adaptive import AdaptiveFeatureEngineer
from src.model_trainer import ModelTrainer

class UnifiedAdvancedLearner:
    """Ù†Ø¸Ø§Ù… Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ù…ØªÙ‚Ø¯Ù… Ù…Ø¹ Ø§Ù„Ù…Ø¹Ø§ÙŠÙŠØ± Ø§Ù„Ù…ÙˆØ­Ø¯Ø©"""
    
    def __init__(self):
        self.feature_engineer = AdaptiveFeatureEngineer(target_features=STANDARD_FEATURES)
        self.trainer = ModelTrainer()
        self.db_path = "data/forex_data.db"
        self.models_dir = Path(SAVING_STANDARDS['models_dir'])
        self.models_dir.mkdir(parents=True, exist_ok=True)
        
        # Ø³Ø¬Ù„ Ø§Ù„Ø£Ø¯Ø§Ø¡
        self.performance_log = self.load_performance_log()
        logger.info(f"ðŸš€ Unified Advanced Learner initialized")
        logger.info(f"ðŸ“Š Standard features: {STANDARD_FEATURES}")
        
    def load_performance_log(self):
        """ØªØ­Ù…ÙŠÙ„ Ø³Ø¬Ù„ Ø§Ù„Ø£Ø¯Ø§Ø¡"""
        log_file = self.models_dir / "performance_log.json"
        if log_file.exists():
            with open(log_file, 'r') as f:
                return json.load(f)
        return {"models": {}, "last_update": None}
    
    def save_performance_log(self):
        """Ø­ÙØ¸ Ø³Ø¬Ù„ Ø§Ù„Ø£Ø¯Ø§Ø¡"""
        log_file = self.models_dir / "performance_log.json"
        self.performance_log['last_update'] = datetime.now().isoformat()
        with open(log_file, 'w') as f:
            json.dump(self.performance_log, f, indent=2)
    
    def get_recent_data(self, symbol, timeframe, days=30):
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø­Ø¯ÙŠØ«Ø©"""
        try:
            conn = sqlite3.connect(self.db_path)
            
            # Ø­Ø³Ø§Ø¨ Ø§Ù„ØªØ§Ø±ÙŠØ®
            end_date = datetime.now()
            start_date = end_date - pd.Timedelta(days=days)
            
            query = """
            SELECT * FROM forex_data 
            WHERE symbol = ? AND timeframe = ? 
            AND datetime >= ?
            ORDER BY datetime
            """
            
            df = pd.read_sql_query(
                query, 
                conn, 
                params=(symbol, timeframe, start_date.isoformat())
            )
            conn.close()
            
            if len(df) < 100:
                logger.warning(f"Not enough data for {symbol} {timeframe}: {len(df)} rows")
                return None
                
            # ØªØ­ÙˆÙŠÙ„ Ø§Ù„ÙˆÙ‚Øª
            df['datetime'] = pd.to_datetime(df['datetime'])
            df.set_index('datetime', inplace=True)
            
            return df
            
        except Exception as e:
            logger.error(f"Error loading data: {e}")
            return None
    
    def evaluate_model_performance(self, symbol, timeframe):
        """ØªÙ‚ÙŠÙŠÙ… Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø­Ø§Ù„ÙŠ"""
        model_key = f"{symbol}_{timeframe}"
        
        # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø­Ø§Ù„ÙŠ
        model_file = self.models_dir / get_model_filename(symbol, timeframe)
        if not model_file.exists():
            logger.info(f"No model found for {model_key}")
            return None
            
        try:
            model_data = joblib.load(model_file)
            current_accuracy = model_data.get('metrics', {}).get('accuracy', 0)
            
            # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø­Ø¯ÙŠØ«Ø©
            df = self.get_recent_data(symbol, timeframe)
            if df is None:
                return None
            
            # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…ÙŠØ²Ø§Øª
            df_features = self.feature_engineer.create_features(df)
            
            # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØµÙÙˆÙ Ø¨Ø¯ÙˆÙ† Ù‡Ø¯Ù
            df_features = df_features.dropna(subset=['target_binary'])
            
            if len(df_features) < 50:
                return None
            
            # ØªØ­Ø¶ÙŠØ± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
            feature_cols = [col for col in df_features.columns 
                          if col not in ['target', 'target_binary', 'target_3class', 
                                       'future_return', 'time', 'open', 'high', 
                                       'low', 'close', 'volume', 'spread', 'datetime']]
            
            # Ø¶Ù…Ø§Ù† 70 Ù…ÙŠØ²Ø©
            df_features, feature_cols = ensure_standard_features(df_features, feature_cols)
            
            X = df_features[feature_cols].values
            y = df_features['target_binary'].values
            
            # ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
            model = model_data['model']
            scaler = model_data['scaler']
            
            X_scaled = scaler.transform(X)
            accuracy = model.score(X_scaled, y)
            
            logger.info(f"{model_key} - Current: {current_accuracy:.2%}, Recent: {accuracy:.2%}")
            
            return {
                'current_accuracy': current_accuracy,
                'recent_accuracy': accuracy,
                'needs_update': accuracy < current_accuracy - 0.05  # Ø§Ù†Ø®ÙØ§Ø¶ 5%
            }
            
        except Exception as e:
            logger.error(f"Error evaluating model: {e}")
            return None
    
    def update_model(self, symbol, timeframe):
        """ØªØ­Ø¯ÙŠØ« Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¥Ø°Ø§ Ù„Ø²Ù… Ø§Ù„Ø£Ù…Ø±"""
        logger.info(f"ðŸ”„ Updating model for {symbol} {timeframe}")
        
        try:
            # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ÙƒØ§Ù…Ù„Ø©
            conn = sqlite3.connect(self.db_path)
            query = """
            SELECT * FROM forex_data 
            WHERE symbol = ? AND timeframe = ?
            ORDER BY datetime
            """
            df = pd.read_sql_query(query, conn, params=(symbol, timeframe))
            conn.close()
            
            if len(df) < 1000:
                logger.warning(f"Not enough data for training: {len(df)} rows")
                return False
            
            # ØªØ­ÙˆÙŠÙ„ Ø§Ù„ÙˆÙ‚Øª
            df['datetime'] = pd.to_datetime(df['datetime'])
            df.set_index('datetime', inplace=True)
            
            # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…ÙŠØ²Ø§Øª
            logger.info("Creating features...")
            df_features = self.feature_engineer.create_features(df)
            
            # Ø¥Ø²Ø§Ù„Ø© NaN
            df_features = df_features.dropna(subset=['target_binary'])
            
            # ØªØ­Ø¶ÙŠØ± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
            feature_cols = [col for col in df_features.columns 
                          if col not in ['target', 'target_binary', 'target_3class', 
                                       'future_return', 'time', 'open', 'high', 
                                       'low', 'close', 'volume', 'spread', 'datetime']]
            
            # Ø¶Ù…Ø§Ù† 70 Ù…ÙŠØ²Ø©
            df_features, feature_cols = ensure_standard_features(df_features, feature_cols)
            
            X = df_features[feature_cols].values
            y = df_features['target_binary'].values
            
            logger.info(f"Training data shape: {X.shape}")
            
            # Ø§Ù„ØªØ¯Ø±ÙŠØ¨
            from sklearn.model_selection import train_test_split
            from sklearn.preprocessing import RobustScaler
            from sklearn.ensemble import VotingClassifier
            import lightgbm as lgb
            import xgboost as xgb
            from catboost import CatBoostClassifier
            from sklearn.ensemble import RandomForestClassifier
            
            X_train, X_test, y_train, y_test = train_test_split(
                X, y, 
                test_size=TRAINING_STANDARDS['test_size'], 
                random_state=TRAINING_STANDARDS['random_state']
            )
            
            # Ø§Ù„ØªØ·Ø¨ÙŠØ¹
            scaler = RobustScaler()
            X_train_scaled = scaler.fit_transform(X_train)
            X_test_scaled = scaler.transform(X_test)
            
            # Ø¨Ù†Ø§Ø¡ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬
            models = []
            
            # LightGBM
            lgb_model = lgb.LGBMClassifier(
                **TRAINING_STANDARDS['models']['lightgbm'],
                random_state=42,
                verbosity=-1
            )
            lgb_model.fit(X_train_scaled, y_train)
            models.append(('lightgbm', lgb_model))
            
            # XGBoost
            xgb_model = xgb.XGBClassifier(
                **TRAINING_STANDARDS['models']['xgboost'],
                random_state=42,
                use_label_encoder=False,
                eval_metric='logloss'
            )
            xgb_model.fit(X_train_scaled, y_train)
            models.append(('xgboost', xgb_model))
            
            # Random Forest
            rf_model = RandomForestClassifier(
                **TRAINING_STANDARDS['models']['random_forest'],
                random_state=42,
                n_jobs=-1
            )
            rf_model.fit(X_train_scaled, y_train)
            models.append(('random_forest', rf_model))
            
            # Ensemble
            ensemble = VotingClassifier(
                estimators=models,
                voting='soft',
                n_jobs=-1
            )
            ensemble.fit(X_train_scaled, y_train)
            
            # Ø§Ù„ØªÙ‚ÙŠÙŠÙ…
            accuracy = ensemble.score(X_test_scaled, y_test)
            logger.info(f"âœ… New model accuracy: {accuracy:.2%}")
            
            # Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
            model_data = {
                'model': ensemble,
                'scaler': scaler,
                'feature_names': feature_cols,
                'n_features': STANDARD_FEATURES,
                'metrics': {
                    'accuracy': float(accuracy),
                    'training_samples': len(X_train),
                    'test_samples': len(X_test),
                    'update_date': datetime.now().isoformat()
                },
                'metadata': {
                    'symbol': symbol,
                    'timeframe': timeframe,
                    'learner': 'UnifiedAdvancedLearner',
                    'standards_version': '1.0'
                }
            }
            
            # Ø­ÙØ¸ Ø¨Ø§Ù„Ø§Ø³Ù… Ø§Ù„Ù‚ÙŠØ§Ø³ÙŠ (Ø¨Ø¯ÙˆÙ† timestamp)
            filename = get_model_filename(symbol, timeframe)
            filepath = self.models_dir / filename
            
            # Ù†Ø³Ø®Ø© Ø§Ø­ØªÙŠØ§Ø·ÙŠØ©
            if filepath.exists():
                backup_dir = Path(SAVING_STANDARDS['backup_dir'])
                backup_dir.mkdir(parents=True, exist_ok=True)
                backup_file = backup_dir / f"{filename}.{datetime.now().strftime('%Y%m%d_%H%M%S')}"
                joblib.dump(joblib.load(filepath), backup_file)
            
            # Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø¬Ø¯ÙŠØ¯
            joblib.dump(model_data, filepath, compress=SAVING_STANDARDS['compression'])
            logger.info(f"âœ… Model saved: {filepath}")
            
            # ØªØ­Ø¯ÙŠØ« Ø³Ø¬Ù„ Ø§Ù„Ø£Ø¯Ø§Ø¡
            model_key = f"{symbol}_{timeframe}"
            self.performance_log['models'][model_key] = {
                'accuracy': float(accuracy),
                'last_update': datetime.now().isoformat(),
                'training_samples': len(X_train)
            }
            self.save_performance_log()
            
            return True
            
        except Exception as e:
            logger.error(f"Error updating model: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def run_continuous_learning(self):
        """ØªØ´ØºÙŠÙ„ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ù…Ø³ØªÙ…Ø±"""
        logger.info("ðŸš€ Starting Unified Advanced Learning...")
        
        symbols = ['EURUSDm', 'GBPUSDm', 'USDJPYm', 'USDCHFm', 
                  'AUDUSDm', 'USDCADm', 'NZDUSDm', 'EURJPYm']
        timeframes = ['PERIOD_M5', 'PERIOD_M15', 'PERIOD_H1', 'PERIOD_H4']
        
        while True:
            try:
                logger.info(f"\n{'='*60}")
                logger.info(f"ðŸ”„ Learning cycle started at {datetime.now()}")
                
                updated_count = 0
                
                for symbol in symbols:
                    for timeframe in timeframes:
                        # ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ø£Ø¯Ø§Ø¡
                        evaluation = self.evaluate_model_performance(symbol, timeframe)
                        
                        if evaluation:
                            if evaluation['needs_update']:
                                logger.warning(f"âš ï¸ {symbol} {timeframe} needs update")
                                logger.info(f"   Current: {evaluation['current_accuracy']:.2%}")
                                logger.info(f"   Recent: {evaluation['recent_accuracy']:.2%}")
                                
                                # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
                                if self.update_model(symbol, timeframe):
                                    updated_count += 1
                            else:
                                logger.info(f"âœ… {symbol} {timeframe} performing well")
                
                logger.info(f"\nðŸ“Š Updated {updated_count} models")
                logger.info(f"ðŸ’¤ Sleeping for 1 hour...")
                
                # Ø§Ù„Ø§Ù†ØªØ¸Ø§Ø± Ø³Ø§Ø¹Ø© ÙˆØ§Ø­Ø¯Ø©
                time.sleep(3600)
                
            except KeyboardInterrupt:
                logger.info("ðŸ›‘ Learning stopped by user")
                break
            except Exception as e:
                logger.error(f"Error in learning cycle: {e}")
                time.sleep(300)  # 5 Ø¯Ù‚Ø§Ø¦Ù‚ ÙÙŠ Ø­Ø§Ù„Ø© Ø§Ù„Ø®Ø·Ø£

if __name__ == "__main__":
    learner = UnifiedAdvancedLearner()
    learner.run_continuous_learning()