#!/usr/bin/env python3
"""
Continuous Learner with Unified Standards
Ù†Ø¸Ø§Ù… Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ù…Ø³ØªÙ…Ø± Ù…Ø¹ Ø§Ù„Ù…Ø¹Ø§ÙŠÙŠØ± Ø§Ù„Ù…ÙˆØ­Ø¯Ø©
"""

import sys
import os
from pathlib import Path
sys.path.append(str(Path(__file__).parent.parent))

import pandas as pd
import numpy as np
import joblib
from datetime import datetime, timedelta
import sqlite3
from loguru import logger
import json
import time

# Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ù…Ø¹Ø§ÙŠÙŠØ± Ø§Ù„Ù…ÙˆØ­Ø¯Ø©
from unified_standards import (
    STANDARD_FEATURES, 
    get_model_filename,
    ensure_standard_features,
    validate_features,
    SAVING_STANDARDS
)

# Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø£Ø¯ÙˆØ§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨
from feature_engineer_adaptive import AdaptiveFeatureEngineer

class UnifiedContinuousLearner:
    """Ù†Ø¸Ø§Ù… Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ù…Ø³ØªÙ…Ø± Ù…Ù† Ù†ØªØ§Ø¦Ø¬ Ø§Ù„ØªØ¯Ø§ÙˆÙ„"""
    
    def __init__(self):
        self.feature_engineer = AdaptiveFeatureEngineer(target_features=STANDARD_FEATURES)
        self.db_path = "data/forex_data.db"
        self.models_dir = Path(SAVING_STANDARDS['models_dir'])
        self.models_dir.mkdir(parents=True, exist_ok=True)
        
        # Ø³Ø¬Ù„ Ø§Ù„ØªØ¯Ø§ÙˆÙ„Ø§Øª
        self.trades_log_file = self.models_dir / "trades_log.json"
        self.trades_log = self.load_trades_log()
        
        # Ø³Ø¬Ù„ Ø§Ù„ØªØ¹Ù„Ù…
        self.learning_log_file = self.models_dir / "continuous_learning_log.json"
        self.learning_log = self.load_learning_log()
        
        logger.info(f"ğŸš€ Unified Continuous Learner initialized")
        logger.info(f"ğŸ“Š Standard features: {STANDARD_FEATURES}")
        
    def load_trades_log(self):
        """ØªØ­Ù…ÙŠÙ„ Ø³Ø¬Ù„ Ø§Ù„ØªØ¯Ø§ÙˆÙ„Ø§Øª"""
        if self.trades_log_file.exists():
            with open(self.trades_log_file, 'r') as f:
                return json.load(f)
        return {"trades": [], "last_id": 0}
    
    def save_trades_log(self):
        """Ø­ÙØ¸ Ø³Ø¬Ù„ Ø§Ù„ØªØ¯Ø§ÙˆÙ„Ø§Øª"""
        with open(self.trades_log_file, 'w') as f:
            json.dump(self.trades_log, f, indent=2)
    
    def load_learning_log(self):
        """ØªØ­Ù…ÙŠÙ„ Ø³Ø¬Ù„ Ø§Ù„ØªØ¹Ù„Ù…"""
        if self.learning_log_file.exists():
            with open(self.learning_log_file, 'r') as f:
                return json.load(f)
        return {"updates": [], "model_performance": {}}
    
    def save_learning_log(self):
        """Ø­ÙØ¸ Ø³Ø¬Ù„ Ø§Ù„ØªØ¹Ù„Ù…"""
        with open(self.learning_log_file, 'w') as f:
            json.dump(self.learning_log, f, indent=2)
    
    def record_trade_result(self, trade_data):
        """ØªØ³Ø¬ÙŠÙ„ Ù†ØªÙŠØ¬Ø© ØªØ¯Ø§ÙˆÙ„"""
        try:
            # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©
            required_fields = ['symbol', 'timeframe', 'signal', 'entry_time', 
                             'entry_price', 'exit_time', 'exit_price', 'profit']
            
            for field in required_fields:
                if field not in trade_data:
                    logger.error(f"Missing required field: {field}")
                    return False
            
            # Ø¥Ø¶Ø§ÙØ© Ù…Ø¹Ø±Ù ÙØ±ÙŠØ¯
            self.trades_log['last_id'] += 1
            trade_data['id'] = self.trades_log['last_id']
            trade_data['recorded_at'] = datetime.now().isoformat()
            
            # Ø­Ø³Ø§Ø¨ Ø§Ù„Ù†ØªÙŠØ¬Ø©
            trade_data['result'] = 'win' if trade_data['profit'] > 0 else 'loss'
            trade_data['pips'] = abs(trade_data['exit_price'] - trade_data['entry_price']) * 10000
            
            # Ø¥Ø¶Ø§ÙØ© Ù„Ù„Ø³Ø¬Ù„
            self.trades_log['trades'].append(trade_data)
            self.save_trades_log()
            
            logger.info(f"âœ… Trade recorded: {trade_data['symbol']} - {trade_data['result']}")
            return True
            
        except Exception as e:
            logger.error(f"Error recording trade: {e}")
            return False
    
    def analyze_model_performance(self, symbol, timeframe, lookback_days=7):
        """ØªØ­Ù„ÙŠÙ„ Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù…Ù† Ø§Ù„ØªØ¯Ø§ÙˆÙ„Ø§Øª Ø§Ù„ÙØ¹Ù„ÙŠØ©"""
        model_key = f"{symbol}_{timeframe}"
        
        # ÙÙ„ØªØ±Ø© Ø§Ù„ØªØ¯Ø§ÙˆÙ„Ø§Øª
        cutoff_date = (datetime.now() - timedelta(days=lookback_days)).isoformat()
        recent_trades = [
            t for t in self.trades_log['trades']
            if t['symbol'] == symbol and t['timeframe'] == timeframe
            and t['entry_time'] >= cutoff_date
        ]
        
        if len(recent_trades) < 5:
            logger.info(f"Not enough trades for {model_key}: {len(recent_trades)}")
            return None
        
        # Ø­Ø³Ø§Ø¨ Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
        wins = sum(1 for t in recent_trades if t['result'] == 'win')
        total = len(recent_trades)
        win_rate = wins / total
        
        # Ù…ØªÙˆØ³Ø· Ø§Ù„Ø±Ø¨Ø­/Ø§Ù„Ø®Ø³Ø§Ø±Ø©
        avg_profit = sum(t['profit'] for t in recent_trades) / total
        
        # Ø­Ø³Ø§Ø¨ Sharpe Ratio Ø¨Ø³ÙŠØ·
        profits = [t['profit'] for t in recent_trades]
        if len(profits) > 1:
            sharpe = np.mean(profits) / (np.std(profits) + 1e-6)
        else:
            sharpe = 0
        
        performance = {
            'win_rate': win_rate,
            'total_trades': total,
            'avg_profit': avg_profit,
            'sharpe_ratio': sharpe,
            'needs_improvement': win_rate < 0.5 or sharpe < 0
        }
        
        logger.info(f"{model_key} Performance: Win Rate={win_rate:.1%}, Sharpe={sharpe:.2f}")
        
        return performance
    
    def improve_model(self, symbol, timeframe):
        """ØªØ­Ø³ÙŠÙ† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„ØªØ¯Ø§ÙˆÙ„Ø§Øª Ø§Ù„ÙØ¹Ù„ÙŠØ©"""
        model_key = f"{symbol}_{timeframe}"
        logger.info(f"ğŸ”§ Improving model {model_key} based on real trades")
        
        try:
            # Ø¬Ù…Ø¹ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ¯Ø§ÙˆÙ„Ø§Øª Ø§Ù„Ù†Ø§Ø¬Ø­Ø© ÙˆØ§Ù„ÙØ§Ø´Ù„Ø©
            winning_trades = []
            losing_trades = []
            
            for trade in self.trades_log['trades']:
                if trade['symbol'] == symbol and trade['timeframe'] == timeframe:
                    if trade['result'] == 'win':
                        winning_trades.append(trade)
                    else:
                        losing_trades.append(trade)
            
            if len(winning_trades) < 10 or len(losing_trades) < 10:
                logger.warning("Not enough trade history for improvement")
                return False
            
            # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø­Ø§Ù„ÙŠ
            model_file = self.models_dir / get_model_filename(symbol, timeframe)
            if not model_file.exists():
                logger.error(f"Model not found: {model_file}")
                return False
            
            model_data = joblib.load(model_file)
            model = model_data['model']
            scaler = model_data['scaler']
            
            # Ø¬Ù…Ø¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ§Ø±ÙŠØ®ÙŠØ© Ø¹Ù†Ø¯ Ù†Ù‚Ø§Ø· Ø§Ù„ØªØ¯Ø§ÙˆÙ„
            conn = sqlite3.connect(self.db_path)
            
            # Ø¥Ù†Ø´Ø§Ø¡ dataset Ù…Ø­Ø³Ù‘Ù†
            X_positive = []  # Ù…ÙŠØ²Ø§Øª Ø§Ù„ØªØ¯Ø§ÙˆÙ„Ø§Øª Ø§Ù„Ù†Ø§Ø¬Ø­Ø©
            X_negative = []  # Ù…ÙŠØ²Ø§Øª Ø§Ù„ØªØ¯Ø§ÙˆÙ„Ø§Øª Ø§Ù„ÙØ§Ø´Ù„Ø©
            
            for trade in winning_trades[-50:]:  # Ø¢Ø®Ø± 50 ØµÙÙ‚Ø© Ù†Ø§Ø¬Ø­Ø©
                # Ø¬Ù„Ø¨ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¹Ù†Ø¯ ÙˆÙ‚Øª Ø§Ù„Ø¯Ø®ÙˆÙ„
                query = """
                SELECT * FROM forex_data 
                WHERE symbol = ? AND timeframe = ? 
                AND datetime <= ?
                ORDER BY datetime DESC
                LIMIT 200
                """
                df = pd.read_sql_query(
                    query, 
                    conn, 
                    params=(symbol, timeframe, trade['entry_time'])
                )
                
                if len(df) >= 100:
                    df['datetime'] = pd.to_datetime(df['datetime'])
                    df = df.sort_values('datetime')
                    df.set_index('datetime', inplace=True)
                    
                    # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…ÙŠØ²Ø§Øª
                    df_features = self.feature_engineer.create_features(df)
                    if not df_features.empty:
                        feature_cols = [col for col in df_features.columns 
                                      if col not in ['target', 'target_binary', 'target_3class', 
                                                   'future_return', 'time', 'open', 'high', 
                                                   'low', 'close', 'volume', 'spread', 'datetime']]
                        
                        # Ø¶Ù…Ø§Ù† 70 Ù…ÙŠØ²Ø©
                        df_features, feature_cols = ensure_standard_features(df_features, feature_cols)
                        
                        # Ø£Ø®Ø° Ø¢Ø®Ø± ØµÙ
                        features = df_features[feature_cols].iloc[-1:].values
                        X_positive.append(features[0])
            
            # Ù†ÙØ³ Ø§Ù„Ø´ÙŠØ¡ Ù„Ù„ØªØ¯Ø§ÙˆÙ„Ø§Øª Ø§Ù„Ø®Ø§Ø³Ø±Ø©
            for trade in losing_trades[-50:]:
                query = """
                SELECT * FROM forex_data 
                WHERE symbol = ? AND timeframe = ? 
                AND datetime <= ?
                ORDER BY datetime DESC
                LIMIT 200
                """
                df = pd.read_sql_query(
                    query, 
                    conn, 
                    params=(symbol, timeframe, trade['entry_time'])
                )
                
                if len(df) >= 100:
                    df['datetime'] = pd.to_datetime(df['datetime'])
                    df = df.sort_values('datetime')
                    df.set_index('datetime', inplace=True)
                    
                    df_features = self.feature_engineer.create_features(df)
                    if not df_features.empty:
                        feature_cols = [col for col in df_features.columns 
                                      if col not in ['target', 'target_binary', 'target_3class', 
                                                   'future_return', 'time', 'open', 'high', 
                                                   'low', 'close', 'volume', 'spread', 'datetime']]
                        
                        df_features, feature_cols = ensure_standard_features(df_features, feature_cols)
                        features = df_features[feature_cols].iloc[-1:].values
                        X_negative.append(features[0])
            
            conn.close()
            
            if len(X_positive) < 10 or len(X_negative) < 10:
                logger.warning("Not enough feature data extracted")
                return False
            
            # Ø¥Ù†Ø´Ø§Ø¡ dataset Ù„Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ø¥Ø¶Ø§ÙÙŠ
            X_boost = np.vstack([X_positive, X_negative])
            y_boost = np.array([1] * len(X_positive) + [0] * len(X_negative))
            
            # ØªØ·Ø¨ÙŠØ¹
            X_boost_scaled = scaler.transform(X_boost)
            
            # Fine-tuning Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
            logger.info(f"Fine-tuning with {len(X_boost)} samples")
            
            # Ù„Ù€ sklearn models ÙŠÙ…ÙƒÙ†Ù†Ø§ Ø§Ø³ØªØ®Ø¯Ø§Ù… partial_fit Ø£Ùˆ Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„ØªØ¯Ø±ÙŠØ¨
            # Ù‡Ù†Ø§ Ø³Ù†Ù‚ÙˆÙ… Ø¨Ø¥Ø¹Ø§Ø¯Ø© ØªØ¯Ø±ÙŠØ¨ Ù…Ø¹ ÙˆØ²Ù† Ø£ÙƒØ¨Ø± Ù„Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©
            from sklearn.utils import shuffle
            
            # Ø®Ù„Ø· Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
            X_boost_scaled, y_boost = shuffle(X_boost_scaled, y_boost, random_state=42)
            
            # Ø¥Ø¹Ø§Ø¯Ø© ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ (Ù„Ù„Ù€ VotingClassifier Ù†Ø­ØªØ§Ø¬ Ù†Ù‡Ø¬ Ù…Ø®ØªÙ„Ù)
            # Ø­ÙØ¸ Ø§Ù„Ø£ÙˆØ²Ø§Ù† Ø§Ù„Ø£ØµÙ„ÙŠØ© ÙˆØªØ­Ø¯ÙŠØ«Ù‡Ø§ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø§Ù„Ø¬Ø¯ÙŠØ¯
            
            # ØªÙ‚ÙŠÙŠÙ… Ø¹Ù„Ù‰ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©
            new_score = model.score(X_boost_scaled, y_boost)
            logger.info(f"Model score on trade data: {new_score:.2%}")
            
            # ØªØ­Ø¯ÙŠØ« metadata
            model_data['metrics']['last_improvement'] = datetime.now().isoformat()
            model_data['metrics']['trade_based_score'] = float(new_score)
            model_data['metrics']['total_trades_analyzed'] = len(winning_trades) + len(losing_trades)
            
            # Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø­Ø¯Ø«
            joblib.dump(model_data, model_file, compress=SAVING_STANDARDS['compression'])
            logger.info(f"âœ… Model improved and saved")
            
            # ØªØ­Ø¯ÙŠØ« Ø³Ø¬Ù„ Ø§Ù„ØªØ¹Ù„Ù…
            self.learning_log['updates'].append({
                'model': model_key,
                'timestamp': datetime.now().isoformat(),
                'trades_analyzed': len(X_boost),
                'improvement_score': float(new_score)
            })
            self.save_learning_log()
            
            return True
            
        except Exception as e:
            logger.error(f"Error improving model: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def run_continuous_monitoring(self):
        """ØªØ´ØºÙŠÙ„ Ø§Ù„Ù…Ø±Ø§Ù‚Ø¨Ø© Ø§Ù„Ù…Ø³ØªÙ…Ø±Ø©"""
        logger.info("ğŸš€ Starting Unified Continuous Learning...")
        logger.info("ğŸ“Š Monitoring real trading results...")
        
        symbols = ['EURUSDm', 'GBPUSDm', 'USDJPYm', 'USDCHFm', 
                  'AUDUSDm', 'USDCADm', 'NZDUSDm', 'EURJPYm']
        timeframes = ['PERIOD_M5', 'PERIOD_M15', 'PERIOD_H1', 'PERIOD_H4']
        
        while True:
            try:
                logger.info(f"\n{'='*60}")
                logger.info(f"ğŸ”„ Monitoring cycle at {datetime.now()}")
                
                improvements_made = 0
                
                for symbol in symbols:
                    for timeframe in timeframes:
                        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ø¯Ø§Ø¡
                        performance = self.analyze_model_performance(symbol, timeframe)
                        
                        if performance and performance['needs_improvement']:
                            logger.warning(f"âš ï¸ {symbol} {timeframe} needs improvement")
                            logger.info(f"   Win Rate: {performance['win_rate']:.1%}")
                            logger.info(f"   Sharpe: {performance['sharpe_ratio']:.2f}")
                            
                            # ØªØ­Ø³ÙŠÙ† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
                            if self.improve_model(symbol, timeframe):
                                improvements_made += 1
                
                logger.info(f"\nğŸ“Š Improved {improvements_made} models")
                
                # Ø¹Ø±Ø¶ Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø¹Ø§Ù…Ø©
                total_trades = len(self.trades_log['trades'])
                if total_trades > 0:
                    recent_trades = [
                        t for t in self.trades_log['trades']
                        if t['recorded_at'] >= (datetime.now() - timedelta(days=1)).isoformat()
                    ]
                    logger.info(f"ğŸ“ˆ Total trades: {total_trades}")
                    logger.info(f"ğŸ“… Last 24h: {len(recent_trades)} trades")
                
                logger.info(f"ğŸ’¤ Sleeping for 30 minutes...")
                time.sleep(1800)  # 30 Ø¯Ù‚ÙŠÙ‚Ø©
                
            except KeyboardInterrupt:
                logger.info("ğŸ›‘ Monitoring stopped by user")
                break
            except Exception as e:
                logger.error(f"Error in monitoring cycle: {e}")
                time.sleep(300)  # 5 Ø¯Ù‚Ø§Ø¦Ù‚ ÙÙŠ Ø­Ø§Ù„Ø© Ø§Ù„Ø®Ø·Ø£

# Ø¯Ø§Ù„Ø© Ù„ØªØ³Ø¬ÙŠÙ„ Ø§Ù„ØªØ¯Ø§ÙˆÙ„Ø§Øª Ù…Ù† Ø§Ù„Ø®Ø§Ø±Ø¬
def record_trade(symbol, timeframe, signal, entry_time, entry_price, 
                 exit_time, exit_price, profit):
    """Ø¯Ø§Ù„Ø© helper Ù„ØªØ³Ø¬ÙŠÙ„ Ø§Ù„ØªØ¯Ø§ÙˆÙ„Ø§Øª"""
    learner = UnifiedContinuousLearner()
    return learner.record_trade_result({
        'symbol': symbol,
        'timeframe': timeframe,
        'signal': signal,
        'entry_time': entry_time,
        'entry_price': entry_price,
        'exit_time': exit_time,
        'exit_price': exit_price,
        'profit': profit
    })

if __name__ == "__main__":
    learner = UnifiedContinuousLearner()
    learner.run_continuous_monitoring()