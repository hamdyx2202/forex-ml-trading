#!/usr/bin/env python3
"""
Retrain Models with Auto Database Discovery
Ø¥Ø¹Ø§Ø¯Ø© ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ù…Ø¹ Ø§ÙƒØªØ´Ø§Ù Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ØªÙ„Ù‚Ø§Ø¦ÙŠØ§Ù‹
"""

import pandas as pd
import numpy as np
import joblib
import json
import os
import sqlite3
from datetime import datetime
from pathlib import Path
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import RobustScaler
from sklearn.ensemble import RandomForestClassifier, VotingClassifier
from sklearn.metrics import accuracy_score
import lightgbm as lgb
import xgboost as xgb
from catboost import CatBoostClassifier
from loguru import logger
import warnings
warnings.filterwarnings('ignore')

# Import unified feature engineer
from feature_engineering_unified import UnifiedFeatureEngineer

class AutoDBModelTrainer:
    """Ù…Ø¯Ø±Ø¨ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ù…Ø¹ Ø§ÙƒØªØ´Ø§Ù Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ØªÙ„Ù‚Ø§Ø¦ÙŠØ§Ù‹"""
    
    def __init__(self):
        self.feature_engineer = UnifiedFeatureEngineer()
        self.models = {}
        self.scalers = {}
        self.feature_info = {}
        self.db_path = None
        self.table_name = None
        self.find_database()
        
    def find_database(self):
        """Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ØªÙ„Ù‚Ø§Ø¦ÙŠØ§Ù‹"""
        logger.info("ğŸ” Searching for database...")
        
        # Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ø§Ù„Ù…Ø³Ø§Ø±Ø§Øª Ø§Ù„Ù…Ø­ØªÙ…Ù„Ø©
        possible_paths = [
            'forex_data.db',
            'data/forex_data.db',
            '../forex_data.db',
            'forex_ml_data.db',
            'mt5_data.db',
            'trading_data.db',
            '../../forex_data.db',
            '../data/forex_data.db'
        ]
        
        for path in possible_paths:
            if os.path.exists(path):
                if self.validate_database(path):
                    self.db_path = path
                    logger.info(f"âœ… Found database: {path}")
                    return
        
        # Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ø¹Ù…ÙŠÙ‚
        logger.info("Searching in subdirectories...")
        for root, dirs, files in os.walk('.', followlinks=True):
            # ØªØ¬Ù†Ø¨ Ø§Ù„Ù…Ø¬Ù„Ø¯Ø§Øª ØºÙŠØ± Ø§Ù„Ù…Ø±ØºÙˆØ¨Ø©
            if 'venv' in root or '.git' in root or '__pycache__' in root:
                continue
                
            for file in files:
                if file.endswith('.db'):
                    full_path = os.path.join(root, file)
                    if self.validate_database(full_path):
                        self.db_path = full_path
                        logger.info(f"âœ… Found database: {full_path}")
                        return
        
        logger.error("âŒ No suitable database found!")
        
    def validate_database(self, db_path):
        """Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ØµØ­Ø© Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª"""
        try:
            conn = sqlite3.connect(db_path)
            cursor = conn.cursor()
            
            # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ø¬Ø¯Ø§ÙˆÙ„
            cursor.execute("SELECT name FROM sqlite_master WHERE type='table';")
            tables = cursor.fetchall()
            
            for table in tables:
                table_name = table[0]
                
                # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø£Ø¹Ù…Ø¯Ø©
                cursor.execute(f"PRAGMA table_info({table_name})")
                columns = cursor.fetchall()
                col_names = [col[1].lower() for col in columns]
                
                # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ø£Ø¹Ù…Ø¯Ø© OHLCV
                required_cols = ['open', 'high', 'low', 'close']
                if all(col in col_names for col in required_cols):
                    # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ø¨ÙŠØ§Ù†Ø§Øª
                    cursor.execute(f"SELECT COUNT(*) FROM {table_name}")
                    count = cursor.fetchone()[0]
                    
                    if count > 0:
                        self.table_name = table_name
                        
                        # Ø·Ø¨Ø§Ø¹Ø© Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù‚Ø§Ø¹Ø¯Ø©
                        cursor.execute(f"SELECT DISTINCT symbol FROM {table_name} LIMIT 10")
                        symbols = [row[0] for row in cursor.fetchall()]
                        
                        cursor.execute(f"SELECT DISTINCT timeframe FROM {table_name} LIMIT 10")
                        timeframes = [row[0] for row in cursor.fetchall()]
                        
                        logger.info(f"ğŸ“Š Table: {table_name}")
                        logger.info(f"   Records: {count}")
                        logger.info(f"   Symbols: {symbols[:5]}...")
                        logger.info(f"   Timeframes: {timeframes[:5]}...")
                        
                        conn.close()
                        return True
                        
            conn.close()
            return False
            
        except Exception as e:
            logger.error(f"Error validating {db_path}: {e}")
            return False
    
    def load_data(self, symbol: str, timeframe: str):
        """ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù† Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª"""
        if not self.db_path or not self.table_name:
            logger.error("No database configured!")
            return None
            
        try:
            conn = sqlite3.connect(self.db_path)
            
            # Ø¨Ù†Ø§Ø¡ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…
            query = f"""
                SELECT time, open, high, low, close, volume
                FROM {self.table_name}
                WHERE symbol = ? AND timeframe = ?
                ORDER BY time
            """
            
            df = pd.read_sql_query(query, conn, params=(symbol, timeframe))
            conn.close()
            
            if len(df) < 500:  # Ø®ÙØ¶ Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ø¯Ù†Ù‰
                logger.warning(f"Not enough data for {symbol} {timeframe}: {len(df)} rows")
                return None
                
            # ØªØ­ÙˆÙŠÙ„ Ø§Ù„ÙˆÙ‚Øª
            df['datetime'] = pd.to_datetime(df['time'], unit='s')
            df.set_index('datetime', inplace=True)
            
            # Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ø¹Ù…ÙˆØ¯ volume
            if 'volume' not in df.columns:
                df['volume'] = 100  # Ù‚ÙŠÙ…Ø© Ø§ÙØªØ±Ø§Ø¶ÙŠØ©
                
            logger.info(f"Loaded {len(df)} rows for {symbol} {timeframe}")
            
            return df
            
        except Exception as e:
            logger.error(f"Error loading data for {symbol} {timeframe}: {e}")
            return None
    
    def prepare_data(self, df: pd.DataFrame, lookahead: int = 5, threshold: float = 0.001):
        """ØªØ­Ø¶ÙŠØ± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù„ØªØ¯Ø±ÙŠØ¨"""
        # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…ÙŠØ²Ø§Øª
        X, feature_names = self.feature_engineer.create_features(df)
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØªØ³Ù…ÙŠØ§Øª
        future_returns = df['close'].shift(-lookahead) / df['close'] - 1
        y = np.where(future_returns > threshold, 1, 0)
        
        # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØµÙÙˆÙ Ø§Ù„Ø£Ø®ÙŠØ±Ø©
        X = X[:-lookahead]
        y = y[:-lookahead]
        
        # Ø¥Ø²Ø§Ù„Ø© NaN
        mask = ~np.isnan(y)
        X = X[mask]
        y = y[mask]
        
        logger.info(f"Prepared {len(X)} samples with {len(feature_names)} features")
        
        return X, y, feature_names
    
    def create_ensemble_model(self):
        """Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ ensemble Ø¨Ø³ÙŠØ·"""
        models = []
        
        # LightGBM
        lgb_model = lgb.LGBMClassifier(
            n_estimators=50,  # ØªÙ‚Ù„ÙŠÙ„ Ù„Ù„Ø³Ø±Ø¹Ø©
            learning_rate=0.1,
            max_depth=5,
            random_state=42,
            n_jobs=-1,
            verbosity=-1
        )
        models.append(('lgb', lgb_model))
        
        # Random Forest
        rf_model = RandomForestClassifier(
            n_estimators=50,
            max_depth=10,
            random_state=42,
            n_jobs=-1
        )
        models.append(('rf', rf_model))
        
        # Voting Classifier
        ensemble = VotingClassifier(
            estimators=models,
            voting='soft',
            n_jobs=-1
        )
        
        return ensemble
    
    def train_model(self, symbol: str, timeframe: str, df: pd.DataFrame):
        """ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬"""
        logger.info(f"Training model for {symbol} {timeframe}")
        
        # ØªØ­Ø¶ÙŠØ± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        X, y, feature_names = self.prepare_data(df)
        
        if len(X) < 100:
            logger.error(f"Not enough samples for {symbol} {timeframe}")
            return None
        
        # ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42
        )
        
        # Ø§Ù„ØªØ·Ø¨ÙŠØ¹
        scaler = RobustScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.transform(X_test)
        
        # Ø§Ù„ØªØ¯Ø±ÙŠØ¨
        model = self.create_ensemble_model()
        model.fit(X_train_scaled, y_train)
        
        # Ø§Ù„ØªÙ‚ÙŠÙŠÙ…
        y_pred = model.predict(X_test_scaled)
        accuracy = accuracy_score(y_test, y_pred)
        
        logger.info(f"Model accuracy: {accuracy:.4f}")
        
        # Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
        model_key = f"{symbol}_{timeframe}"
        
        model_package = {
            'model': model,
            'scaler': scaler,
            'feature_names': feature_names,
            'n_features': len(feature_names),
            'feature_version': UnifiedFeatureEngineer.VERSION,
            'metrics': {
                'accuracy': float(accuracy),
                'samples': len(X)
            },
            'training_metadata': {
                'symbol': symbol,
                'timeframe': timeframe,
                'training_date': datetime.now().isoformat(),
                'db_path': self.db_path,
                'table_name': self.table_name
            }
        }
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø¬Ù„Ø¯ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬
        os.makedirs('models/unified', exist_ok=True)
        
        # Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
        model_filename = f'models/unified/{model_key}_unified_v2.pkl'
        joblib.dump(model_package, model_filename)
        logger.info(f"âœ… Model saved: {model_filename}")
        
        return model_package
    
    def train_all_models(self, symbols=None, timeframes=None):
        """ØªØ¯Ø±ÙŠØ¨ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…ØªØ§Ø­Ø©"""
        if not self.db_path:
            logger.error("No database found!")
            return None
            
        # Ø¥Ø°Ø§ Ù„Ù… ÙŠØªÙ… ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø±Ù…ÙˆØ²ØŒ Ø§Ø­ØµÙ„ Ø¹Ù„ÙŠÙ‡Ø§ Ù…Ù† Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        if symbols is None or timeframes is None:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            if symbols is None:
                cursor.execute(f"SELECT DISTINCT symbol FROM {self.table_name}")
                symbols = [row[0] for row in cursor.fetchall()]
                # ØªØµÙÙŠØ© Ø§Ù„Ø±Ù…ÙˆØ² Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©
                wanted_symbols = ['EURUSDm', 'GBPUSDm', 'USDJPYm', 'EURJPYm', 'GBPJPYm']
                symbols = [s for s in symbols if any(w in s for w in wanted_symbols)]
                
            if timeframes is None:
                cursor.execute(f"SELECT DISTINCT timeframe FROM {self.table_name}")
                timeframes = [row[0] for row in cursor.fetchall()]
                # ØªØµÙÙŠØ© Ø§Ù„Ø£Ø·Ø± Ø§Ù„Ø²Ù…Ù†ÙŠØ© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©
                wanted_tf = ['M5', 'M15', 'H1', 'H4']
                timeframes = [tf for tf in timeframes if any(w in tf for w in wanted_tf)]
                
            conn.close()
        
        logger.info(f"Training models for {len(symbols)} symbols and {len(timeframes)} timeframes")
        
        successful = 0
        failed = 0
        
        for symbol in symbols:
            for timeframe in timeframes:
                try:
                    # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
                    df = self.load_data(symbol, timeframe)
                    if df is not None:
                        # ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
                        model_package = self.train_model(symbol, timeframe, df)
                        if model_package:
                            successful += 1
                        else:
                            failed += 1
                    else:
                        failed += 1
                        
                except Exception as e:
                    logger.error(f"Error training {symbol} {timeframe}: {e}")
                    failed += 1
        
        logger.info(f"âœ… Training completed: {successful} successful, {failed} failed")
        
        # Ø­ÙØ¸ Ø§Ù„Ù…Ù„Ø®Øµ
        if successful > 0:
            summary = {
                'training_date': datetime.now().isoformat(),
                'successful': successful,
                'failed': failed,
                'db_path': self.db_path,
                'table_name': self.table_name,
                'feature_version': UnifiedFeatureEngineer.VERSION
            }
            
            with open('models/unified/training_summary.json', 'w') as f:
                json.dump(summary, f, indent=2)
                
        return successful, failed

# Ù„Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù…Ø¨Ø§Ø´Ø±
if __name__ == "__main__":
    print("ğŸš€ Starting Auto-DB Model Training...")
    
    # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…Ø¯Ø±Ø¨
    trainer = AutoDBModelTrainer()
    
    if trainer.db_path:
        # ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬
        successful, failed = trainer.train_all_models()
        
        print(f"\nâœ… Training Summary:")
        print(f"   Successful: {successful}")
        print(f"   Failed: {failed}")
        print(f"   Database: {trainer.db_path}")
        print(f"   Table: {trainer.table_name}")
    else:
        print("âŒ No database found! Please check your data files.")