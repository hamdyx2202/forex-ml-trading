#!/usr/bin/env python3
"""
ğŸ§¹ Maintenance Script - Ø³ÙƒØ±ÙŠØ¨Øª Ø§Ù„ØµÙŠØ§Ù†Ø© Ø§Ù„Ø¯ÙˆØ±ÙŠØ©
ğŸ“Š ÙŠÙ†Ø¸Ù Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø© ÙˆØ§Ù„Ù†Ù…Ø§Ø°Ø¬ ØºÙŠØ± Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…Ø©
ğŸ’¾ ÙŠØ­Ø§ÙØ¸ Ø¹Ù„Ù‰ Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù†Ø¸Ø§Ù… ÙˆØªÙˆÙÙŠØ± Ø§Ù„Ù…Ø³Ø§Ø­Ø©
ğŸ—„ï¸ ÙŠØ³ØªØ®Ø¯Ù… Ù†Ø¸Ø§Ù… Ø§Ù„Ù†Ø³Ø® Ø§Ù„Ø§Ø­ØªÙŠØ§Ø·ÙŠØ© Ø§Ù„Ø°ÙƒÙŠ
"""

import os
import sqlite3
import json
import shutil
from datetime import datetime, timedelta
import logging
from smart_backup_manager import SmartBackupManager

# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s'
)
logger = logging.getLogger(__name__)

class SystemMaintenance:
    """Ù†Ø¸Ø§Ù… Ø§Ù„ØµÙŠØ§Ù†Ø© Ø§Ù„Ø¯ÙˆØ±ÙŠØ©"""
    
    def __init__(self, db_path=None):
        # Ø§Ù„Ø³Ù…Ø§Ø­ Ø¨ØªØ­Ø¯ÙŠØ¯ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        self.db_path = db_path or './data/forex_ml.db'
        self.models_dir = './trained_models'
        self.backup_dir = './backups'
        self.logs_dir = './logs'
        self.model_performance_file = './model_performance.json'
        
        # Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø§Ø­ØªÙØ§Ø¸ Ø¨Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        self.data_retention = {
            'M1': 90,    # 3 Ø´Ù‡ÙˆØ±
            'M5': 180,   # 6 Ø´Ù‡ÙˆØ±
            'M15': 365,  # Ø³Ù†Ø©
            'M30': 365,  # Ø³Ù†Ø©
            'H1': 730,   # Ø³Ù†ØªÙŠÙ†
            'H4': 730,   # Ø³Ù†ØªÙŠÙ†
            'D1': 1095   # 3 Ø³Ù†ÙˆØ§Øª
        }
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø¬Ù„Ø¯ Ø§Ù„Ù†Ø³Ø® Ø§Ù„Ø§Ø­ØªÙŠØ§Ø·ÙŠØ©
        os.makedirs(self.backup_dir, exist_ok=True)
        
        # Ù…Ø¯ÙŠØ± Ø§Ù„Ù†Ø³Ø® Ø§Ù„Ø§Ø­ØªÙŠØ§Ø·ÙŠØ© Ø§Ù„Ø°ÙƒÙŠ
        self.backup_manager = SmartBackupManager()
        
    def run_full_maintenance(self):
        """ØªØ´ØºÙŠÙ„ Ø§Ù„ØµÙŠØ§Ù†Ø© Ø§Ù„ÙƒØ§Ù…Ù„Ø©"""
        logger.info("="*70)
        logger.info("ğŸ§¹ Starting Full System Maintenance")
        logger.info("="*70)
        
        # 1. Ù†Ø³Ø® Ø§Ø­ØªÙŠØ§Ø·ÙŠ Ù‚Ø¨Ù„ Ø§Ù„ØµÙŠØ§Ù†Ø©
        self.backup_database()
        
        # 2. ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø©
        self.cleanup_old_data()
        
        # 3. ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø©
        self.cleanup_old_models()
        
        # 4. ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø³Ø¬Ù„Ø§Øª Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø©
        self.cleanup_old_logs()
        
        # 5. ØªØ­Ø³ÙŠÙ† Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        self.optimize_database()
        
        # 6. ØªÙ‚Ø±ÙŠØ± Ø§Ù„ØµÙŠØ§Ù†Ø©
        self.generate_maintenance_report()
        
        logger.info("âœ… Maintenance completed successfully!")
        
    def backup_database(self):
        """Ù†Ø³Ø® Ø§Ø­ØªÙŠØ§Ø·ÙŠ Ø°ÙƒÙŠ Ù„Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª"""
        try:
            if not os.path.exists(self.db_path):
                logger.warning("Database not found, skipping backup")
                return
            
            logger.info("ğŸ“¦ Creating smart backup...")
            
            # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…Ø¯ÙŠØ± Ø§Ù„Ù†Ø³Ø® Ø§Ù„Ø§Ø­ØªÙŠØ§Ø·ÙŠØ© Ø§Ù„Ø°ÙƒÙŠ
            backup_path = self.backup_manager.smart_backup(self.db_path)
            
            if backup_path:
                logger.info(f"âœ… Smart backup created successfully")
            else:
                logger.info("ğŸ“Š No backup needed (no changes detected)")
            
        except Exception as e:
            logger.error(f"âŒ Backup failed: {e}")
            
    def cleanup_old_data(self):
        """ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø© Ù…Ù† Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª"""
        logger.info("\nğŸ“Š Cleaning old price data...")
        
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            total_deleted = 0
            
            # Ø­Ø°Ù Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø© Ø­Ø³Ø¨ Ø§Ù„ÙØ±ÙŠÙ…
            for timeframe, days in self.data_retention.items():
                cutoff_date = datetime.now() - timedelta(days=days)
                
                # Ø¹Ø¯ Ø§Ù„Ø³Ø¬Ù„Ø§Øª Ù‚Ø¨Ù„ Ø§Ù„Ø­Ø°Ù
                cursor.execute(
                    "SELECT COUNT(*) FROM price_data WHERE timeframe = ? AND time < ?",
                    (timeframe, cutoff_date.strftime('%Y-%m-%d'))
                )
                count = cursor.fetchone()[0]
                
                if count > 0:
                    # Ø­Ø°Ù Ø§Ù„Ø³Ø¬Ù„Ø§Øª Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø©
                    cursor.execute(
                        "DELETE FROM price_data WHERE timeframe = ? AND time < ?",
                        (timeframe, cutoff_date.strftime('%Y-%m-%d'))
                    )
                    
                    logger.info(f"   âœ… {timeframe}: Deleted {count:,} records older than {days} days")
                    total_deleted += count
            
            # Ø­Ø°Ù Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ÙƒØ±Ø±Ø© (Ø¥Ù† ÙˆØ¬Ø¯Øª)
            cursor.execute("""
                DELETE FROM price_data 
                WHERE rowid NOT IN (
                    SELECT MIN(rowid) 
                    FROM price_data 
                    GROUP BY symbol, timeframe, time
                )
            """)
            duplicates = cursor.rowcount
            if duplicates > 0:
                logger.info(f"   âœ… Removed {duplicates:,} duplicate records")
                total_deleted += duplicates
            
            conn.commit()
            conn.close()
            
            logger.info(f"   ğŸ“Š Total records deleted: {total_deleted:,}")
            
        except Exception as e:
            logger.error(f"âŒ Data cleanup failed: {e}")
            
    def cleanup_old_models(self):
        """ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø© ÙˆØ§Ù„Ø§Ø­ØªÙØ§Ø¸ Ø¨Ø£ÙØ¶Ù„ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø­Ø³Ø¨ Ø§Ù„Ø£Ø¯Ø§Ø¡"""
        logger.info("\nğŸ¤– Cleaning old models and keeping best performers...")
        
        try:
            if not os.path.exists(self.models_dir):
                logger.warning("Models directory not found")
                return
            
            # Ù‚Ø±Ø§Ø¡Ø© Ø³Ø¬Ù„ Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬
            model_performance = self._load_model_performance()
            
            # Ø¬Ù…Ø¹ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù†Ù…Ø§Ø°Ø¬
            models_info = {}
            
            for file in os.listdir(self.models_dir):
                if file.endswith('.pkl') and 'scaler' not in file:
                    file_path = os.path.join(self.models_dir, file)
                    
                    # ØªØ­Ù„ÙŠÙ„ Ø§Ø³Ù… Ø§Ù„Ù…Ù„Ù
                    parts = file.replace('.pkl', '').split('_')
                    if len(parts) >= 3:
                        symbol = parts[0]
                        timeframe = parts[1]
                        model_type = '_'.join(parts[2:])
                        
                        # Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…Ù„Ù
                        file_stat = os.stat(file_path)
                        age_days = (datetime.now() - datetime.fromtimestamp(file_stat.st_mtime)).days
                        size_mb = file_stat.st_size / (1024 * 1024)
                        
                        # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
                        performance_key = f"{symbol}_{timeframe}_{model_type}"
                        performance = model_performance.get(performance_key, {})
                        accuracy = performance.get('accuracy', 0.5)
                        win_rate = performance.get('win_rate', 50)
                        profit_factor = performance.get('profit_factor', 1.0)
                        
                        # Ø­Ø³Ø§Ø¨ Ù†Ù‚Ø§Ø· Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù…Ø±ÙƒØ¨Ø©
                        performance_score = (accuracy * 100 * 0.4) + (win_rate * 0.3) + (profit_factor * 10 * 0.3)
                        
                        key = f"{symbol}_{timeframe}"
                        if key not in models_info:
                            models_info[key] = []
                        
                        models_info[key].append({
                            'file': file,
                            'path': file_path,
                            'model_type': model_type,
                            'age_days': age_days,
                            'size_mb': size_mb,
                            'modified': datetime.fromtimestamp(file_stat.st_mtime),
                            'accuracy': accuracy,
                            'win_rate': win_rate,
                            'profit_factor': profit_factor,
                            'performance_score': performance_score
                        })
            
            total_deleted = 0
            total_size_saved = 0
            kept_models = []
            
            # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ù„ÙƒÙ„ Ø²ÙˆØ¬
            for pair_key, models in models_info.items():
                logger.info(f"\n   ğŸ“Š Processing {pair_key}...")
                
                # ØªØ±ØªÙŠØ¨ Ø­Ø³Ø¨ Ø§Ù„Ø£Ø¯Ø§Ø¡ (Ø§Ù„Ø£ÙØ¶Ù„ Ø£ÙˆÙ„Ø§Ù‹)
                models.sort(key=lambda x: x['performance_score'], reverse=True)
                
                # Ø·Ø¨Ø§Ø¹Ø© Ø£ÙØ¶Ù„ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬
                if models:
                    logger.info(f"      Best model: {models[0]['model_type']} - Score: {models[0]['performance_score']:.2f}")
                
                # Ø§Ù„Ø§Ø­ØªÙØ§Ø¸ Ø¨Ø£ÙØ¶Ù„ 3 Ù†Ù…Ø§Ø°Ø¬ ÙÙ‚Ø· (Ù…Ø¹ Ø´Ø±ÙˆØ·)
                models_to_keep = []
                models_to_delete = []
                
                for i, model in enumerate(models):
                    # Ø§Ù„Ø§Ø­ØªÙØ§Ø¸ Ø¨Ø£ÙØ¶Ù„ 3 Ù†Ù…Ø§Ø°Ø¬ Ø¥Ø°Ø§ ÙƒØ§Ù†Øª:
                    # 1. Ù…Ù† Ø£ÙØ¶Ù„ 3 ÙÙŠ Ø§Ù„Ø£Ø¯Ø§Ø¡
                    # 2. Ø£Ø­Ø¯Ø« Ù…Ù† 60 ÙŠÙˆÙ…
                    # 3. Ù„Ù‡Ø§ performance_score > 50
                    if i < 3 and model['age_days'] < 60 and model['performance_score'] > 50:
                        models_to_keep.append(model)
                        kept_models.append({
                            'pair': pair_key,
                            'model': model['model_type'],
                            'score': model['performance_score'],
                            'accuracy': model['accuracy'],
                            'win_rate': model['win_rate']
                        })
                    else:
                        models_to_delete.append(model)
                
                # Ø­Ø°Ù Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ø¶Ø¹ÙŠÙØ© Ø£Ùˆ Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø©
                for model in models_to_delete:
                    reason = ""
                    if model['age_days'] > 60:
                        reason = f"too old ({model['age_days']} days)"
                    elif model['performance_score'] <= 50:
                        reason = f"low performance (score: {model['performance_score']:.1f})"
                    else:
                        reason = "excess model"
                    
                    os.remove(model['path'])
                    logger.info(f"      âŒ Deleted: {model['model_type']} - {reason}")
                    total_deleted += 1
                    total_size_saved += model['size_mb']
                
                # Ø¥Ø°Ø§ Ù„Ù… Ù†Ø­ØªÙØ¸ Ø¨Ø£ÙŠ Ù†Ù…ÙˆØ°Ø¬ØŒ Ø§Ø­ØªÙØ¸ Ø¨Ø§Ù„Ø£ÙØ¶Ù„ Ø­ØªÙ‰ Ù„Ùˆ ÙƒØ§Ù† Ø¶Ø¹ÙŠÙ
                if not models_to_keep and models:
                    best_model = models[0]
                    logger.info(f"      âš ï¸  Keeping best available model despite low performance")
                    models_to_delete.remove(best_model)
                    total_deleted -= 1
                    total_size_saved -= best_model['size_mb']
            
            # Ø­Ø°Ù Ù…Ù„ÙØ§Øª scalers Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø©
            for file in os.listdir(self.models_dir):
                if 'scaler' in file and file.endswith('.pkl'):
                    file_path = os.path.join(self.models_dir, file)
                    file_stat = os.stat(file_path)
                    age_days = (datetime.now() - datetime.fromtimestamp(file_stat.st_mtime)).days
                    
                    if age_days > 30:
                        os.remove(file_path)
                        logger.info(f"   âœ… Deleted old scaler: {file}")
                        total_deleted += 1
            
            logger.info(f"   ğŸ¤– Total models deleted: {total_deleted}")
            logger.info(f"   ğŸ’¾ Space saved: {total_size_saved:.2f} MB")
            
            # Ø­ÙØ¸ ØªÙ‚Ø±ÙŠØ± Ø¨Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…Ø­ØªÙØ¸ Ø¨Ù‡Ø§
            if kept_models:
                self._save_kept_models_report(kept_models)
            
        except Exception as e:
            logger.error(f"âŒ Model cleanup failed: {e}")
            
    def cleanup_old_logs(self):
        """ØªÙ†Ø¸ÙŠÙ Ù…Ù„ÙØ§Øª Ø§Ù„Ø³Ø¬Ù„Ø§Øª Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø©"""
        logger.info("\nğŸ“ Cleaning old logs...")
        
        try:
            log_files = [
                'enhanced_ml_server.log',
                'server.log',
                'advanced_ml_server.log',
                'server_output.log'
            ]
            
            total_cleaned = 0
            
            for log_file in log_files:
                if os.path.exists(log_file):
                    file_size = os.path.getsize(log_file) / (1024 * 1024)  # MB
                    
                    if file_size > 100:  # Ø¥Ø°Ø§ ÙƒØ§Ù† Ø£ÙƒØ¨Ø± Ù…Ù† 100 Ù…ÙŠØ¬Ø§
                        # Ø§Ø­ØªÙØ¸ Ø¨Ø¢Ø®Ø± 1000 Ø³Ø·Ø± ÙÙ‚Ø·
                        with open(log_file, 'r') as f:
                            lines = f.readlines()
                        
                        if len(lines) > 1000:
                            with open(log_file, 'w') as f:
                                f.writelines(lines[-1000:])
                            
                            logger.info(f"   âœ… Truncated {log_file} (was {file_size:.1f} MB)")
                            total_cleaned += 1
            
            # Ø­Ø°Ù Ù…Ù„ÙØ§Øª nohup.out Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø©
            if os.path.exists('nohup.out'):
                os.remove('nohup.out')
                logger.info("   âœ… Deleted nohup.out")
                total_cleaned += 1
            
            logger.info(f"   ğŸ“ Total log files cleaned: {total_cleaned}")
            
        except Exception as e:
            logger.error(f"âŒ Log cleanup failed: {e}")
            
    def optimize_database(self):
        """ØªØ­Ø³ÙŠÙ† Ø£Ø¯Ø§Ø¡ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª"""
        logger.info("\nğŸ”§ Optimizing database...")
        
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            # Ø­Ø¬Ù… Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù‚Ø¨Ù„ Ø§Ù„ØªØ­Ø³ÙŠÙ†
            cursor.execute("SELECT page_count * page_size / 1024 / 1024 FROM pragma_page_count(), pragma_page_size()")
            size_before = cursor.fetchone()[0]
            
            # ØªØ´ØºÙŠÙ„ VACUUM Ù„ØªØ­Ø³ÙŠÙ† ÙˆØ¶ØºØ· Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
            cursor.execute("VACUUM")
            
            # Ø¥Ø¹Ø§Ø¯Ø© Ø¨Ù†Ø§Ø¡ Ø§Ù„ÙÙ‡Ø§Ø±Ø³
            cursor.execute("REINDEX")
            
            # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¬Ø¯Ø§ÙˆÙ„ Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª
            cursor.execute("ANALYZE")
            
            # Ø­Ø¬Ù… Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ø¹Ø¯ Ø§Ù„ØªØ­Ø³ÙŠÙ†
            cursor.execute("SELECT page_count * page_size / 1024 / 1024 FROM pragma_page_count(), pragma_page_size()")
            size_after = cursor.fetchone()[0]
            
            conn.close()
            
            space_saved = size_before - size_after
            logger.info(f"   âœ… Database optimized")
            logger.info(f"   ğŸ’¾ Size before: {size_before:.2f} MB")
            logger.info(f"   ğŸ’¾ Size after: {size_after:.2f} MB")
            logger.info(f"   ğŸ’¾ Space saved: {space_saved:.2f} MB")
            
        except Exception as e:
            logger.error(f"âŒ Database optimization failed: {e}")
            
    def _cleanup_old_backups(self):
        """Ø­Ø°Ù Ø§Ù„Ù†Ø³Ø® Ø§Ù„Ø§Ø­ØªÙŠØ§Ø·ÙŠØ© Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø© - Ù„Ù… ØªØ¹Ø¯ Ù…Ø·Ù„ÙˆØ¨Ø© Ù…Ø¹ Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ø°ÙƒÙŠ"""
        # Ù‡Ø°Ù‡ Ø§Ù„Ø¯Ø§Ù„Ø© Ù„Ù… ØªØ¹Ø¯ Ù…Ø·Ù„ÙˆØ¨Ø© Ù„Ø£Ù† SmartBackupManager ÙŠØªÙˆÙ„Ù‰ Ø§Ù„ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø°ÙƒÙŠ
        pass
            
    def generate_maintenance_report(self):
        """ØªÙˆÙ„ÙŠØ¯ ØªÙ‚Ø±ÙŠØ± Ø§Ù„ØµÙŠØ§Ù†Ø©"""
        logger.info("\nğŸ“Š Generating maintenance report...")
        
        try:
            report = {
                'timestamp': datetime.now().isoformat(),
                'database_size': 0,
                'models_count': 0,
                'total_records': 0,
                'disk_usage': {}
            }
            
            # Ø­Ø¬Ù… Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
            if os.path.exists(self.db_path):
                report['database_size'] = os.path.getsize(self.db_path) / (1024 * 1024)
                
                # Ø¹Ø¯Ø¯ Ø§Ù„Ø³Ø¬Ù„Ø§Øª
                conn = sqlite3.connect(self.db_path)
                cursor = conn.cursor()
                cursor.execute("SELECT COUNT(*) FROM price_data")
                report['total_records'] = cursor.fetchone()[0]
                
                # Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ù„ÙƒÙ„ Ø²ÙˆØ¬
                cursor.execute("""
                    SELECT symbol, timeframe, COUNT(*) as count 
                    FROM price_data 
                    GROUP BY symbol, timeframe 
                    ORDER BY count DESC 
                    LIMIT 10
                """)
                report['top_pairs'] = cursor.fetchall()
                conn.close()
            
            # Ø¹Ø¯Ø¯ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬
            if os.path.exists(self.models_dir):
                report['models_count'] = len([f for f in os.listdir(self.models_dir) if f.endswith('.pkl')])
            
            # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù‚Ø±Øµ
            report['disk_usage'] = {
                'database_mb': report['database_size'],
                'models_mb': sum(os.path.getsize(os.path.join(self.models_dir, f)) 
                               for f in os.listdir(self.models_dir) if f.endswith('.pkl')) / (1024 * 1024) if os.path.exists(self.models_dir) else 0,
                'backups_mb': sum(os.path.getsize(os.path.join(self.backup_dir, f)) 
                               for f in os.listdir(self.backup_dir)) / (1024 * 1024) if os.path.exists(self.backup_dir) else 0
            }
            
            # Ø­ÙØ¸ Ø§Ù„ØªÙ‚Ø±ÙŠØ±
            report_path = f"maintenance_report_{datetime.now().strftime('%Y%m%d')}.json"
            with open(report_path, 'w') as f:
                json.dump(report, f, indent=2)
            
            logger.info(f"   âœ… Report saved to: {report_path}")
            logger.info(f"   ğŸ“Š Database: {report['database_size']:.2f} MB ({report['total_records']:,} records)")
            logger.info(f"   ğŸ¤– Models: {report['models_count']} files ({report['disk_usage']['models_mb']:.2f} MB)")
            logger.info(f"   ğŸ’¾ Total disk usage: {sum(report['disk_usage'].values()):.2f} MB")
            
        except Exception as e:
            logger.error(f"âŒ Report generation failed: {e}")

    def _load_model_performance(self):
        """Ù‚Ø±Ø§Ø¡Ø© Ø³Ø¬Ù„ Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬"""
        if os.path.exists(self.model_performance_file):
            try:
                with open(self.model_performance_file, 'r') as f:
                    return json.load(f)
            except:
                pass
        return {}
    
    def _save_kept_models_report(self, kept_models):
        """Ø­ÙØ¸ ØªÙ‚Ø±ÙŠØ± Ø¨Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…Ø­ØªÙØ¸ Ø¨Ù‡Ø§"""
        report_file = f"kept_models_report_{datetime.now().strftime('%Y%m%d')}.json"
        with open(report_file, 'w') as f:
            json.dump({
                'timestamp': datetime.now().isoformat(),
                'kept_models': kept_models,
                'total_models': len(kept_models)
            }, f, indent=2)
        logger.info(f"   ğŸ“Š Kept models report saved to: {report_file}")

def main():
    """ØªØ´ØºÙŠÙ„ Ø§Ù„ØµÙŠØ§Ù†Ø©"""
    # Ø§Ù„Ø³Ù…Ø§Ø­ Ø¨ØªØ­Ø¯ÙŠØ¯ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
    import argparse
    parser = argparse.ArgumentParser(description='System Maintenance Script')
    parser.add_argument('--db', help='Database path', default='./data/forex_ml.db')
    parser.add_argument('--data', action='store_true', help='Clean old data only')
    parser.add_argument('--models', action='store_true', help='Clean old models only')
    parser.add_argument('--logs', action='store_true', help='Clean old logs only')
    parser.add_argument('--optimize', action='store_true', help='Optimize database only')
    parser.add_argument('--backup', action='store_true', help='Backup only')
    parser.add_argument('--list-dbs', action='store_true', help='List available databases')
    
    args = parser.parse_args()
    
    # Ø¹Ø±Ø¶ Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªØ§Ø­Ø©
    if args.list_dbs:
        print("\nğŸ“Š Available databases:")
        if os.path.exists('./data'):
            for file in os.listdir('./data'):
                if file.endswith('.db'):
                    size_mb = os.path.getsize(f'./data/{file}') / (1024 * 1024)
                    print(f"   - ./data/{file} ({size_mb:.2f} MB)")
        return
    
    maintenance = SystemMaintenance(db_path=args.db)
    
    # ØªÙ†ÙÙŠØ° Ø§Ù„Ø£ÙˆØ§Ù…Ø±
    if args.data:
        maintenance.cleanup_old_data()
    elif args.models:
        maintenance.cleanup_old_models()
    elif args.logs:
        maintenance.cleanup_old_logs()
    elif args.optimize:
        maintenance.optimize_database()
    elif args.backup:
        maintenance.backup_database()
    else:
        # ØªØ´ØºÙŠÙ„ Ø§Ù„ØµÙŠØ§Ù†Ø© Ø§Ù„ÙƒØ§Ù…Ù„Ø©
        maintenance.run_full_maintenance()
        
        # Ø¹Ø±Ø¶ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù†Ø³Ø® Ø§Ù„Ø§Ø­ØªÙŠØ§Ø·ÙŠØ©
        logger.info("\nğŸ“¦ Backup Information:")
        backups = maintenance.backup_manager.list_backups()
        logger.info(f"   Total backups: {len(backups)}")
        if backups:
            total_size = sum(b['size_mb'] for b in backups)
            logger.info(f"   Total backup size: {total_size:.1f} MB")
            logger.info(f"   Oldest backup: {backups[-1]['age_days']} days old")
            logger.info(f"   Newest backup: {backups[0]['age_days']} days old")

if __name__ == "__main__":
    main()