#!/usr/bin/env python3
"""
Advanced Training System for 95% Accuracy
Ù†Ø¸Ø§Ù… Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù…ØªÙ‚Ø¯Ù… Ù„Ù„ÙˆØµÙˆÙ„ Ù„Ø¯Ù‚Ø© 95%
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, TimeSeriesSplit
from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.ensemble import RandomForestClassifier, VotingClassifier
import lightgbm as lgb
import xgboost as xgb
try:
    import catboost as cb
    CATBOOST_AVAILABLE = True
except:
    CATBOOST_AVAILABLE = False
    
try:
    import optuna
    OPTUNA_AVAILABLE = True
except:
    OPTUNA_AVAILABLE = False

import joblib
import sqlite3
from datetime import datetime
import json
import os
from pathlib import Path
import warnings
warnings.filterwarnings('ignore')

# Import our feature engineer
from feature_engineer_fixed_v2 import FeatureEngineer

class AdvancedModelTrainer:
    """Ù†Ø¸Ø§Ù… Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù…ØªÙ‚Ø¯Ù… Ù„Ù„ÙˆØµÙˆÙ„ Ù„Ø¯Ù‚Ø© Ø¹Ø§Ù„ÙŠØ© Ø¬Ø¯Ø§Ù‹"""
    
    def __init__(self):
        self.models = {}
        self.scalers = {}
        self.feature_importance = {}
        self.performance_metrics = {}
        Path("models/advanced").mkdir(parents=True, exist_ok=True)
        Path("logs").mkdir(exist_ok=True)
        
    def prepare_data(self, symbol, timeframe):
        """ØªØ­Ø¶ÙŠØ± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ø¹ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø©"""
        print(f"\nðŸ“Š Preparing data for {symbol} {timeframe}...")
        
        # Ø¬Ù„Ø¨ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        conn = sqlite3.connect("data/forex_ml.db")
        query = """
            SELECT time, open, high, low, close, volume, spread
            FROM price_data
            WHERE symbol = ? AND timeframe = ?
            ORDER BY time
        """
        df = pd.read_sql_query(query, conn, params=(symbol, timeframe))
        conn.close()
        
        if len(df) < 1000:
            print(f"âš ï¸ Not enough data for {symbol} {timeframe}")
            return None, None, None
            
        # ØªØ­ÙˆÙŠÙ„ Ø§Ù„ÙˆÙ‚Øª
        df['datetime'] = pd.to_datetime(df['time'], unit='s')
        df.set_index('datetime', inplace=True)
        
        # Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ù…Ø¤Ø´Ø±Ø§Øª
        print("  â€¢ Adding advanced technical indicators...")
        engineer = FeatureEngineer()
        df_features = engineer.create_features(
            df, 
            target_config={'lookahead': 5, 'threshold': 0.001}
        )
        
        if df_features.empty or 'target' not in df_features.columns:
            return None, None, None
            
        # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØµÙÙˆÙ Ø¨Ø¯ÙˆÙ† Ù‡Ø¯Ù ÙˆØ§Ø¶Ø­
        df_features = df_features[df_features['target'] != 0]
        
        # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù‡Ø¯Ù Ù„Ø«Ù†Ø§Ø¦ÙŠ (1 Ù„Ù„ØµØ¹ÙˆØ¯ØŒ 0 Ù„Ù„Ù‡Ø¨ÙˆØ·)
        df_features['target_binary'] = (df_features['target'] > 0).astype(int)
        
        # Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„Ù…ÙŠØ²Ø§Øª
        feature_cols = [col for col in df_features.columns 
                       if col not in ['target', 'target_binary', 'target_3class', 
                                     'future_return', 'time', 'open', 'high', 
                                     'low', 'close', 'volume', 'spread']]
        
        X = df_features[feature_cols]
        y = df_features['target_binary']
        
        print(f"  â€¢ Total samples: {len(X)}")
        print(f"  â€¢ Features: {len(feature_cols)}")
        print(f"  â€¢ Class distribution: {y.value_counts().to_dict()}")
        
        return X, y, feature_cols
        
    def optimize_lgb_params(self, X_train, y_train, X_val, y_val):
        """ØªØ­Ø³ÙŠÙ† Ù…Ø¹Ø§Ù…Ù„Ø§Øª LightGBM ØªÙ„Ù‚Ø§Ø¦ÙŠØ§Ù‹"""
        if not OPTUNA_AVAILABLE:
            return {
                'n_estimators': 300,
                'learning_rate': 0.05,
                'num_leaves': 31,
                'max_depth': -1,
                'min_child_samples': 20,
                'subsample': 0.8,
                'colsample_bytree': 0.8,
                'random_state': 42
            }
            
        print("  â€¢ Optimizing LightGBM parameters with Optuna...")
        
        def objective(trial):
            params = {
                'n_estimators': trial.suggest_int('n_estimators', 100, 1000),
                'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),
                'num_leaves': trial.suggest_int('num_leaves', 10, 100),
                'max_depth': trial.suggest_int('max_depth', 3, 20),
                'min_child_samples': trial.suggest_int('min_child_samples', 5, 50),
                'subsample': trial.suggest_float('subsample', 0.5, 1.0),
                'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),
                'random_state': 42,
                'verbose': -1
            }
            
            model = lgb.LGBMClassifier(**params)
            model.fit(X_train, y_train)
            preds = model.predict(X_val)
            accuracy = accuracy_score(y_val, preds)
            
            return accuracy
            
        study = optuna.create_study(direction='maximize')
        study.optimize(objective, n_trials=50, show_progress_bar=True)
        
        best_params = study.best_params
        best_params['random_state'] = 42
        best_params['verbose'] = -1
        
        return best_params
        
    def train_ensemble_model(self, X, y, symbol, timeframe):
        """ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬ Ù…Ø¬Ù…Ø¹ Ù‚ÙˆÙŠ Ø¬Ø¯Ø§Ù‹"""
        print(f"\nðŸš€ Training advanced ensemble for {symbol} {timeframe}...")
        
        # ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42, stratify=y
        )
        
        # ØªÙ‚Ø³ÙŠÙ… Ø¥Ø¶Ø§ÙÙŠ Ù„Ù„ØªØ­Ù‚Ù‚
        X_train, X_val, y_train, y_val = train_test_split(
            X_train, y_train, test_size=0.2, random_state=42, stratify=y_train
        )
        
        # ØªØ­Ø¬ÙŠÙ… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        scaler = RobustScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        X_val_scaled = scaler.transform(X_val)
        X_test_scaled = scaler.transform(X_test)
        
        # Ø­ÙØ¸ Ø§Ù„Ù…Ø­Ø¬Ù…
        self.scalers[f"{symbol}_{timeframe}"] = scaler
        
        # 1. LightGBM Ù…Ø¹ Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ù…Ø­Ø³Ù†Ø©
        print("\n  ðŸ“ˆ Training LightGBM...")
        lgb_params = self.optimize_lgb_params(X_train, y_train, X_val, y_val)
        lgb_model = lgb.LGBMClassifier(**lgb_params)
        lgb_model.fit(X_train, y_train)
        
        # 2. XGBoost
        print("  ðŸ“Š Training XGBoost...")
        xgb_model = xgb.XGBClassifier(
            n_estimators=300,
            learning_rate=0.05,
            max_depth=6,
            subsample=0.8,
            colsample_bytree=0.8,
            random_state=42,
            use_label_encoder=False,
            eval_metric='logloss'
        )
        xgb_model.fit(X_train, y_train)
        
        # 3. CatBoost (Ø¥Ø°Ø§ ÙƒØ§Ù† Ù…ØªØ§Ø­Ø§Ù‹)
        models_list = [
            ('lgb', lgb_model),
            ('xgb', xgb_model)
        ]
        
        if CATBOOST_AVAILABLE:
            print("  ðŸ± Training CatBoost...")
            cb_model = cb.CatBoostClassifier(
                iterations=300,
                learning_rate=0.05,
                depth=6,
                random_state=42,
                verbose=False
            )
            cb_model.fit(X_train, y_train)
            models_list.append(('catboost', cb_model))
        
        # 4. Random Forest
        print("  ðŸŒ² Training Random Forest...")
        rf_model = RandomForestClassifier(
            n_estimators=200,
            max_depth=15,
            min_samples_split=10,
            random_state=42,
            n_jobs=-1
        )
        rf_model.fit(X_train_scaled, y_train)
        models_list.append(('rf', rf_model))
        
        # 5. Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¬Ù…Ø¹ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ
        print("  ðŸŽ¯ Creating final ensemble...")
        ensemble = VotingClassifier(
            estimators=models_list,
            voting='soft'
        )
        
        # ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¬Ù…Ø¹
        ensemble.fit(X_train, y_train)
        
        # ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ø£Ø¯Ø§Ø¡
        print("\nðŸ“Š Evaluating performance...")
        predictions = ensemble.predict(X_test)
        proba = ensemble.predict_proba(X_test)
        
        # Ø­Ø³Ø§Ø¨ Ø§Ù„Ù…Ù‚Ø§ÙŠÙŠØ³
        accuracy = accuracy_score(y_test, predictions)
        precision = precision_score(y_test, predictions)
        recall = recall_score(y_test, predictions)
        f1 = f1_score(y_test, predictions)
        
        # Ø­Ø³Ø§Ø¨ Ø§Ù„Ø«Ù‚Ø©
        confidence_scores = np.max(proba, axis=1)
        high_confidence_mask = confidence_scores > 0.7
        high_conf_accuracy = accuracy_score(
            y_test[high_confidence_mask], 
            predictions[high_confidence_mask]
        ) if sum(high_confidence_mask) > 0 else 0
        
        metrics = {
            'accuracy': accuracy,
            'precision': precision,
            'recall': recall,
            'f1_score': f1,
            'high_confidence_accuracy': high_conf_accuracy,
            'high_confidence_trades': sum(high_confidence_mask),
            'total_test_samples': len(y_test)
        }
        
        print(f"\nâœ… Results for {symbol} {timeframe}:")
        print(f"  â€¢ Overall Accuracy: {accuracy:.2%}")
        print(f"  â€¢ Precision: {precision:.2%}")
        print(f"  â€¢ Recall: {recall:.2%}")
        print(f"  â€¢ F1 Score: {f1:.2%}")
        print(f"  â€¢ High Confidence Accuracy: {high_conf_accuracy:.2%}")
        print(f"  â€¢ High Confidence Trades: {sum(high_confidence_mask)}/{len(y_test)}")
        
        # Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙˆØ§Ù„Ù…Ù‚Ø§ÙŠÙŠØ³
        self.models[f"{symbol}_{timeframe}"] = ensemble
        self.performance_metrics[f"{symbol}_{timeframe}"] = metrics
        
        # Ø­Ø³Ø§Ø¨ Ø£Ù‡Ù…ÙŠØ© Ø§Ù„Ù…ÙŠØ²Ø§Øª
        feature_importance = pd.DataFrame({
            'feature': X.columns,
            'importance': lgb_model.feature_importances_
        }).sort_values('importance', ascending=False)
        
        self.feature_importance[f"{symbol}_{timeframe}"] = feature_importance
        
        return ensemble, metrics
        
    def save_models(self):
        """Ø­ÙØ¸ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ ÙˆØ§Ù„Ø¨ÙŠØ§Ù†Ø§Øª"""
        print("\nðŸ’¾ Saving advanced models...")
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # Ø­ÙØ¸ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬
        for key, model in self.models.items():
            model_path = f"models/advanced/{key}_ensemble_{timestamp}.pkl"
            joblib.dump({
                'model': model,
                'scaler': self.scalers.get(key),
                'metrics': self.performance_metrics.get(key),
                'feature_importance': self.feature_importance.get(key),
                'timestamp': timestamp
            }, model_path)
            print(f"  â€¢ Saved: {model_path}")
        
        # Ø­ÙØ¸ Ù…Ù„Ø®Øµ Ø§Ù„Ø£Ø¯Ø§Ø¡
        summary = {
            'timestamp': timestamp,
            'models': list(self.models.keys()),
            'overall_metrics': self.performance_metrics,
            'average_accuracy': np.mean([m['accuracy'] for m in self.performance_metrics.values()]),
            'average_high_conf_accuracy': np.mean([m['high_confidence_accuracy'] for m in self.performance_metrics.values()])
        }
        
        with open(f"models/advanced/training_summary_{timestamp}.json", 'w') as f:
            json.dump(summary, f, indent=2)
            
        print(f"\nðŸŽ¯ Average Accuracy: {summary['average_accuracy']:.2%}")
        print(f"ðŸŽ¯ Average High-Confidence Accuracy: {summary['average_high_conf_accuracy']:.2%}")
        
    def train_all_pairs(self):
        """ØªØ¯Ø±ÙŠØ¨ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø£Ø²ÙˆØ§Ø¬"""
        print("="*80)
        print("ðŸ§  ADVANCED TRAINING SYSTEM FOR 95% ACCURACY")
        print("="*80)
        
        # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø£Ø²ÙˆØ§Ø¬
        conn = sqlite3.connect("data/forex_ml.db")
        cursor = conn.execute("""
            SELECT DISTINCT symbol, timeframe, COUNT(*) as count
            FROM price_data
            GROUP BY symbol, timeframe
            HAVING count > 1000
            ORDER BY count DESC
        """)
        pairs = cursor.fetchall()
        conn.close()
        
        print(f"\nðŸ“Š Found {len(pairs)} pairs to train")
        
        success_count = 0
        for symbol, timeframe, count in pairs:
            print(f"\n{'='*60}")
            print(f"Processing {symbol} {timeframe} ({count:,} bars)")
            print(f"{'='*60}")
            
            try:
                # ØªØ­Ø¶ÙŠØ± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
                X, y, features = self.prepare_data(symbol, timeframe)
                
                if X is not None and len(X) > 500:
                    # ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
                    model, metrics = self.train_ensemble_model(X, y, symbol, timeframe)
                    success_count += 1
                else:
                    print("âŒ Insufficient data for training")
                    
            except Exception as e:
                print(f"âŒ Error training {symbol} {timeframe}: {e}")
                
        # Ø­ÙØ¸ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬
        if success_count > 0:
            self.save_models()
            
        print("\n" + "="*80)
        print(f"âœ… Training completed: {success_count}/{len(pairs)} models")
        print("="*80)

def main():
    trainer = AdvancedModelTrainer()
    trainer.train_all_pairs()

if __name__ == "__main__":
    main()