#!/usr/bin/env python3
"""
ğŸ” ÙØ­Øµ ÙˆØªØ¯Ø±ÙŠØ¨ Ù…Ù† Ø¬Ø¯ÙˆÙ„ price_data
ğŸ“Š Ù…Ø¹Ø§Ù„Ø¬Ø© 7.8 Ù…Ù„ÙŠÙˆÙ† Ø³Ø¬Ù„
"""

import os
import sys
import sqlite3
import pandas as pd
import numpy as np
import logging
from datetime import datetime
import joblib

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s'
)
logger = logging.getLogger(__name__)

def inspect_price_data():
    """ÙØ­Øµ Ø¨Ù†ÙŠØ© Ø¬Ø¯ÙˆÙ„ price_data"""
    db_path = './data/forex_ml.db'
    logger.info(f"ğŸ” ÙØ­Øµ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: {db_path}")
    
    try:
        conn = sqlite3.connect(db_path)
        
        # ÙØ­Øµ Ø¨Ù†ÙŠØ© Ø§Ù„Ø¬Ø¯ÙˆÙ„
        cursor = conn.cursor()
        cursor.execute("PRAGMA table_info(price_data)")
        columns = cursor.fetchall()
        
        logger.info("\nğŸ“‹ Ø£Ø¹Ù…Ø¯Ø© Ø¬Ø¯ÙˆÙ„ price_data:")
        for col in columns:
            logger.info(f"   - {col[1]} ({col[2]})")
        
        # Ø¹ÙŠÙ†Ø© Ù…Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        query = "SELECT * FROM price_data LIMIT 10"
        df_sample = pd.read_sql_query(query, conn)
        logger.info(f"\nğŸ“Š Ø¹ÙŠÙ†Ø© Ù…Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª:")
        logger.info(df_sample)
        
        # Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø£Ø²ÙˆØ§Ø¬
        if 'symbol' in df_sample.columns:
            query = "SELECT symbol, COUNT(*) as count FROM price_data GROUP BY symbol ORDER BY count DESC"
            pairs_stats = pd.read_sql_query(query, conn)
            logger.info(f"\nğŸ“Š Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø£Ø²ÙˆØ§Ø¬:")
            for _, row in pairs_stats.head(20).iterrows():
                logger.info(f"   - {row['symbol']}: {row['count']:,} Ø³Ø¬Ù„")
        
        # Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„ÙØ±ÙŠÙ…Ø§Øª
        if 'timeframe' in df_sample.columns:
            query = "SELECT timeframe, COUNT(*) as count FROM price_data GROUP BY timeframe"
            tf_stats = pd.read_sql_query(query, conn)
            logger.info(f"\nâ° Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„ÙØ±ÙŠÙ…Ø§Øª:")
            for _, row in tf_stats.iterrows():
                logger.info(f"   - {row['timeframe']}: {row['count']:,} Ø³Ø¬Ù„")
        
        conn.close()
        return df_sample.columns.tolist()
        
    except Exception as e:
        logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ÙØ­Øµ: {str(e)}")
        return []

def train_from_price_data():
    """ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ù…Ù† Ø¬Ø¯ÙˆÙ„ price_data"""
    db_path = './data/forex_ml.db'
    
    try:
        # Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ù†Ø¸Ø§Ù…
        from optimized_forex_server import OptimizedForexSystem
        system = OptimizedForexSystem()
        system.db_path = db_path
        
        conn = sqlite3.connect(db_path)
        
        # Ø¬Ù„Ø¨ Ø§Ù„Ø£Ø²ÙˆØ§Ø¬ Ø§Ù„Ù…ØªØ§Ø­Ø©
        query = """
        SELECT symbol, COUNT(*) as count 
        FROM price_data 
        WHERE symbol NOT LIKE '%BRL%' 
        AND symbol NOT LIKE '%RUB%'
        AND symbol NOT LIKE '%ZAR%'
        GROUP BY symbol 
        HAVING count > 1000
        ORDER BY count DESC
        LIMIT 20
        """
        
        pairs = pd.read_sql_query(query, conn)
        logger.info(f"\nğŸ¯ ÙˆØ¬Ø¯Øª {len(pairs)} Ø²ÙˆØ¬ Ù„Ù„ØªØ¯Ø±ÙŠØ¨")
        
        trained_count = 0
        
        for _, pair_row in pairs.iterrows():
            symbol = pair_row['symbol']
            count = pair_row['count']
            
            logger.info(f"\n{'='*60}")
            logger.info(f"ğŸ¯ ØªØ¯Ø±ÙŠØ¨ {symbol} ({count:,} Ø³Ø¬Ù„)")
            
            try:
                # Ø¬Ù„Ø¨ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø²ÙˆØ¬
                query = f"""
                SELECT * FROM price_data 
                WHERE symbol = '{symbol}'
                ORDER BY time DESC
                LIMIT 10000
                """
                
                df = pd.read_sql_query(query, conn)
                
                if df.empty:
                    logger.warning(f"âš ï¸ Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù€ {symbol}")
                    continue
                
                # ØªØ­ÙˆÙŠÙ„ Ø§Ù„ÙˆÙ‚Øª
                if 'time' in df.columns:
                    df['time'] = pd.to_datetime(df['time'])
                    df.set_index('time', inplace=True)
                
                # Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©
                required_cols = ['open', 'high', 'low', 'close']
                if all(col in df.columns for col in required_cols):
                    # ØªØ­ÙˆÙŠÙ„ Ù„Ù„Ø£Ø±Ù‚Ø§Ù…
                    for col in required_cols:
                        df[col] = pd.to_numeric(df[col], errors='coerce')
                    
                    # Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ø­Ø¬Ù… Ø¥Ø°Ø§ Ù„Ù… ÙŠÙƒÙ† Ù…ÙˆØ¬ÙˆØ¯
                    if 'volume' not in df.columns:
                        df['volume'] = 1000
                    
                    # Ø¥Ø²Ø§Ù„Ø© NaN
                    df = df.dropna()
                    
                    if len(df) < 1000:
                        logger.warning(f"âš ï¸ Ø¨ÙŠØ§Ù†Ø§Øª ØºÙŠØ± ÙƒØ§ÙÙŠØ© Ø¨Ø¹Ø¯ Ø§Ù„ØªÙ†Ø¸ÙŠÙ: {len(df)}")
                        continue
                    
                    # Ø­ÙØ¸ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©
                    processed_table = f"{symbol}_processed"
                    df.to_sql(processed_table, conn, if_exists='replace', index=True)
                    logger.info(f"âœ… ØªÙ… Ø­ÙØ¸ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© ÙÙŠ {processed_table}")
                    
                    # ØªØ¯Ø±ÙŠØ¨ Ù„ÙØ±ÙŠÙ…Ø§Øª Ù…Ø®ØªÙ„ÙØ©
                    timeframes = ['M15', 'M30', 'H1', 'H4', 'D1']
                    
                    for tf in timeframes:
                        try:
                            # Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ù„ØªØ¯Ø±ÙŠØ¨
                            logger.info(f"   ğŸ¤– ØªØ¯Ø±ÙŠØ¨ {symbol} {tf}...")
                            
                            # Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø³ÙŠØ· Ø¥Ø°Ø§ ÙØ´Ù„ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„ÙƒØ§Ù…Ù„
                            success = train_simple_model(system, symbol, tf, df)
                            
                            if success:
                                trained_count += 1
                                logger.info(f"   âœ… ØªÙ… ØªØ¯Ø±ÙŠØ¨ {symbol} {tf}")
                            else:
                                logger.warning(f"   âš ï¸ ÙØ´Ù„ ØªØ¯Ø±ÙŠØ¨ {symbol} {tf}")
                                
                        except Exception as e:
                            logger.error(f"   âŒ Ø®Ø·Ø£ ÙÙŠ {symbol} {tf}: {str(e)}")
                            
                else:
                    logger.warning(f"âš ï¸ Ø£Ø¹Ù…Ø¯Ø© Ù†Ø§Ù‚ØµØ© ÙÙŠ {symbol}")
                    
            except Exception as e:
                logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© {symbol}: {str(e)}")
        
        conn.close()
        
        logger.info(f"\nâœ… ØªÙ… ØªØ¯Ø±ÙŠØ¨ {trained_count} Ù†Ù…ÙˆØ°Ø¬")
        return trained_count
        
    except Exception as e:
        logger.error(f"âŒ Ø®Ø·Ø£ Ø¹Ø§Ù…: {str(e)}")
        return 0

def train_simple_model(system, symbol, timeframe, df):
    """ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø³ÙŠØ·"""
    try:
        from sklearn.ensemble import RandomForestClassifier
        from sklearn.preprocessing import RobustScaler
        from sklearn.model_selection import train_test_split
        
        # Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø£Ù† df Ù„Ù‡ index
        if not isinstance(df.index, pd.DatetimeIndex) and 'time' in df.columns:
            df['time'] = pd.to_datetime(df['time'])
            df.set_index('time', inplace=True)
        
        # Ø­Ø³Ø§Ø¨ Ù…ÙŠØ²Ø§Øª Ø¨Ø³ÙŠØ·Ø© Ù…Ø¹ Ø§Ù„Ø­ÙØ§Ø¸ Ø¹Ù„Ù‰ Ù†ÙØ³ Ø§Ù„Ù€ index
        features = pd.DataFrame(index=df.index)
        
        # Ø§Ù„Ù…ØªÙˆØ³Ø·Ø§Øª Ø§Ù„Ù…ØªØ­Ø±ÙƒØ©
        features['sma_20'] = df['close'].rolling(20).mean()
        features['sma_50'] = df['close'].rolling(50).mean()
        features['sma_200'] = df['close'].rolling(200).mean()
        
        # RSI
        delta = df['close'].diff()
        gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()
        loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()
        rs = gain / loss
        features['rsi'] = 100 - (100 / (1 + rs))
        
        # Ø§Ù„ØªØºÙŠØ±Ø§Øª
        features['price_change'] = df['close'].pct_change()
        features['high_low_ratio'] = df['high'] / df['low']
        
        # Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ø£Ø³Ø¹Ø§Ø± Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
        features['open'] = df['open']
        features['high'] = df['high']
        features['low'] = df['low']
        features['close'] = df['close']
        
        # Ø¥Ø²Ø§Ù„Ø© NaN
        features = features.dropna()
        
        if len(features) < 100:
            return False
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù‡Ø¯Ù
        y = (df['close'].shift(-1) > df['close']).astype(int)
        
        # Ù…Ø­Ø§Ø°Ø§Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª - Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø£Ù† features Ùˆ y Ù„Ù‡Ù…Ø§ Ù†ÙØ³ Ø§Ù„Ù€ index
        common_index = features.index.intersection(y.index).intersection(df.index)
        X = features.loc[common_index]
        y = y.loc[common_index]
        df_aligned = df.loc[common_index]
        
        if len(X) < 100:
            return False
        
        # Ø¥Ø²Ø§Ù„Ø© Ø¢Ø®Ø± ØµÙ (Ù„Ø£Ù† shift(-1) ÙŠØ®Ù„Ù‚ NaN ÙÙŠ Ø¢Ø®Ø± ØµÙ)
        X = X[:-1]
        y = y[:-1]
        
        # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„ØªØ·Ø§Ø¨Ù‚
        if len(X) != len(y):
            logger.error(f"Ø¹Ø¯Ù… ØªØ·Ø§Ø¨Ù‚: X={len(X)}, y={len(y)}")
            return False
            
        # ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        X_train, X_test, y_train, y_test = train_test_split(
            X.values, y.values, test_size=0.2, random_state=42
        )
        
        # ØªØ·Ø¨ÙŠØ¹
        scaler = RobustScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.transform(X_test)
        
        # ØªØ¯Ø±ÙŠØ¨
        model = RandomForestClassifier(n_estimators=100, random_state=42)
        model.fit(X_train_scaled, y_train)
        
        # Ø­ÙØ¸
        os.makedirs('./trained_models', exist_ok=True)
        
        model_path = f'./trained_models/{symbol}_{timeframe}_random_forest.pkl'
        joblib.dump(model, model_path)
        
        scaler_path = f'./trained_models/{symbol}_{timeframe}_scaler.pkl'
        joblib.dump(scaler, scaler_path)
        
        # Ø­ÙØ¸ ÙÙŠ Ø§Ù„Ù†Ø¸Ø§Ù…
        key = f"{symbol}_{timeframe}"
        if key not in system.models:
            system.models[key] = {}
        system.models[key]['random_forest'] = model
        system.scalers[key] = scaler
        
        return True
        
    except Exception as e:
        logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø¨Ø³ÙŠØ·: {str(e)}")
        return False

def create_optimized_training_script():
    """Ø¥Ù†Ø´Ø§Ø¡ Ø³ÙƒØ±ÙŠØ¨Øª Ù…Ø­Ø³Ù† Ù„Ù„ØªØ¯Ø±ÙŠØ¨"""
    script = """#!/usr/bin/env python3
# Ø³ÙƒØ±ÙŠØ¨Øª Ù…Ø­Ø³Ù† Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ù…Ù† price_data

import sqlite3
import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import RobustScaler
import joblib
import os

# Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ø£Ø²ÙˆØ§Ø¬ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©
MAJOR_PAIRS = [
    'EURUSD', 'GBPUSD', 'USDJPY', 'USDCHF', 'AUDUSD',
    'USDCAD', 'NZDUSD', 'EURJPY', 'GBPJPY', 'EURNZD'
]

def train_pair(symbol):
    print(f"Training {symbol}...")
    
    conn = sqlite3.connect('./data/forex_ml.db')
    
    # Ø¬Ù„Ø¨ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
    query = f"SELECT * FROM price_data WHERE symbol = '{symbol}' ORDER BY time DESC LIMIT 5000"
    df = pd.read_sql_query(query, conn)
    
    if len(df) < 1000:
        print(f"Not enough data for {symbol}")
        return
    
    # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
    df['time'] = pd.to_datetime(df['time'])
    df.set_index('time', inplace=True)
    
    # Ø­Ø³Ø§Ø¨ Ø§Ù„Ù…ÙŠØ²Ø§Øª
    df['sma_20'] = df['close'].rolling(20).mean()
    df['sma_50'] = df['close'].rolling(50).mean()
    df['rsi'] = calculate_rsi(df['close'])
    
    # Ø¥Ø²Ø§Ù„Ø© NaN
    df = df.dropna()
    
    # Ø§Ù„Ù…ÙŠØ²Ø§Øª ÙˆØ§Ù„Ù‡Ø¯Ù
    features = ['open', 'high', 'low', 'close', 'sma_20', 'sma_50', 'rsi']
    X = df[features].values
    y = (df['close'].shift(-1) > df['close']).astype(int).values[:-1]
    X = X[:-1]
    
    # ØªØ¯Ø±ÙŠØ¨
    model = RandomForestClassifier(n_estimators=100)
    model.fit(X, y)
    
    # Ø­ÙØ¸
    os.makedirs('./trained_models', exist_ok=True)
    joblib.dump(model, f'./trained_models/{symbol}_M15_model.pkl')
    
    print(f"âœ“ {symbol} trained successfully")
    conn.close()

def calculate_rsi(prices, period=14):
    delta = prices.diff()
    gain = (delta.where(delta > 0, 0)).rolling(window=period).mean()
    loss = (-delta.where(delta < 0, 0)).rolling(window=period).mean()
    rs = gain / loss
    return 100 - (100 / (1 + rs))

# ØªØ¯Ø±ÙŠØ¨ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø£Ø²ÙˆØ§Ø¬
for pair in MAJOR_PAIRS:
    try:
        train_pair(pair)
    except Exception as e:
        print(f"Error training {pair}: {e}")

print("Training complete!")
"""
    
    with open('quick_train_models.py', 'w') as f:
        f.write(script)
    
    logger.info("âœ… ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ quick_train_models.py")

def main():
    logger.info("\n" + "="*80)
    logger.info("ğŸ” ÙØ­Øµ ÙˆØªØ¯Ø±ÙŠØ¨ Ù…Ù† Ø¬Ø¯ÙˆÙ„ price_data")
    logger.info("="*80)
    
    # ÙØ­Øµ Ø§Ù„Ø¨Ù†ÙŠØ©
    columns = inspect_price_data()
    
    if not columns:
        logger.error("âŒ ÙØ´Ù„ ÙØ­Øµ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª")
        return
    
    # ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬
    logger.info("\nğŸ¤– Ø¨Ø¯Ø¡ Ø§Ù„ØªØ¯Ø±ÙŠØ¨...")
    trained = train_from_price_data()
    
    # Ø¥Ù†Ø´Ø§Ø¡ Ø³ÙƒØ±ÙŠØ¨Øª Ø³Ø±ÙŠØ¹
    create_optimized_training_script()
    
    logger.info("\n" + "="*80)
    logger.info("ğŸ“Š Ø§Ù„Ù…Ù„Ø®Øµ:")
    logger.info(f"âœ… ØªÙ… ØªØ¯Ø±ÙŠØ¨ {trained} Ù†Ù…ÙˆØ°Ø¬")
    logger.info(f"ğŸ“ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ ÙÙŠ: ./trained_models/")
    logger.info(f"ğŸš€ Ù„Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ø³Ø±ÙŠØ¹: python3 quick_train_models.py")
    logger.info("="*80)

if __name__ == "__main__":
    main()