#!/usr/bin/env python3
"""
Advanced Complete Training System with Fixed Parallel Processing - Ù†Ø¸Ø§Ù… Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù…ØªÙ‚Ø¯Ù… Ø§Ù„Ù…Ø­Ø³Ù†
ØªÙ… Ø¥ØµÙ„Ø§Ø­ Ù…Ø´Ø§ÙƒÙ„ Ø§Ù„ØªÙˆÙ‚Ù ÙˆØ§Ù„ØªØ¬Ù…Ø¯ ÙÙŠ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…ØªÙˆØ§Ø²ÙŠØ©
"""

import os
import sys
import json
import sqlite3
import numpy as np
import pandas as pd
from datetime import datetime, timedelta
from pathlib import Path
import joblib
import warnings
warnings.filterwarnings('ignore')

from loguru import logger
from sklearn.model_selection import train_test_split, TimeSeriesSplit, GridSearchCV
from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report
from sklearn.ensemble import VotingClassifier, RandomForestClassifier, ExtraTreesClassifier
from sklearn.neural_network import MLPClassifier
import lightgbm as lgb
import xgboost as xgb
from catboost import CatBoostClassifier
import optuna
from ta import add_all_ta_features
from ta.utils import dropna
import talib

# Ù…ÙƒØªØ¨Ø§Øª Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…ØªÙˆØ§Ø²ÙŠØ©
from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor, as_completed, TimeoutError
from multiprocessing import cpu_count, Manager, Pool
import threading
from functools import partial
import time
import gc
import psutil

# Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ³Ø¬ÙŠÙ„
logger.remove()
logger.add(sys.stdout, format="<green>{time:YYYY-MM-DD HH:mm:ss}</green> | <level>{level: <8}</level> | <level>{message}</level>")
logger.add("logs/advanced_training_parallel_fixed.log", rotation="1 day", retention="30 days")

# Ø¯Ø§Ù„Ø© Ù…Ø³Ø§Ø¹Ø¯Ø© Ù„Ù„ØªØ¯Ø±ÙŠØ¨ - ÙŠØ¬Ø¨ Ø£Ù† ØªÙƒÙˆÙ† ÙÙŠ Ø§Ù„Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø£Ø¹Ù„Ù‰ Ù„Ù„Ù€ pickle
def train_strategy_worker(args):
    """Ø¯Ø§Ù„Ø© Ø¹Ø§Ù…Ù„ Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© ÙˆØ§Ø­Ø¯Ø©"""
    try:
        (X, y, confidence, feature_names, strategy_name, strategy, 
         symbol, timeframe, trainer_config) = args
        
        logger.info(f"ğŸ”§ Ø¨Ø¯Ø¡ ØªØ¯Ø±ÙŠØ¨ {strategy_name} Ù„Ù€ {symbol} {timeframe}")
        
        # Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© Ø¯Ø§Ø®Ù„ Ø§Ù„Ø¹Ù…Ù„ÙŠØ©
        from sklearn.preprocessing import RobustScaler
        from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
        from imblearn.over_sampling import SMOTE
        from imblearn.under_sampling import RandomUnderSampler
        from imblearn.pipeline import Pipeline
        
        # Ù…ÙˆØ§Ø²Ù†Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        high_conf_mask = confidence > 0.7
        X_high = X[high_conf_mask]
        y_high = y[high_conf_mask]
        
        # Ù…ÙˆØ§Ø²Ù†Ø© Ø§Ù„ÙØ¦Ø§Øª
        try:
            over = SMOTE(sampling_strategy=0.8, random_state=42, n_jobs=1)
            under = RandomUnderSampler(sampling_strategy=1.0, random_state=42)
            steps = [('o', over), ('u', under)]
            pipeline = Pipeline(steps=steps)
            X_balanced, y_balanced = pipeline.fit_resample(X_high, y_high)
        except:
            X_balanced, y_balanced = X_high, y_high
        
        # ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        split_idx = int(len(X_balanced) * 0.7)
        X_temp = X_balanced[:split_idx]
        X_test = X_balanced[split_idx:]
        y_temp = y_balanced[:split_idx]
        y_test = y_balanced[split_idx:]
        
        split_idx2 = int(len(X_temp) * 0.85)
        X_train = X_temp[:split_idx2]
        X_val = X_temp[split_idx2:]
        y_train = y_temp[:split_idx2]
        y_val = y_temp[split_idx2:]
        
        # Ù…Ø¹Ø§ÙŠØ±Ø©
        scaler = RobustScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        X_val_scaled = scaler.transform(X_val)
        X_test_scaled = scaler.transform(X_test)
        
        # ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø³ÙŠØ· Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† Ø§Ù„Ù…Ø¬Ù…Ø¹ Ø§Ù„Ù…Ø¹Ù‚Ø¯
        # Ø§Ø³ØªØ®Ø¯Ø§Ù… LightGBM ÙÙ‚Ø· Ù„ØªØ¬Ù†Ø¨ Ù…Ø´Ø§ÙƒÙ„ Ø§Ù„ØªØ³Ù„Ø³Ù„
        lgb_params = {
            'objective': 'multiclass',
            'num_class': 3,
            'metric': 'multi_logloss',
            'boosting_type': 'gbdt',
            'num_leaves': 100,
            'learning_rate': 0.01,
            'n_estimators': 1000,
            'feature_fraction': 0.9,
            'bagging_fraction': 0.9,
            'verbosity': -1,
            'n_jobs': 1
        }
        
        model = lgb.LGBMClassifier(**lgb_params)
        model.fit(X_train_scaled, y_train, 
                 eval_set=[(X_val_scaled, y_val)],
                 callbacks=[lgb.early_stopping(50), lgb.log_evaluation(0)])
        
        # ØªÙ‚ÙŠÙŠÙ…
        y_pred = model.predict(X_test_scaled)
        test_accuracy = accuracy_score(y_test, y_pred)
        test_precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)
        test_recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)
        test_f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)
        
        # Ù…Ø¹Ø¯Ù„ Ù†Ø¬Ø§Ø­ Ø§Ù„ØµÙÙ‚Ø§Øª
        trade_mask = y_pred != 1
        if trade_mask.sum() > 0:
            trade_accuracy = accuracy_score(y_test[trade_mask], y_pred[trade_mask])
        else:
            trade_accuracy = 0
        
        logger.info(f"âœ… {strategy_name}: Ø¯Ù‚Ø© {test_accuracy:.4f}, ØµÙÙ‚Ø§Øª {trade_accuracy:.4f}")
        
        # Ø§Ù„Ù†ØªØ§Ø¦Ø¬
        strategy_results = {
            'accuracy': test_accuracy,
            'trade_accuracy': trade_accuracy,
            'precision': test_precision,
            'recall': test_recall,
            'f1': test_f1,
            'confidence_threshold': strategy['confidence_threshold']
        }
        
        # Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¥Ø°Ø§ ÙƒØ§Ù† Ø¬ÙŠØ¯
        if test_accuracy >= 0.85:
            model_dir = Path(f"models/{symbol}_{timeframe}/{strategy_name}")
            model_dir.mkdir(parents=True, exist_ok=True)
            
            model_data = {
                'model': model,
                'scaler': scaler,
                'feature_names': feature_names,
                'strategy': strategy,
                'results': strategy_results,
                'training_date': datetime.now()
            }
            
            joblib.dump(model_data, model_dir / 'model_advanced.pkl')
            logger.info(f"ğŸ’¾ ØªÙ… Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬: {model_dir}")
        
        return strategy_name, strategy_results
        
    except Exception as e:
        logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªØ¯Ø±ÙŠØ¨ {strategy_name}: {str(e)}")
        import traceback
        traceback.print_exc()
        return strategy_name, None

class AdvancedCompleteTrainerParallelFixed:
    """Ù†Ø¸Ø§Ù… Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù…ØªÙ‚Ø¯Ù… Ø§Ù„Ù…Ø­Ø³Ù† Ù…Ø¹ Ø­Ù„ Ù…Ø´Ø§ÙƒÙ„ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…ØªÙˆØ§Ø²ÙŠØ©"""
    
    def __init__(self, max_workers=None):
        self.min_data_points = 10000
        self.test_size = 0.2
        self.validation_split = 0.15
        self.random_state = 42
        
        # ØªØ­Ø¯ÙŠØ¯ Ø¹Ø¯Ø¯ Ø§Ù„Ø¹Ù…Ù„ÙŠØ§Øª Ø§Ù„Ù…ØªÙˆØ§Ø²ÙŠØ© Ø¨Ø´ÙƒÙ„ Ù…Ø­Ø§ÙØ¸
        available_cores = cpu_count()
        self.max_workers = max_workers or max(1, min(available_cores - 1, 4))
        logger.info(f"ğŸš€ Ø§Ø³ØªØ®Ø¯Ø§Ù… {self.max_workers} Ø¹Ù…Ù„ÙŠØ§Øª Ù…ØªÙˆØ§Ø²ÙŠØ© Ù…Ù† Ø£ØµÙ„ {available_cores} Ù…Ø¹Ø§Ù„Ø¬")
        
        # Ù…Ø±Ø§Ù‚Ø¨Ø© Ø§Ù„Ø°Ø§ÙƒØ±Ø©
        self.monitor_memory()
        
        # Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨
        self.training_strategies = {
            'ultra_short': {'lookahead': 5, 'min_pips': 3, 'confidence_threshold': 0.95},
            'scalping': {'lookahead': 15, 'min_pips': 5, 'confidence_threshold': 0.92},
            'short_term': {'lookahead': 30, 'min_pips': 10, 'confidence_threshold': 0.90},
            'medium_term': {'lookahead': 60, 'min_pips': 20, 'confidence_threshold': 0.88},
            'long_term': {'lookahead': 240, 'min_pips': 40, 'confidence_threshold': 0.85}
        }
        
        # Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Stop Loss Ùˆ Take Profit
        self.sl_tp_settings = {
            'ultra_short': {
                'stop_loss_atr': 0.5,
                'take_profit_ratios': [1.0, 1.5, 2.0],
                'trailing_stop_atr': 0.3,
                'breakeven_pips': 5
            },
            'scalping': {
                'stop_loss_atr': 1.0,
                'take_profit_ratios': [1.0, 2.0, 3.0],
                'trailing_stop_atr': 0.5,
                'breakeven_pips': 10
            },
            'short_term': {
                'stop_loss_atr': 1.5,
                'take_profit_ratios': [1.5, 2.5, 3.5],
                'trailing_stop_atr': 0.7,
                'breakeven_pips': 15
            },
            'medium_term': {
                'stop_loss_atr': 2.0,
                'take_profit_ratios': [2.0, 3.0, 4.0],
                'trailing_stop_atr': 1.0,
                'breakeven_pips': 20
            },
            'long_term': {
                'stop_loss_atr': 2.5,
                'take_profit_ratios': [2.5, 4.0, 6.0],
                'trailing_stop_atr': 1.5,
                'breakeven_pips': 30
            }
        }
    
    def monitor_memory(self):
        """Ù…Ø±Ø§Ù‚Ø¨Ø© Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø°Ø§ÙƒØ±Ø©"""
        memory = psutil.virtual_memory()
        logger.info(f"ğŸ’¾ Ø§Ù„Ø°Ø§ÙƒØ±Ø©: {memory.percent}% Ù…Ø³ØªØ®Ø¯Ù…Ø© ({memory.used / 1024**3:.1f}GB / {memory.total / 1024**3:.1f}GB)")
        if memory.percent > 80:
            logger.warning("âš ï¸ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ù…Ø±ØªÙØ¹! Ù‚Ø¯ ÙŠØ¤Ø«Ø± Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø¯Ø§Ø¡")
    
    def check_database(self):
        """Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª"""
        db_path = Path("data/forex_ml.db")
        
        if not db_path.exists():
            logger.error("âŒ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯Ø©!")
            return False
        
        try:
            conn = sqlite3.connect(db_path)
            cursor = conn.cursor()
            cursor.execute("SELECT COUNT(*) FROM price_data")
            count = cursor.fetchone()[0]
            conn.close()
            
            if count == 0:
                logger.error("âŒ Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¨ÙŠØ§Ù†Ø§Øª ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª!")
                return False
            
            logger.info(f"âœ… Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¬Ø§Ù‡Ø²Ø©: {count:,} Ø³Ø¬Ù„")
            return True
            
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: {e}")
            return False
    
    def load_data_advanced(self, symbol, timeframe, limit=100000):
        """ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª"""
        try:
            logger.info(f"ğŸ“Š ØªØ­Ù…ÙŠÙ„ Ø¨ÙŠØ§Ù†Ø§Øª {symbol} {timeframe}...")
            
            conn = sqlite3.connect("data/forex_ml.db")
            query = """
                SELECT * FROM price_data 
                WHERE symbol = ? AND timeframe = ?
                ORDER BY time ASC
                LIMIT ?
            """
            df = pd.read_sql_query(query, conn, params=(symbol, timeframe, limit))
            conn.close()
            
            if len(df) < self.min_data_points:
                logger.warning(f"âš ï¸ Ø¨ÙŠØ§Ù†Ø§Øª ØºÙŠØ± ÙƒØ§ÙÙŠØ©: {len(df)} Ø³Ø¬Ù„")
                return None
            
            # ØªØ­ÙˆÙŠÙ„ Ø§Ù„ÙˆÙ‚Øª
            df['time'] = pd.to_datetime(df['time'], unit='s')
            df = df.set_index('time')
            
            # Ù…Ù„Ø¡ Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ù…ÙÙ‚ÙˆØ¯Ø©
            df = df.fillna(method='ffill').fillna(method='bfill')
            
            logger.info(f"âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ {len(df)} Ø³Ø¬Ù„")
            return df
            
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: {e}")
            return None
    
    def create_ultra_advanced_features(self, df, symbol):
        """Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø© Ù…Ø¹ Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ø­Ø³Ù†Ø©"""
        logger.info("ğŸ”¬ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø©...")
        start_time = time.time()
        
        features = pd.DataFrame(index=df.index)
        
        # Ø­Ø³Ø§Ø¨ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø¨Ø´ÙƒÙ„ Ù…ØªØ³Ù„Ø³Ù„ Ù„ØªØ¬Ù†Ø¨ Ù…Ø´Ø§ÙƒÙ„ Ø§Ù„Ø°Ø§ÙƒØ±Ø©
        logger.info("  â€¢ Ø­Ø³Ø§Ø¨ Ù…ÙŠØ²Ø§Øª Ø§Ù„Ø£Ø³Ø¹Ø§Ø±...")
        features = pd.concat([features, self._calculate_price_features(df)], axis=1)
        
        logger.info("  â€¢ Ø­Ø³Ø§Ø¨ Ø§Ù„Ù…ØªÙˆØ³Ø·Ø§Øª Ø§Ù„Ù…ØªØ­Ø±ÙƒØ©...")
        features = pd.concat([features, self._calculate_ma_features(df)], axis=1)
        
        logger.info("  â€¢ Ø­Ø³Ø§Ø¨ Ù…Ø¤Ø´Ø±Ø§Øª TA-Lib...")
        features = pd.concat([features, self._calculate_talib_features(df)], axis=1)
        
        logger.info("  â€¢ Ø­Ø³Ø§Ø¨ Ù…ÙŠØ²Ø§Øª Ø§Ù„Ø­Ø¬Ù…...")
        features = pd.concat([features, self._calculate_volume_features(df)], axis=1)
        
        logger.info("  â€¢ Ø­Ø³Ø§Ø¨ Ù…ÙŠØ²Ø§Øª Ø§Ù„ØªÙ‚Ù„Ø¨...")
        features = pd.concat([features, self._calculate_volatility_features(df)], axis=1)
        
        logger.info("  â€¢ Ø­Ø³Ø§Ø¨ Ù…ÙŠØ²Ø§Øª Ø§Ù„Ø³ÙˆÙ‚...")
        features = pd.concat([features, self._calculate_market_features(df)], axis=1)
        
        # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        features = features.fillna(method='ffill').fillna(0)
        features = features.replace([np.inf, -np.inf], 0)
        
        # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø°Ø§ÙƒØ±Ø©
        gc.collect()
        
        end_time = time.time()
        logger.info(f"âœ… ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ {len(features.columns)} Ù…ÙŠØ²Ø© ÙÙŠ {end_time - start_time:.2f} Ø«Ø§Ù†ÙŠØ©")
        
        return features
    
    def _calculate_price_features(self, df):
        """Ø­Ø³Ø§Ø¨ Ù…ÙŠØ²Ø§Øª Ø§Ù„Ø£Ø³Ø¹Ø§Ø±"""
        features = pd.DataFrame(index=df.index)
        
        features['returns'] = df['close'].pct_change()
        features['log_returns'] = np.log(df['close'] / df['close'].shift(1))
        features['price_range'] = (df['high'] - df['low']) / df['close']
        features['body_size'] = abs(df['close'] - df['open']) / df['close']
        features['upper_shadow'] = (df['high'] - df[['close', 'open']].max(axis=1)) / df['close']
        features['lower_shadow'] = (df[['close', 'open']].min(axis=1) - df['low']) / df['close']
        
        return features
    
    def _calculate_ma_features(self, df):
        """Ø­Ø³Ø§Ø¨ Ø§Ù„Ù…ØªÙˆØ³Ø·Ø§Øª Ø§Ù„Ù…ØªØ­Ø±ÙƒØ©"""
        features = pd.DataFrame(index=df.index)
        
        ma_periods = [5, 10, 20, 50, 100, 200]
        
        for period in ma_periods:
            sma = df['close'].rolling(period).mean()
            ema = df['close'].ewm(span=period, adjust=False).mean()
            
            features[f'sma_{period}'] = (df['close'] - sma) / sma
            features[f'ema_{period}'] = (df['close'] - ema) / ema
            features[f'sma_{period}_slope'] = sma.pct_change(5)
            features[f'ema_{period}_slope'] = ema.pct_change(5)
        
        return features
    
    def _calculate_talib_features(self, df):
        """Ø­Ø³Ø§Ø¨ Ù…Ø¤Ø´Ø±Ø§Øª TA-Lib"""
        features = pd.DataFrame(index=df.index)
        
        try:
            # RSI
            for period in [7, 14, 21]:
                features[f'rsi_{period}'] = talib.RSI(df['close'].values, timeperiod=period)
            
            # MACD
            macd, macdsignal, macdhist = talib.MACD(df['close'].values)
            features['macd'] = macd
            features['macd_signal'] = macdsignal
            features['macd_hist'] = macdhist
            
            # Bollinger Bands
            upper, middle, lower = talib.BBANDS(df['close'].values)
            features['bb_upper'] = (upper - df['close']) / df['close']
            features['bb_lower'] = (df['close'] - lower) / df['close']
            features['bb_width'] = (upper - lower) / middle
            
            # ATR
            features['atr_14'] = talib.ATR(df['high'].values, df['low'].values, 
                                          df['close'].values, timeperiod=14) / df['close']
            
            # ADX
            features['adx_14'] = talib.ADX(df['high'].values, df['low'].values, 
                                          df['close'].values, timeperiod=14)
            
        except Exception as e:
            logger.warning(f"âš ï¸ Ø¨Ø¹Ø¶ Ù…Ø¤Ø´Ø±Ø§Øª TA-Lib ÙØ´Ù„Øª: {e}")
        
        return features
    
    def _calculate_volume_features(self, df):
        """Ø­Ø³Ø§Ø¨ Ù…ÙŠØ²Ø§Øª Ø§Ù„Ø­Ø¬Ù…"""
        features = pd.DataFrame(index=df.index)
        
        features['volume_sma_ratio'] = df['volume'] / df['volume'].rolling(20).mean()
        features['obv'] = (np.sign(df['close'].diff()) * df['volume']).cumsum()
        features['force_index'] = df['close'].diff() * df['volume']
        
        return features
    
    def _calculate_volatility_features(self, df):
        """Ø­Ø³Ø§Ø¨ Ù…ÙŠØ²Ø§Øª Ø§Ù„ØªÙ‚Ù„Ø¨"""
        features = pd.DataFrame(index=df.index)
        
        for period in [10, 20, 30]:
            returns = df['close'].pct_change()
            features[f'volatility_{period}'] = returns.rolling(period).std()
        
        return features
    
    def _calculate_market_features(self, df):
        """Ø­Ø³Ø§Ø¨ Ù…ÙŠØ²Ø§Øª Ø§Ù„Ø³ÙˆÙ‚"""
        features = pd.DataFrame(index=df.index)
        
        features['hour'] = df.index.hour
        features['day_of_week'] = df.index.dayofweek
        features['day_of_month'] = df.index.day
        
        # Ø¬Ù„Ø³Ø§Øª Ø§Ù„ØªØ¯Ø§ÙˆÙ„
        features['asian_session'] = ((features['hour'] >= 0) & (features['hour'] < 9)).astype(int)
        features['london_session'] = ((features['hour'] >= 8) & (features['hour'] < 17)).astype(int)
        features['ny_session'] = ((features['hour'] >= 13) & (features['hour'] < 22)).astype(int)
        
        return features
    
    def create_advanced_targets(self, df, strategy):
        """Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ø£Ù‡Ø¯Ø§Ù"""
        lookahead = strategy['lookahead']
        min_pips = strategy['min_pips']
        
        pip_value = 0.0001 if 'JPY' not in str(df.index.name) else 0.01
        
        targets = []
        confidences = []
        
        for i in range(len(df) - lookahead):
            future_prices = df['close'].iloc[i+1:i+lookahead+1].values
            current_price = df['close'].iloc[i]
            
            max_up = (future_prices.max() - current_price) / pip_value
            max_down = (current_price - future_prices.min()) / pip_value
            
            if max_up >= min_pips * 2:
                targets.append(2)
                confidences.append(min(max_up / (min_pips * 3), 1.0))
            elif max_down >= min_pips * 2:
                targets.append(0)
                confidences.append(min(max_down / (min_pips * 3), 1.0))
            else:
                targets.append(1)
                confidences.append(0.5)
        
        targets.extend([1] * lookahead)
        confidences.extend([0] * lookahead)
        
        return np.array(targets), np.array(confidences)
    
    def train_symbol_advanced(self, symbol, timeframe):
        """ØªØ¯Ø±ÙŠØ¨ Ø¹Ù…Ù„Ø© ÙˆØ§Ø­Ø¯Ø© Ù…Ø¹ Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ø­Ø³Ù†Ø©"""
        logger.info(f"\n{'='*80}")
        logger.info(f"ğŸš€ ØªØ¯Ø±ÙŠØ¨ {symbol} {timeframe}")
        logger.info(f"{'='*80}")
        
        # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        df = self.load_data_advanced(symbol, timeframe)
        if df is None:
            return None
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…ÙŠØ²Ø§Øª
        features = self.create_ultra_advanced_features(df, symbol)
        X = features.values
        feature_names = features.columns.tolist()
        
        results = {
            'symbol': symbol,
            'timeframe': timeframe,
            'strategies': {},
            'best_accuracy': 0,
            'best_strategy': None
        }
        
        # ØªØ¯Ø±ÙŠØ¨ ÙƒÙ„ Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© Ø¨Ø´ÙƒÙ„ Ù…ØªØ³Ù„Ø³Ù„ Ù„ØªØ¬Ù†Ø¨ Ù…Ø´Ø§ÙƒÙ„ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…ØªÙˆØ§Ø²ÙŠØ©
        for strategy_name, strategy in self.training_strategies.items():
            try:
                logger.info(f"\nğŸ“Š ØªØ¯Ø±ÙŠØ¨ Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© {strategy_name}...")
                
                # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ø£Ù‡Ø¯Ø§Ù
                y, confidence = self.create_advanced_targets(df, strategy)
                
                # Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù„ØªØ¯Ø±ÙŠØ¨
                trainer_config = {
                    'min_data_points': self.min_data_points,
                    'test_size': self.test_size,
                    'validation_split': self.validation_split,
                    'random_state': self.random_state
                }
                
                args = (X, y, confidence, feature_names, strategy_name, 
                       strategy, symbol, timeframe, trainer_config)
                
                # ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ©
                strat_name, strategy_results = train_strategy_worker(args)
                
                if strategy_results:
                    results['strategies'][strat_name] = strategy_results
                    
                    # ØªØ­Ø¯ÙŠØ« Ø£ÙØ¶Ù„ Ù†ØªÙŠØ¬Ø©
                    if strategy_results['accuracy'] > results['best_accuracy']:
                        results['best_accuracy'] = strategy_results['accuracy']
                        results['best_strategy'] = strat_name
                    
                    logger.info(f"âœ… {strat_name}: Ø¯Ù‚Ø© {strategy_results['accuracy']:.4f}")
                
                # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø¨Ø¹Ø¯ ÙƒÙ„ Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ©
                gc.collect()
                
            except Exception as e:
                logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªØ¯Ø±ÙŠØ¨ {strategy_name}: {e}")
                import traceback
                traceback.print_exc()
        
        # Ø·Ø¨Ø§Ø¹Ø© Ø§Ù„Ù…Ù„Ø®Øµ
        logger.info(f"\n{'='*60}")
        logger.info(f"ğŸ“Š Ù…Ù„Ø®Øµ {symbol} {timeframe}")
        logger.info(f"ğŸ† Ø£ÙØ¶Ù„ Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ©: {results['best_strategy']}")
        logger.info(f"ğŸ¯ Ø£ÙØ¶Ù„ Ø¯Ù‚Ø©: {results['best_accuracy']:.4f}")
        
        return results
    
    def train_symbols_batch(self, symbols_batch):
        """ØªØ¯Ø±ÙŠØ¨ Ù…Ø¬Ù…ÙˆØ¹Ø© Ù…Ù† Ø§Ù„Ø¹Ù…Ù„Ø§Øª"""
        results = []
        
        for symbol, timeframe in symbols_batch:
            try:
                logger.info(f"\nğŸ”„ Ù…Ø¹Ø§Ù„Ø¬Ø© {symbol} {timeframe}")
                result = self.train_symbol_advanced(symbol, timeframe)
                if result:
                    results.append(result)
                
                # Ù…Ø±Ø§Ù‚Ø¨Ø© Ø§Ù„Ø°Ø§ÙƒØ±Ø©
                self.monitor_memory()
                
                # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø°Ø§ÙƒØ±Ø©
                gc.collect()
                
            except Exception as e:
                logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ {symbol} {timeframe}: {e}")
        
        return results
    
    def train_all_advanced(self):
        """ØªØ¯Ø±ÙŠØ¨ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø¹Ù…Ù„Ø§Øª Ø¨Ø·Ø±ÙŠÙ‚Ø© Ù…Ø­Ø³Ù†Ø©"""
        logger.info("\n" + "="*100)
        logger.info("ğŸš€ Ø¨Ø¯Ø¡ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù…ØªÙ‚Ø¯Ù… Ø§Ù„Ù…Ø­Ø³Ù†")
        logger.info("="*100)
        
        # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        if not self.check_database():
            return
        
        # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ø¹Ù…Ù„Ø§Øª Ø§Ù„Ù…ØªØ§Ø­Ø©
        try:
            conn = sqlite3.connect("data/forex_ml.db")
            query = """
                SELECT symbol, timeframe, COUNT(*) as count
                FROM price_data
                GROUP BY symbol, timeframe
                HAVING count >= ?
                ORDER BY symbol, timeframe
            """
            available = pd.read_sql_query(query, conn, params=(self.min_data_points,))
            conn.close()
            
            logger.info(f"ğŸ“Š Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ù…ØªØ§Ø­Ø©: {len(available)}")
            
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: {e}")
            return
        
        # Ù†ØªØ§Ø¦Ø¬ Ø§Ù„ØªØ¯Ø±ÙŠØ¨
        excellent_models = []  # 90%+
        good_models = []       # 85-90%
        acceptable_models = [] # 80-85%
        failed_models = []
        
        # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø¹Ù…Ù„Ø§Øª Ø¨Ø´ÙƒÙ„ Ù…ØªØ³Ù„Ø³Ù„ Ù…Ø¹ Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…ØªÙˆØ§Ø²ÙŠØ© Ù„Ù„Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ§Øª ÙÙ‚Ø·
        total_symbols = len(available)
        
        for idx, row in available.iterrows():
            symbol = row['symbol']
            timeframe = row['timeframe']
            
            logger.info(f"\nğŸ“ˆ Ù…Ø¹Ø§Ù„Ø¬Ø© {idx+1}/{total_symbols}: {symbol} {timeframe}")
            
            try:
                result = self.train_symbol_advanced(symbol, timeframe)
                
                if result:
                    best_acc = result['best_accuracy']
                    
                    model_info = {
                        'symbol': symbol,
                        'timeframe': timeframe,
                        'accuracy': best_acc,
                        'strategy': result['best_strategy']
                    }
                    
                    if best_acc >= 0.90:
                        excellent_models.append(model_info)
                        logger.info(f"ğŸŒŸ Ù…Ù…ØªØ§Ø²! Ø¯Ù‚Ø© {best_acc:.4f}")
                    elif best_acc >= 0.85:
                        good_models.append(model_info)
                        logger.info(f"âœ… Ø¬ÙŠØ¯ Ø¬Ø¯Ø§Ù‹! Ø¯Ù‚Ø© {best_acc:.4f}")
                    elif best_acc >= 0.80:
                        acceptable_models.append(model_info)
                        logger.info(f"ğŸ‘ Ù…Ù‚Ø¨ÙˆÙ„! Ø¯Ù‚Ø© {best_acc:.4f}")
                    else:
                        failed_models.append(f"{symbol} {timeframe}")
                else:
                    failed_models.append(f"{symbol} {timeframe}")
                
                # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø¨Ø¹Ø¯ ÙƒÙ„ Ø¹Ù…Ù„Ø©
                gc.collect()
                
            except Exception as e:
                logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ {symbol} {timeframe}: {e}")
                failed_models.append(f"{symbol} {timeframe}: {str(e)}")
        
        # Ø·Ø¨Ø§Ø¹Ø© Ø§Ù„ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ
        self._print_final_report(excellent_models, good_models, acceptable_models, failed_models)
    
    def _print_final_report(self, excellent, good, acceptable, failed):
        """Ø·Ø¨Ø§Ø¹Ø© Ø§Ù„ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ"""
        logger.info("\n" + "="*100)
        logger.info("ğŸ“Š Ø§Ù„ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ Ù„Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù…Ø­Ø³Ù†")
        logger.info("="*100)
        
        total = len(excellent) + len(good) + len(acceptable) + len(failed)
        
        logger.info(f"\nğŸ“ˆ Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø£Ø¯Ø§Ø¡:")
        logger.info(f"  ğŸŒŸ Ù…Ù…ØªØ§Ø² (90%+): {len(excellent)} Ù†Ù…ÙˆØ°Ø¬")
        logger.info(f"  âœ… Ø¬ÙŠØ¯ Ø¬Ø¯Ø§Ù‹ (85-90%): {len(good)} Ù†Ù…ÙˆØ°Ø¬")
        logger.info(f"  ğŸ‘ Ù…Ù‚Ø¨ÙˆÙ„ (80-85%): {len(acceptable)} Ù†Ù…ÙˆØ°Ø¬")
        logger.info(f"  âŒ ÙØ´Ù„ (<80%): {len(failed)} Ù†Ù…ÙˆØ°Ø¬")
        
        success_rate = (len(excellent) + len(good)) / total * 100 if total > 0 else 0
        logger.info(f"\nğŸ¯ Ù…Ø¹Ø¯Ù„ Ø§Ù„Ù†Ø¬Ø§Ø­ (85%+): {success_rate:.1f}%")
        
        if excellent:
            logger.info(f"\nğŸŒŸ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…Ù…ØªØ§Ø²Ø© (90%+):")
            for model in sorted(excellent, key=lambda x: x['accuracy'], reverse=True)[:10]:
                logger.info(f"  â€¢ {model['symbol']} {model['timeframe']}: "
                          f"{model['accuracy']:.4f} ({model['strategy']})")
        
        if good:
            logger.info(f"\nâœ… Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ø¬ÙŠØ¯Ø© Ø¬Ø¯Ø§Ù‹ (85-90%):")
            for model in sorted(good, key=lambda x: x['accuracy'], reverse=True)[:10]:
                logger.info(f"  â€¢ {model['symbol']} {model['timeframe']}: "
                          f"{model['accuracy']:.4f} ({model['strategy']})")
        
        # Ø­ÙØ¸ Ø§Ù„ØªÙ‚Ø±ÙŠØ±
        report = {
            'training_date': datetime.now().isoformat(),
            'total_models': total,
            'excellent_count': len(excellent),
            'good_count': len(good),
            'acceptable_count': len(acceptable),
            'failed_count': len(failed),
            'success_rate': success_rate,
            'excellent_models': excellent,
            'good_models': good,
            'acceptable_models': acceptable,
            'configuration': {
                'min_data_points': self.min_data_points,
                'strategies': list(self.training_strategies.keys()),
                'max_workers': self.max_workers
            }
        }
        
        report_path = Path("models/advanced_training_report_fixed.json")
        report_path.parent.mkdir(exist_ok=True)
        
        with open(report_path, 'w') as f:
            json.dump(report, f, indent=2, default=str)
        
        logger.info(f"\nğŸ’¾ ØªÙ… Ø­ÙØ¸ Ø§Ù„ØªÙ‚Ø±ÙŠØ±: {report_path}")

def main():
    """Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©"""
    import argparse
    
    parser = argparse.ArgumentParser(description='Advanced Training System - Fixed Version')
    parser.add_argument('--quick', action='store_true', help='Quick test with one symbol')
    parser.add_argument('--workers', type=int, default=None, help='Number of parallel workers')
    parser.add_argument('--symbol', type=str, default='EURUSD', help='Symbol to train (for quick mode)')
    parser.add_argument('--timeframe', type=str, default='H1', help='Timeframe to train (for quick mode)')
    args = parser.parse_args()
    
    trainer = AdvancedCompleteTrainerParallelFixed(max_workers=args.workers)
    
    if args.quick:
        # ØªØ¯Ø±ÙŠØ¨ Ø¹Ù…Ù„Ø© ÙˆØ§Ø­Ø¯Ø© ÙÙ‚Ø·
        trainer.train_symbol_advanced(args.symbol, args.timeframe)
    else:
        # ØªØ¯Ø±ÙŠØ¨ Ø´Ø§Ù…Ù„
        trainer.train_all_advanced()

if __name__ == "__main__":
    main()