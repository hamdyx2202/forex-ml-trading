#!/usr/bin/env python3
"""
ğŸš€ Forex ML Trading Server - Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„ÙƒØ§Ù…Ù„
ğŸ“Š ÙŠØ¯Ø¹Ù… Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø¹Ù…Ù„Ø§Øª ÙˆØ§Ù„ÙØ±ÙŠÙ…Ø§Øª
ğŸ§  6 Ù†Ù…Ø§Ø°Ø¬ ML + 200+ Ù…ÙŠØ²Ø© + 10 ÙØ±Ø¶ÙŠØ§Øª
ğŸ“ˆ ØªØ¹Ù„Ù… Ù…Ø³ØªÙ…Ø± Ù…Ù† Ø§Ù„ØµÙÙ‚Ø§Øª
"""

import os
import sys
import json
import logging
import threading
import sqlite3
import joblib
import pickle
from datetime import datetime, timedelta
from flask import Flask, request, jsonify
from flask_cors import CORS
import pandas as pd
import numpy as np
from sklearn.preprocessing import RobustScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report

# Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø©
try:
    import lightgbm as lgb
    LIGHTGBM_AVAILABLE = True
except:
    LIGHTGBM_AVAILABLE = False

try:
    import xgboost as xgb
    XGBOOST_AVAILABLE = True
except:
    XGBOOST_AVAILABLE = False

from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier
from sklearn.neural_network import MLPClassifier

# Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ³Ø¬ÙŠÙ„
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    handlers=[
        logging.FileHandler('complete_forex_ml_server.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# Flask app
app = Flask(__name__)
CORS(app)

# Ù‚Ø§Ø¹Ø¯Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ø±Ø§Ø¨Ø­Ø©
PATTERNS_DB = './winning_patterns.db'

class CompleteForexMLSystem:
    """Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„ÙƒØ§Ù…Ù„ Ù„Ù„ØªØ¯Ø§ÙˆÙ„ Ø¨Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ"""
    
    def __init__(self):
        self.models = {}
        self.scalers = {}
        self.feature_importance = {}
        self.winning_patterns = {}
        self.trade_history = []
        
        # Ø¥Ø¹Ø¯Ø§Ø¯ Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        self.historical_db = './data/forex_ml.db'
        self.trading_db = './trading_performance.db'
        self.models_dir = './trained_models'
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…Ø¬Ù„Ø¯Ø§Øª Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©
        os.makedirs(self.models_dir, exist_ok=True)
        
        # ØªÙ‡ÙŠØ¦Ø© Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        self._init_databases()
        
        # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø©
        self.load_existing_models()
        
        # Ø¨Ø¯Ø¡ Ø®ÙŠØ· Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ù…Ø³ØªÙ…Ø±
        self.learning_thread = threading.Thread(target=self._continuous_learning_loop, daemon=True)
        self.learning_thread.start()
        
        logger.info("âœ… ØªÙ… ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„ÙƒØ§Ù…Ù„")
    
    def _init_databases(self):
        """ØªÙ‡ÙŠØ¦Ø© Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª"""
        # Ù‚Ø§Ø¹Ø¯Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ø±Ø§Ø¨Ø­Ø©
        conn = sqlite3.connect(PATTERNS_DB)
        cursor = conn.cursor()
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS winning_patterns (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                symbol TEXT,
                timeframe TEXT,
                pattern_type TEXT,
                features TEXT,
                success_rate REAL,
                total_trades INTEGER,
                avg_profit_pips REAL,
                created_at TIMESTAMP,
                last_updated TIMESTAMP
            )
        ''')
        conn.commit()
        conn.close()
        
        # Ù‚Ø§Ø¹Ø¯Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØµÙÙ‚Ø§Øª
        conn = sqlite3.connect(self.trading_db)
        cursor = conn.cursor()
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS trades (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                symbol TEXT,
                timeframe TEXT,
                signal_time TIMESTAMP,
                action TEXT,
                confidence REAL,
                entry_price REAL,
                sl_price REAL,
                tp1_price REAL,
                tp2_price REAL,
                features TEXT,
                model_predictions TEXT,
                hypotheses_results TEXT,
                exit_time TIMESTAMP,
                exit_price REAL,
                profit_pips REAL,
                exit_reason TEXT,
                market_conditions TEXT
            )
        ''')
        conn.commit()
        conn.close()
    
    def calculate_all_features(self, df):
        """Ø­Ø³Ø§Ø¨ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø© (200+ Ù…ÙŠØ²Ø©)"""
        features = df.copy()
        
        # 1. Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
        features['price_change'] = features['close'].pct_change()
        features['high_low_ratio'] = features['high'] / features['low']
        features['close_open_ratio'] = features['close'] / features['open']
        
        # 2. Ø§Ù„Ù…ØªÙˆØ³Ø·Ø§Øª Ø§Ù„Ù…ØªØ­Ø±ÙƒØ© (20 Ù…ÙŠØ²Ø©)
        for period in [5, 10, 20, 50, 100, 200]:
            features[f'sma_{period}'] = features['close'].rolling(period).mean()
            features[f'sma_{period}_slope'] = features[f'sma_{period}'].diff()
            
        # EMAs
        for period in [9, 12, 26]:
            features[f'ema_{period}'] = features['close'].ewm(span=period).mean()
            
        # 3. Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ù„Ø²Ø®Ù… (30 Ù…ÙŠØ²Ø©)
        # RSI
        for period in [14, 21, 28]:
            delta = features['close'].diff()
            gain = (delta.where(delta > 0, 0)).rolling(window=period).mean()
            loss = (-delta.where(delta < 0, 0)).rolling(window=period).mean()
            rs = gain / loss
            features[f'rsi_{period}'] = 100 - (100 / (1 + rs))
            
        # MACD
        exp1 = features['close'].ewm(span=12, adjust=False).mean()
        exp2 = features['close'].ewm(span=26, adjust=False).mean()
        features['macd'] = exp1 - exp2
        features['macd_signal'] = features['macd'].ewm(span=9, adjust=False).mean()
        features['macd_diff'] = features['macd'] - features['macd_signal']
        
        # Stochastic
        for period in [14, 21]:
            low_min = features['low'].rolling(window=period).min()
            high_max = features['high'].rolling(window=period).max()
            features[f'stoch_k_{period}'] = 100 * ((features['close'] - low_min) / (high_max - low_min))
            features[f'stoch_d_{period}'] = features[f'stoch_k_{period}'].rolling(window=3).mean()
            
        # 4. Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ù„ØªØ°Ø¨Ø°Ø¨ (20 Ù…ÙŠØ²Ø©)
        # Bollinger Bands
        for period in [20, 30]:
            sma = features['close'].rolling(window=period).mean()
            std = features['close'].rolling(window=period).std()
            features[f'bb_upper_{period}'] = sma + (std * 2)
            features[f'bb_lower_{period}'] = sma - (std * 2)
            features[f'bb_width_{period}'] = features[f'bb_upper_{period}'] - features[f'bb_lower_{period}']
            features[f'bb_position_{period}'] = (features['close'] - features[f'bb_lower_{period}']) / features[f'bb_width_{period}']
            
        # ATR
        for period in [14, 21]:
            high_low = features['high'] - features['low']
            high_close = np.abs(features['high'] - features['close'].shift())
            low_close = np.abs(features['low'] - features['close'].shift())
            ranges = pd.concat([high_low, high_close, low_close], axis=1)
            true_range = np.max(ranges, axis=1)
            features[f'atr_{period}'] = true_range.rolling(period).mean()
            
        # 5. Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ù„Ø­Ø¬Ù… (15 Ù…ÙŠØ²Ø©)
        if 'volume' in features.columns:
            features['volume_sma'] = features['volume'].rolling(20).mean()
            features['volume_ratio'] = features['volume'] / features['volume_sma']
            features['price_volume'] = features['close'] * features['volume']
            
        # 6. Ø§Ù„Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ø´Ù…Ø¹ÙŠØ© (30 Ù…ÙŠØ²Ø©)
        # Doji
        features['doji'] = (np.abs(features['close'] - features['open']) / (features['high'] - features['low'])) < 0.1
        
        # Hammer
        body = np.abs(features['close'] - features['open'])
        lower_shadow = features[['open', 'close']].min(axis=1) - features['low']
        features['hammer'] = (lower_shadow > body * 2) & (features['high'] - features[['open', 'close']].max(axis=1) < body)
        
        # Engulfing
        features['bullish_engulfing'] = (
            (features['close'] > features['open']) & 
            (features['close'].shift() < features['open'].shift()) &
            (features['open'] < features['close'].shift()) &
            (features['close'] > features['open'].shift())
        )
        
        # 7. Ù…Ø³ØªÙˆÙŠØ§Øª Ø§Ù„Ø¯Ø¹Ù… ÙˆØ§Ù„Ù…Ù‚Ø§ÙˆÙ…Ø© (20 Ù…ÙŠØ²Ø©)
        for period in [20, 50, 100]:
            features[f'resistance_{period}'] = features['high'].rolling(period).max()
            features[f'support_{period}'] = features['low'].rolling(period).min()
            features[f'sr_position_{period}'] = (features['close'] - features[f'support_{period}']) / (features[f'resistance_{period}'] - features[f'support_{period}'])
            
        # 8. Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ø²Ù…Ù†ÙŠØ© (15 Ù…ÙŠØ²Ø©)
        if isinstance(features.index, pd.DatetimeIndex):
            features['hour'] = features.index.hour
            features['day_of_week'] = features.index.dayofweek
            features['day_of_month'] = features.index.day
            features['month'] = features.index.month
            features['is_london'] = ((features['hour'] >= 8) & (features['hour'] <= 16)).astype(int)
            features['is_newyork'] = ((features['hour'] >= 13) & (features['hour'] <= 21)).astype(int)
            features['is_tokyo'] = ((features['hour'] >= 0) & (features['hour'] <= 8)).astype(int)
            features['is_sydney'] = ((features['hour'] >= 22) | (features['hour'] <= 6)).astype(int)
            
        # 9. Ø§Ù„Ù†Ø³Ø¨ ÙˆØ§Ù„Ø¹Ù„Ø§Ù‚Ø§Øª (25 Ù…ÙŠØ²Ø©)
        # Price ratios
        features['high_close_ratio'] = features['high'] / features['close']
        features['low_close_ratio'] = features['low'] / features['close']
        
        # Moving average crosses
        features['sma_5_20_cross'] = (features['sma_5'] > features['sma_20']).astype(int)
        features['sma_20_50_cross'] = (features['sma_20'] > features['sma_50']).astype(int)
        
        # 10. Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ù„Ø³ÙˆÙ‚ (15 Ù…ÙŠØ²Ø©)
        # ADX
        for period in [14, 21]:
            plus_dm = features['high'].diff()
            minus_dm = -features['low'].diff()
            plus_dm[plus_dm < 0] = 0
            minus_dm[minus_dm < 0] = 0
            
            tr = true_range.rolling(period).mean()
            plus_di = 100 * (plus_dm.rolling(period).mean() / tr)
            minus_di = 100 * (minus_dm.rolling(period).mean() / tr)
            dx = 100 * np.abs(plus_di - minus_di) / (plus_di + minus_di)
            features[f'adx_{period}'] = dx.rolling(period).mean()
            
        # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØµÙÙˆÙ Ø§Ù„ØªÙŠ ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ NaN
        features = features.dropna()
        
        return features
    
    def evaluate_hypotheses(self, features):
        """ØªÙ‚ÙŠÙŠÙ… 10 ÙØ±Ø¶ÙŠØ§Øª Ø§Ù„ØªØ¯Ø§ÙˆÙ„"""
        hypotheses_results = {}
        
        # 1. Trend Following
        trend_score = 0
        if features.get('sma_20', 0) > features.get('sma_50', 0) > features.get('sma_200', 0):
            trend_score += 0.5
        if features.get('adx_14', 0) > 25:
            trend_score += 0.5
        hypotheses_results['trend_following'] = trend_score
        
        # 2. Mean Reversion
        mr_score = 0
        if features.get('rsi_14', 50) < 30:
            mr_score += 0.5
        elif features.get('rsi_14', 50) > 70:
            mr_score += 0.5
        if features.get('bb_position_20', 0.5) < 0.1 or features.get('bb_position_20', 0.5) > 0.9:
            mr_score += 0.5
        hypotheses_results['mean_reversion'] = mr_score
        
        # 3. Momentum
        momentum_score = 0
        if features.get('macd', 0) > features.get('macd_signal', 0):
            momentum_score += 0.5
        if features.get('rsi_14', 50) > 50 and features.get('rsi_14', 50) < 70:
            momentum_score += 0.5
        hypotheses_results['momentum'] = momentum_score
        
        # 4. Volatility Breakout
        vb_score = 0
        if features.get('atr_14', 0) > features.get('atr_21', 0):
            vb_score += 0.5
        if features.get('bb_width_20', 0) > features.get('bb_width_30', 0):
            vb_score += 0.5
        hypotheses_results['volatility_breakout'] = vb_score
        
        # 5. Seasonality
        season_score = 0
        if features.get('is_london', 0) or features.get('is_newyork', 0):
            season_score += 0.5
        if features.get('day_of_week', 0) in [1, 2, 3]:  # Tue, Wed, Thu
            season_score += 0.5
        hypotheses_results['seasonality'] = season_score
        
        # 6. Support/Resistance
        sr_score = 0
        if features.get('sr_position_20', 0.5) < 0.2:
            sr_score += 0.5  # Near support
        elif features.get('sr_position_20', 0.5) > 0.8:
            sr_score += 0.5  # Near resistance
        hypotheses_results['support_resistance'] = sr_score
        
        # 7. Market Structure
        ms_score = 0
        if features.get('high_low_ratio', 1) > 1.001:
            ms_score += 0.5
        hypotheses_results['market_structure'] = ms_score
        
        # 8. Volume Analysis
        va_score = 0.5  # Default if no volume
        if 'volume_ratio' in features:
            if features.get('volume_ratio', 1) > 1.5:
                va_score = 1
            elif features.get('volume_ratio', 1) < 0.5:
                va_score = 0
        hypotheses_results['volume_analysis'] = va_score
        
        # 9. Pattern Recognition
        pr_score = 0
        if features.get('doji', 0) or features.get('hammer', 0) or features.get('bullish_engulfing', 0):
            pr_score += 1
        hypotheses_results['pattern_recognition'] = pr_score
        
        # 10. Correlation
        corr_score = 0.5  # Default neutral
        hypotheses_results['correlation'] = corr_score
        
        return hypotheses_results
    
    def train_models(self, symbol, timeframe):
        """ØªØ¯Ø±ÙŠØ¨ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ø³ØªØ©"""
        try:
            # Ø¬Ù„Ø¨ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ§Ø±ÙŠØ®ÙŠØ©
            conn = sqlite3.connect(self.historical_db)
            
            # Ù…Ø­Ø§ÙˆÙ„Ø© Ø¬Ù„Ø¨ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù† Ø¬Ø¯Ø§ÙˆÙ„ Ù…Ø®ØªÙ„ÙØ©
            possible_tables = [
                f"{symbol}_{timeframe}",
                f"{symbol}{timeframe}",
                f"{symbol}",
                "forex_data"
            ]
            
            df = None
            for table in possible_tables:
                try:
                    query = f"SELECT * FROM {table} ORDER BY time DESC LIMIT 10000"
                    df = pd.read_sql_query(query, conn)
                    if not df.empty:
                        logger.info(f"âœ… ØªÙ… Ø¬Ù„Ø¨ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù† Ø¬Ø¯ÙˆÙ„ {table}")
                        break
                except:
                    continue
            
            conn.close()
            
            if df is None or df.empty:
                logger.warning(f"âš ï¸ Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¨ÙŠØ§Ù†Ø§Øª ÙƒØ§ÙÙŠØ© Ù„Ù€ {symbol} {timeframe}")
                return False
            
            # ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØªØ§Ø±ÙŠØ®
            if 'time' in df.columns:
                df['time'] = pd.to_datetime(df['time'])
                df.set_index('time', inplace=True)
            
            # Ø­Ø³Ø§Ø¨ Ø§Ù„Ù…ÙŠØ²Ø§Øª
            features_df = self.calculate_all_features(df)
            
            # ØªØ­Ø¶ÙŠØ± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù„ØªØ¯Ø±ÙŠØ¨
            feature_cols = [col for col in features_df.columns if col not in ['open', 'high', 'low', 'close', 'volume']]
            X = features_df[feature_cols].values
            
            # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù‡Ø¯Ù (Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø§Ù„Ø§ØªØ¬Ø§Ù‡)
            y = (features_df['close'].shift(-1) > features_df['close']).astype(int)
            y = y[:-1]
            X = X[:-1]
            
            # ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
            
            # ØªØ·Ø¨ÙŠØ¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
            scaler = RobustScaler()
            X_train_scaled = scaler.fit_transform(X_train)
            X_test_scaled = scaler.transform(X_test)
            
            # Ø­ÙØ¸ Ø§Ù„Ù…Ù‚ÙŠØ§Ø³
            self.scalers[f"{symbol}_{timeframe}"] = scaler
            
            # ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ø³ØªØ©
            models = {}
            accuracies = {}
            
            # 1. LightGBM
            if LIGHTGBM_AVAILABLE:
                try:
                    lgb_model = lgb.LGBMClassifier(
                        n_estimators=100,
                        learning_rate=0.05,
                        num_leaves=31,
                        random_state=42,
                        verbosity=-1
                    )
                    lgb_model.fit(X_train_scaled, y_train)
                    lgb_pred = lgb_model.predict(X_test_scaled)
                    lgb_acc = accuracy_score(y_test, lgb_pred)
                    models['lightgbm'] = lgb_model
                    accuracies['lightgbm'] = lgb_acc
                    logger.info(f"   LightGBM Accuracy: {lgb_acc:.4f}")
                except Exception as e:
                    logger.error(f"   LightGBM Error: {str(e)}")
            
            # 2. XGBoost
            if XGBOOST_AVAILABLE:
                try:
                    xgb_model = xgb.XGBClassifier(
                        n_estimators=100,
                        learning_rate=0.05,
                        max_depth=5,
                        random_state=42,
                        use_label_encoder=False,
                        eval_metric='logloss'
                    )
                    xgb_model.fit(X_train_scaled, y_train)
                    xgb_pred = xgb_model.predict(X_test_scaled)
                    xgb_acc = accuracy_score(y_test, xgb_pred)
                    models['xgboost'] = xgb_model
                    accuracies['xgboost'] = xgb_acc
                    logger.info(f"   XGBoost Accuracy: {xgb_acc:.4f}")
                except Exception as e:
                    logger.error(f"   XGBoost Error: {str(e)}")
            
            # 3. Random Forest
            try:
                rf_model = RandomForestClassifier(
                    n_estimators=100,
                    max_depth=10,
                    random_state=42,
                    n_jobs=-1
                )
                rf_model.fit(X_train_scaled, y_train)
                rf_pred = rf_model.predict(X_test_scaled)
                rf_acc = accuracy_score(y_test, rf_pred)
                models['random_forest'] = rf_model
                accuracies['random_forest'] = rf_acc
                logger.info(f"   Random Forest Accuracy: {rf_acc:.4f}")
            except Exception as e:
                logger.error(f"   Random Forest Error: {str(e)}")
            
            # 4. Gradient Boosting
            try:
                gb_model = GradientBoostingClassifier(
                    n_estimators=100,
                    learning_rate=0.05,
                    max_depth=5,
                    random_state=42
                )
                gb_model.fit(X_train_scaled, y_train)
                gb_pred = gb_model.predict(X_test_scaled)
                gb_acc = accuracy_score(y_test, gb_pred)
                models['gradient_boosting'] = gb_model
                accuracies['gradient_boosting'] = gb_acc
                logger.info(f"   Gradient Boosting Accuracy: {gb_acc:.4f}")
            except Exception as e:
                logger.error(f"   Gradient Boosting Error: {str(e)}")
            
            # 5. Extra Trees
            try:
                et_model = ExtraTreesClassifier(
                    n_estimators=100,
                    max_depth=10,
                    random_state=42,
                    n_jobs=-1
                )
                et_model.fit(X_train_scaled, y_train)
                et_pred = et_model.predict(X_test_scaled)
                et_acc = accuracy_score(y_test, et_pred)
                models['extra_trees'] = et_model
                accuracies['extra_trees'] = et_acc
                logger.info(f"   Extra Trees Accuracy: {et_acc:.4f}")
            except Exception as e:
                logger.error(f"   Extra Trees Error: {str(e)}")
            
            # 6. Neural Network
            try:
                nn_model = MLPClassifier(
                    hidden_layer_sizes=(100, 50),
                    activation='relu',
                    solver='adam',
                    learning_rate_rate=0.001,
                    max_iter=500,
                    random_state=42
                )
                nn_model.fit(X_train_scaled, y_train)
                nn_pred = nn_model.predict(X_test_scaled)
                nn_acc = accuracy_score(y_test, nn_pred)
                models['neural_network'] = nn_model
                accuracies['neural_network'] = nn_acc
                logger.info(f"   Neural Network Accuracy: {nn_acc:.4f}")
            except Exception as e:
                logger.error(f"   Neural Network Error: {str(e)}")
            
            # Ø­ÙØ¸ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬
            self.models[f"{symbol}_{timeframe}"] = models
            
            # Ø­ÙØ¸ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø¹Ù„Ù‰ Ø§Ù„Ù‚Ø±Øµ
            for model_name, model in models.items():
                model_path = os.path.join(self.models_dir, f"{symbol}_{timeframe}_{model_name}.pkl")
                joblib.dump(model, model_path)
            
            # Ø­ÙØ¸ Ø§Ù„Ù…Ù‚ÙŠØ§Ø³
            scaler_path = os.path.join(self.models_dir, f"{symbol}_{timeframe}_scaler.pkl")
            joblib.dump(scaler, scaler_path)
            
            logger.info(f"âœ… ØªÙ… ØªØ¯Ø±ÙŠØ¨ {len(models)} Ù†Ù…Ø§Ø°Ø¬ Ù„Ù€ {symbol} {timeframe}")
            return True
            
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬: {str(e)}")
            return False
    
    def predict(self, symbol, timeframe, features_df):
        """Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ ÙˆØ§Ù„ÙØ±Ø¶ÙŠØ§Øª"""
        try:
            key = f"{symbol}_{timeframe}"
            
            # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ù†Ù…Ø§Ø°Ø¬ Ù…Ø¯Ø±Ø¨Ø©
            if key not in self.models or not self.models[key]:
                logger.info(f"ğŸ§  ØªØ¯Ø±ÙŠØ¨ Ù†Ù…Ø§Ø°Ø¬ Ø¬Ø¯ÙŠØ¯Ø© Ù„Ù€ {symbol} {timeframe}...")
                if not self.train_models(symbol, timeframe):
                    # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø³ÙŠØ·
                    return self._simple_prediction(features_df)
            
            # ØªØ­Ø¶ÙŠØ± Ø§Ù„Ù…ÙŠØ²Ø§Øª
            feature_cols = [col for col in features_df.columns if col not in ['open', 'high', 'low', 'close', 'volume']]
            X = features_df[feature_cols].values
            
            # ØªØ·Ø¨ÙŠØ¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
            if key in self.scalers:
                X_scaled = self.scalers[key].transform(X)
            else:
                X_scaled = X
            
            # Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨ÙƒÙ„ Ù†Ù…ÙˆØ°Ø¬
            predictions = {}
            probabilities = {}
            
            for model_name, model in self.models[key].items():
                try:
                    pred = model.predict(X_scaled)
                    prob = model.predict_proba(X_scaled)[:, 1]
                    predictions[model_name] = pred[0]
                    probabilities[model_name] = prob[0]
                except:
                    continue
            
            if not predictions:
                return self._simple_prediction(features_df)
            
            # Ø­Ø³Ø§Ø¨ Ø§Ù„ØªØµÙˆÙŠØª
            buy_votes = sum(1 for p in predictions.values() if p == 1)
            sell_votes = sum(1 for p in predictions.values() if p == 0)
            avg_confidence = np.mean(list(probabilities.values()))
            
            # ØªÙ‚ÙŠÙŠÙ… Ø§Ù„ÙØ±Ø¶ÙŠØ§Øª
            latest_features = features_df.iloc[-1].to_dict()
            hypotheses_results = self.evaluate_hypotheses(latest_features)
            
            # Ø¯Ù…Ø¬ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ (70% ML + 30% ÙØ±Ø¶ÙŠØ§Øª)
            hypotheses_score = np.mean(list(hypotheses_results.values()))
            
            if buy_votes > sell_votes:
                final_prediction = 0  # Buy
                ml_confidence = avg_confidence
            else:
                final_prediction = 1  # Sell
                ml_confidence = 1 - avg_confidence
            
            # Ø§Ù„Ø«Ù‚Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©
            final_confidence = (0.7 * ml_confidence) + (0.3 * hypotheses_score)
            
            # ØªØ³Ø¬ÙŠÙ„ Ø§Ù„ØªÙØ§ØµÙŠÙ„
            logger.info(f"   ğŸ“Š ØªØµÙˆÙŠØª Ø§Ù„Ù†Ù…Ø§Ø°Ø¬: Buy={buy_votes}, Sell={sell_votes}")
            logger.info(f"   ğŸ¯ Ø§Ù„ÙØ±Ø¶ÙŠØ§Øª: {hypotheses_score:.2f}")
            logger.info(f"   ğŸ’ª Ø§Ù„Ø«Ù‚Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©: {final_confidence:.2%}")
            
            return final_prediction, final_confidence
            
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ØªÙ†Ø¨Ø¤: {str(e)}")
            return self._simple_prediction(features_df)
    
    def _simple_prediction(self, features_df):
        """ØªÙ†Ø¨Ø¤ Ø¨Ø³ÙŠØ· ÙÙŠ Ø­Ø§Ù„Ø© Ø¹Ø¯Ù… ØªÙˆÙØ± Ø§Ù„Ù†Ù…Ø§Ø°Ø¬"""
        try:
            latest = features_df.iloc[-1]
            
            # Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© MA Ø¨Ø³ÙŠØ·Ø©
            if 'sma_20' in latest and 'sma_50' in latest:
                if latest['close'] > latest['sma_20'] > latest['sma_50']:
                    return 0, 0.65  # Buy
                elif latest['close'] < latest['sma_20'] < latest['sma_50']:
                    return 1, 0.65  # Sell
            
            return 2, 0.5  # Hold
        except:
            return 2, 0.5
    
    def calculate_dynamic_levels(self, features_df, action, symbol):
        """Ø­Ø³Ø§Ø¨ Ù…Ø³ØªÙˆÙŠØ§Øª SL/TP Ø§Ù„Ø¯ÙŠÙ†Ø§Ù…ÙŠÙƒÙŠØ©"""
        try:
            latest = features_df.iloc[-1]
            current_price = latest['close']
            
            # Ø­Ø³Ø§Ø¨ pip value
            pip_value = 0.01 if 'JPY' in symbol else 0.0001
            
            # ATR Ù„Ù„ØªÙ‚Ù„Ø¨
            atr = latest.get('atr_14', 50 * pip_value)
            
            # Ø­Ø³Ø§Ø¨ SL Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ ATR
            sl_multiplier = 1.5
            if latest.get('adx_14', 0) > 30:  # ØªØ±Ù†Ø¯ Ù‚ÙˆÙŠ
                sl_multiplier = 2.0
            elif latest.get('adx_14', 0) < 20:  # Ø³ÙˆÙ‚ Ø¬Ø§Ù†Ø¨ÙŠ
                sl_multiplier = 1.0
            
            sl_pips = max(min(atr / pip_value * sl_multiplier, 100), 20)
            
            # Ø­Ø³Ø§Ø¨ TP Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ù†Ø³Ø¨Ø© Ø§Ù„Ù…Ø®Ø§Ø·Ø±Ø©/Ø§Ù„Ù…ÙƒØ§ÙØ£Ø©
            tp1_multiplier = 2.0
            tp2_multiplier = 3.0
            
            # ØªØ¹Ø¯ÙŠÙ„ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ù‚ÙˆØ© Ø§Ù„Ø¥Ø´Ø§Ø±Ø©
            if latest.get('rsi_14', 50) < 30 or latest.get('rsi_14', 50) > 70:
                tp1_multiplier = 2.5
                tp2_multiplier = 4.0
            
            tp1_pips = sl_pips * tp1_multiplier
            tp2_pips = sl_pips * tp2_multiplier
            
            # Ø­Ø³Ø§Ø¨ Ø§Ù„Ø£Ø³Ø¹Ø§Ø±
            if action == 'BUY':
                sl_price = current_price - (sl_pips * pip_value)
                tp1_price = current_price + (tp1_pips * pip_value)
                tp2_price = current_price + (tp2_pips * pip_value)
            else:  # SELL
                sl_price = current_price + (sl_pips * pip_value)
                tp1_price = current_price - (tp1_pips * pip_value)
                tp2_price = current_price - (tp2_pips * pip_value)
            
            return {
                'sl_price': float(sl_price),
                'tp1_price': float(tp1_price),
                'tp2_price': float(tp2_price),
                'sl_pips': float(sl_pips),
                'tp1_pips': float(tp1_pips),
                'tp2_pips': float(tp2_pips)
            }
            
        except Exception as e:
            logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø­Ø³Ø§Ø¨ Ø§Ù„Ù…Ø³ØªÙˆÙŠØ§Øª: {str(e)}")
            # Ù‚ÙŠÙ… Ø§ÙØªØ±Ø§Ø¶ÙŠØ©
            pip_value = 0.01 if 'JPY' in symbol else 0.0001
            current_price = features_df['close'].iloc[-1]
            
            if action == 'BUY':
                return {
                    'sl_price': current_price - (50 * pip_value),
                    'tp1_price': current_price + (100 * pip_value),
                    'tp2_price': current_price + (150 * pip_value),
                    'sl_pips': 50,
                    'tp1_pips': 100,
                    'tp2_pips': 150
                }
            else:
                return {
                    'sl_price': current_price + (50 * pip_value),
                    'tp1_price': current_price - (100 * pip_value),
                    'tp2_price': current_price - (150 * pip_value),
                    'sl_pips': 50,
                    'tp1_pips': 100,
                    'tp2_pips': 150
                }
    
    def record_trade_result(self, trade_data):
        """ØªØ³Ø¬ÙŠÙ„ Ù†ØªÙŠØ¬Ø© Ø§Ù„ØµÙÙ‚Ø© Ù„Ù„ØªØ¹Ù„Ù… Ø§Ù„Ù…Ø³ØªÙ…Ø±"""
        try:
            conn = sqlite3.connect(self.trading_db)
            cursor = conn.cursor()
            
            cursor.execute('''
                INSERT INTO trades (
                    symbol, timeframe, signal_time, action, confidence,
                    entry_price, sl_price, tp1_price, tp2_price,
                    exit_time, exit_price, profit_pips, exit_reason
                ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            ''', (
                trade_data['symbol'],
                trade_data['timeframe'],
                trade_data.get('signal_time', datetime.now()),
                trade_data['action'],
                trade_data.get('confidence', 0),
                trade_data['entry_price'],
                trade_data.get('sl_price', 0),
                trade_data.get('tp1_price', 0),
                trade_data.get('tp2_price', 0),
                trade_data.get('exit_time', datetime.now()),
                trade_data['exit_price'],
                trade_data['profit_pips'],
                trade_data.get('exit_reason', 'Unknown')
            ))
            
            conn.commit()
            conn.close()
            
            # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†Ù…Ø· Ø¥Ø°Ø§ ÙƒØ§Ù†Øª ØµÙÙ‚Ø© Ø±Ø§Ø¨Ø­Ø©
            if trade_data['profit_pips'] > 20:
                self._analyze_winning_pattern(trade_data)
            
            logger.info(f"âœ… ØªÙ… ØªØ³Ø¬ÙŠÙ„ Ù†ØªÙŠØ¬Ø© Ø§Ù„ØµÙÙ‚Ø©: {trade_data['profit_pips']} pips")
            
        except Exception as e:
            logger.error(f"Ø®Ø·Ø£ ÙÙŠ ØªØ³Ø¬ÙŠÙ„ Ø§Ù„ØµÙÙ‚Ø©: {str(e)}")
    
    def _analyze_winning_pattern(self, trade_data):
        """ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ø±Ø§Ø¨Ø­Ø©"""
        try:
            # Ù‡Ù†Ø§ ÙŠÙ…ÙƒÙ† Ø¥Ø¶Ø§ÙØ© ØªØ­Ù„ÙŠÙ„ Ù…ØªÙ‚Ø¯Ù… Ù„Ù„Ø£Ù†Ù…Ø§Ø·
            conn = sqlite3.connect(PATTERNS_DB)
            cursor = conn.cursor()
            
            pattern_type = "MA_Cross" if trade_data.get('pattern_type') else "Unknown"
            
            cursor.execute('''
                INSERT INTO winning_patterns (
                    symbol, timeframe, pattern_type, success_rate,
                    total_trades, avg_profit_pips, created_at, last_updated
                ) VALUES (?, ?, ?, ?, ?, ?, ?, ?)
            ''', (
                trade_data['symbol'],
                trade_data['timeframe'],
                pattern_type,
                1.0,  # 100% Ù„Ø£Ù†Ù‡Ø§ ØµÙÙ‚Ø© Ø±Ø§Ø¨Ø­Ø©
                1,
                trade_data['profit_pips'],
                datetime.now(),
                datetime.now()
            ))
            
            conn.commit()
            conn.close()
            
        except Exception as e:
            logger.error(f"Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†Ù…Ø·: {str(e)}")
    
    def _continuous_learning_loop(self):
        """Ø­Ù„Ù‚Ø© Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ù…Ø³ØªÙ…Ø±"""
        while True:
            try:
                # Ø§Ù†ØªØ¸Ø§Ø± 5 Ø¯Ù‚Ø§Ø¦Ù‚
                threading.Event().wait(300)
                
                # ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØµÙÙ‚Ø§Øª Ø§Ù„Ø£Ø®ÙŠØ±Ø©
                conn = sqlite3.connect(self.trading_db)
                recent_trades = pd.read_sql_query(
                    "SELECT * FROM trades WHERE exit_time > datetime('now', '-1 day')",
                    conn
                )
                conn.close()
                
                if not recent_trades.empty:
                    # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ø¯Ø§Ø¡
                    win_rate = (recent_trades['profit_pips'] > 0).mean()
                    avg_profit = recent_trades['profit_pips'].mean()
                    
                    logger.info(f"ğŸ“Š Ø£Ø¯Ø§Ø¡ Ø¢Ø®Ø± 24 Ø³Ø§Ø¹Ø©: Win Rate={win_rate:.1%}, Avg Profit={avg_profit:.1f} pips")
                    
                    # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø¥Ø°Ø§ Ù„Ø²Ù… Ø§Ù„Ø£Ù…Ø±
                    if win_rate < 0.5:
                        logger.info("âš ï¸ Ø§Ù„Ø£Ø¯Ø§Ø¡ Ù…Ù†Ø®ÙØ¶ØŒ ÙŠÙÙ†ØµØ­ Ø¨Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„ØªØ¯Ø±ÙŠØ¨")
                
            except Exception as e:
                logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ù…Ø³ØªÙ…Ø±: {str(e)}")
    
    def load_existing_models(self):
        """ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…Ø¯Ø±Ø¨Ø© Ù…Ø³Ø¨Ù‚Ø§Ù‹"""
        try:
            if not os.path.exists(self.models_dir):
                return
            
            model_files = [f for f in os.listdir(self.models_dir) if f.endswith('.pkl')]
            
            for model_file in model_files:
                try:
                    parts = model_file.replace('.pkl', '').split('_')
                    if len(parts) >= 3:
                        symbol = parts[0]
                        timeframe = parts[1]
                        model_type = '_'.join(parts[2:])
                        
                        if model_type == 'scaler':
                            scaler = joblib.load(os.path.join(self.models_dir, model_file))
                            self.scalers[f"{symbol}_{timeframe}"] = scaler
                        else:
                            key = f"{symbol}_{timeframe}"
                            if key not in self.models:
                                self.models[key] = {}
                            
                            model = joblib.load(os.path.join(self.models_dir, model_file))
                            self.models[key][model_type] = model
                            
                except Exception as e:
                    logger.error(f"Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù…ÙŠÙ„ {model_file}: {str(e)}")
            
            total_models = sum(len(models) for models in self.models.values())
            logger.info(f"âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ {total_models} Ù†Ù…ÙˆØ°Ø¬ Ù…Ù† Ø§Ù„Ù…Ù„ÙØ§Øª")
            
        except Exception as e:
            logger.error(f"Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬: {str(e)}")

# Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø«ÙŠÙ„ Ù…Ù† Ø§Ù„Ù†Ø¸Ø§Ù…
system = CompleteForexMLSystem()

# Ù…ØªØºÙŠØ±Ø§Øª Ù„Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
server_stats = {
    'start_time': datetime.now(),
    'total_requests': 0,
    'total_signals': 0,
    'active_trades': 0
}

@app.route('/status', methods=['GET'])
def status():
    """Ø­Ø§Ù„Ø© Ø§Ù„Ø³ÙŠØ±ÙØ±"""
    uptime = datetime.now() - server_stats['start_time']
    total_models = sum(len(models) for models in system.models.values())
    
    return jsonify({
        'status': 'running',
        'version': '4.0-complete',
        'server': '69.62.121.53:5000',
        'uptime': str(uptime),
        'models_loaded': total_models,
        'total_requests': server_stats['total_requests'],
        'total_signals': server_stats['total_signals'],
        'features': '200+',
        'ml_models': 6,
        'hypotheses': 10,
        'continuous_learning': 'active'
    })

@app.route('/predict', methods=['POST'])
def predict():
    """Ù†Ù‚Ø·Ø© Ø§Ù„Ù†Ù‡Ø§ÙŠØ© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ù„Ù„ØªÙ†Ø¨Ø¤ - Ù…Ø­Ø³Ù†Ø© Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© JSON Ø§Ù„ÙƒØ¨ÙŠØ±"""
    try:
        server_stats['total_requests'] += 1
        
        # Ù…Ø¹Ø§Ù„Ø¬Ø© JSON Ù…Ø­Ø³Ù†Ø©
        try:
            # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø®Ø§Ù…
            raw_data = request.get_data(as_text=True)
            logger.info(f"ğŸ“¥ Ø­Ø¬Ù… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø³ØªÙ„Ù…Ø©: {len(raw_data)} Ø­Ø±Ù")
            
            # Ù…Ø­Ø§ÙˆÙ„Ø© ØªØ­Ù„ÙŠÙ„ JSON
            try:
                data = json.loads(raw_data)
            except json.JSONDecodeError as e:
                logger.error(f"Ø®Ø·Ø£ JSON ÙÙŠ Ø§Ù„Ù…ÙˆØ¶Ø¹ {e.pos}")
                
                # Ù…Ø­Ø§ÙˆÙ„Ø© Ø¥ØµÙ„Ø§Ø­ JSON Ø§Ù„Ù…ÙƒØ³ÙˆØ±
                if '"candles":[' in raw_data:
                    # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ù†Ù‡Ø§ÙŠØ© Ù…ØµÙÙˆÙØ© Ø§Ù„Ø´Ù…ÙˆØ¹
                    candles_start = raw_data.find('"candles":[')
                    if candles_start != -1:
                        # Ù…Ø­Ø§ÙˆÙ„Ø© Ø¥ÙŠØ¬Ø§Ø¯ Ø§Ù„Ù†Ù‡Ø§ÙŠØ© Ø§Ù„ØµØ­ÙŠØ­Ø©
                        bracket_count = 0
                        in_string = False
                        escape_next = False
                        
                        i = candles_start + len('"candles":[')
                        while i < len(raw_data):
                            char = raw_data[i]
                            
                            if escape_next:
                                escape_next = False
                            elif char == '\\':
                                escape_next = True
                            elif char == '"' and not escape_next:
                                in_string = not in_string
                            elif not in_string:
                                if char == '[':
                                    bracket_count += 1
                                elif char == ']':
                                    if bracket_count == 0:
                                        # ÙˆØ¬Ø¯Ù†Ø§ Ù†Ù‡Ø§ÙŠØ© Ø§Ù„Ù…ØµÙÙˆÙØ©
                                        try:
                                            fixed_json = raw_data[:i+1] + '}'
                                            data = json.loads(fixed_json)
                                            logger.info("âœ… ØªÙ… Ø¥ØµÙ„Ø§Ø­ JSON Ø¨Ù†Ø¬Ø§Ø­")
                                            break
                                        except:
                                            pass
                                    else:
                                        bracket_count -= 1
                            i += 1
                
                # Ø¥Ø°Ø§ ÙØ´Ù„ Ø§Ù„Ø¥ØµÙ„Ø§Ø­ØŒ Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¬Ø²Ø¦ÙŠØ©
                if 'data' not in locals():
                    if e.pos > 0:
                        try:
                            partial_data = raw_data[:e.pos]
                            # Ù…Ø­Ø§ÙˆÙ„Ø© Ø¥ØºÙ„Ø§Ù‚ JSON
                            open_brackets = partial_data.count('[') - partial_data.count(']')
                            open_braces = partial_data.count('{') - partial_data.count('}')
                            
                            closing = ']' * open_brackets + '}' * open_braces
                            fixed_json = partial_data + closing
                            
                            data = json.loads(fixed_json)
                            logger.warning("âš ï¸ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø¨ÙŠØ§Ù†Ø§Øª JSON Ø¬Ø²Ø¦ÙŠØ©")
                        except:
                            return jsonify({
                                'error': 'Invalid JSON format',
                                'action': 'NONE',
                                'confidence': 0
                            }), 200
                    else:
                        return jsonify({
                            'error': 'Empty or invalid JSON',
                            'action': 'NONE',
                            'confidence': 0
                        }), 200
        
        except Exception as e:
            logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø·Ù„Ø¨: {str(e)}")
            return jsonify({
                'error': 'Request processing error',
                'action': 'NONE',
                'confidence': 0
            }), 200
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        symbol = data.get('symbol', 'UNKNOWN')
        timeframe = data.get('timeframe', 'M15')
        candles = data.get('candles', [])
        
        logger.info(f"\nğŸ“Š Ø·Ù„Ø¨ ØªÙ†Ø¨Ø¤: {symbol} {timeframe}")
        logger.info(f"   Ø¹Ø¯Ø¯ Ø§Ù„Ø´Ù…ÙˆØ¹: {len(candles)}")
        
        # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø´Ù…ÙˆØ¹
        if not candles or len(candles) < 20:
            logger.warning(f"âš ï¸ Ø¹Ø¯Ø¯ Ø§Ù„Ø´Ù…ÙˆØ¹ ØºÙŠØ± ÙƒØ§ÙÙŠ: {len(candles)}")
            return jsonify({
                'symbol': symbol,
                'timeframe': timeframe,
                'action': 'NONE',
                'confidence': 0,
                'error': f'Need at least 20 candles, got {len(candles)}'
            }), 200
        
        # ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ DataFrame
        try:
            df = pd.DataFrame(candles)
            
            # Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø£Ù† Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø±Ù‚Ù…ÙŠØ©
            for col in ['open', 'high', 'low', 'close', 'volume']:
                if col in df.columns:
                    df[col] = pd.to_numeric(df[col], errors='coerce')
            
            # ØªØ­ÙˆÙŠÙ„ Ø§Ù„ÙˆÙ‚Øª
            if 'time' in df.columns:
                df['time'] = pd.to_datetime(df['time'])
                df.set_index('time', inplace=True)
            
            # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ù†Ø§Ù‚ØµØ©
            df = df.dropna()
            
            if df.empty:
                raise ValueError("No valid data after cleaning")
                
        except Exception as e:
            logger.error(f"Ø®Ø·Ø£ ÙÙŠ ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: {str(e)}")
            return jsonify({
                'symbol': symbol,
                'timeframe': timeframe,
                'action': 'NONE',
                'confidence': 0,
                'error': 'Invalid candle data'
            }), 200
        
        # Ø­Ø³Ø§Ø¨ Ø§Ù„Ù…ÙŠØ²Ø§Øª
        try:
            features = system.calculate_all_features(df)
            if features.empty:
                raise ValueError("No features calculated")
        except Exception as e:
            logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø­Ø³Ø§Ø¨ Ø§Ù„Ù…ÙŠØ²Ø§Øª: {str(e)}")
            # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…ÙŠØ²Ø§Øª Ø¨Ø³ÙŠØ·Ø©
            features = df.copy()
            features['sma_20'] = features['close'].rolling(20).mean()
            features['sma_50'] = features['close'].rolling(50).mean()
            features = features.dropna()
        
        # Ø§Ù„ØªÙ†Ø¨Ø¤
        try:
            prediction, confidence = system.predict(symbol, timeframe, features)
        except Exception as e:
            logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ØªÙ†Ø¨Ø¤: {str(e)}")
            prediction, confidence = 2, 0.5
        
        # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø¥Ø´Ø§Ø±Ø©
        current_price = float(df['close'].iloc[-1])
        
        if prediction == 0 and confidence >= 0.65:
            action = 'BUY'
        elif prediction == 1 and confidence >= 0.65:
            action = 'SELL'
        else:
            action = 'NONE'
        
        # Ø­Ø³Ø§Ø¨ SL/TP
        if action != 'NONE':
            levels = system.calculate_dynamic_levels(features, action, symbol)
        else:
            pip_value = 0.01 if 'JPY' in symbol else 0.0001
            levels = {
                'sl_price': current_price,
                'tp1_price': current_price,
                'tp2_price': current_price,
                'sl_pips': 0,
                'tp1_pips': 0,
                'tp2_pips': 0
            }
        
        # Ø¨Ù†Ø§Ø¡ Ø§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø©
        response = {
            'symbol': symbol,
            'timeframe': timeframe,
            'action': action,
            'confidence': float(confidence),
            'current_price': current_price,
            'sl_price': levels['sl_price'],
            'tp1_price': levels['tp1_price'],
            'tp2_price': levels['tp2_price'],
            'sl_pips': levels['sl_pips'],
            'tp1_pips': levels['tp1_pips'],
            'tp2_pips': levels['tp2_pips'],
            'risk_reward_ratio': levels['tp1_pips'] / levels['sl_pips'] if levels['sl_pips'] > 0 else 0,
            'timestamp': datetime.now().isoformat(),
            'models_used': len(system.models.get(f"{symbol}_{timeframe}", {})),
            'features_count': len(features.columns)
        }
        
        if action != 'NONE':
            server_stats['total_signals'] += 1
            
        logger.info(f"   âœ… Ø¥Ø´Ø§Ø±Ø© {action} Ø¨Ø«Ù‚Ø© {confidence:.1%}")
        
        return jsonify(response)
        
    except Exception as e:
        logger.error(f"Ø®Ø·Ø£ ØºÙŠØ± Ù…ØªÙˆÙ‚Ø¹: {str(e)}")
        import traceback
        traceback.print_exc()
        
        return jsonify({
            'error': str(e),
            'action': 'NONE',
            'confidence': 0
        }), 200

@app.route('/trade_result', methods=['POST'])
def trade_result():
    """ØªØ³Ø¬ÙŠÙ„ Ù†ØªÙŠØ¬Ø© Ø§Ù„ØµÙÙ‚Ø©"""
    try:
        data = request.json
        system.record_trade_result(data)
        return jsonify({'status': 'success', 'message': 'Trade result recorded'})
    except Exception as e:
        logger.error(f"Ø®Ø·Ø£ ÙÙŠ ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ù†ØªÙŠØ¬Ø©: {str(e)}")
        return jsonify({'status': 'error', 'message': str(e)}), 500

@app.route('/train', methods=['POST'])
def train():
    """ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ ÙŠØ¯ÙˆÙŠØ§Ù‹"""
    try:
        data = request.json
        symbol = data.get('symbol')
        timeframe = data.get('timeframe')
        
        if not symbol or not timeframe:
            return jsonify({'error': 'Symbol and timeframe required'}), 400
        
        success = system.train_models(symbol, timeframe)
        
        if success:
            return jsonify({
                'status': 'success',
                'message': f'Models trained for {symbol} {timeframe}'
            })
        else:
            return jsonify({
                'status': 'error',
                'message': 'Training failed - insufficient data'
            }), 400
            
    except Exception as e:
        logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ØªØ¯Ø±ÙŠØ¨: {str(e)}")
        return jsonify({'status': 'error', 'message': str(e)}), 500

@app.route('/models', methods=['GET'])
def get_models():
    """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…Ø¯Ø±Ø¨Ø©"""
    models_info = {}
    
    for key, models in system.models.items():
        models_info[key] = {
            'models': list(models.keys()),
            'count': len(models)
        }
    
    return jsonify({
        'total_pairs': len(system.models),
        'models': models_info
    })

if __name__ == '__main__':
    logger.info("\n" + "="*80)
    logger.info("ğŸš€ FOREX ML TRADING SERVER - Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„ÙƒØ§Ù…Ù„")
    logger.info("ğŸ“Š Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ù†Ø´Ø·Ø©")
    logger.info("ğŸŒ Ø§Ù„Ø³ÙŠØ±ÙØ±: http://69.62.121.53:5000")
    logger.info("ğŸ¤– 6 Ù†Ù…Ø§Ø°Ø¬ ML | 200+ Ù…ÙŠØ²Ø© | SL/TP Ø¯ÙŠÙ†Ø§Ù…ÙŠÙƒÙŠ")
    logger.info("="*80 + "\n")
    
    # ØªØ´ØºÙŠÙ„ Ø§Ù„Ø³ÙŠØ±ÙØ±
    app.run(host='0.0.0.0', port=5000, debug=False)