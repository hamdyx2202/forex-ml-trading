#!/usr/bin/env python3
"""
Ultimate Model Training System - Ù†Ø¸Ø§Ù… Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ø´Ø§Ù…Ù„ Ø§Ù„Ù…ØªÙƒØ§Ù…Ù„
ÙŠØ¯Ù…Ø¬ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø©: Ø¯Ø¹Ù…/Ù…Ù‚Ø§ÙˆÙ…Ø©ØŒ ATR Ù…ØªØ¹Ø¯Ø¯ØŒ Ø£Ù‡Ø¯Ø§Ù Ù…ØªÙ†ÙˆØ¹Ø©ØŒ ØªØ¹Ù„Ù… Ù…Ø³ØªÙ…Ø±
"""

import os
import sys
import json
import sqlite3
import numpy as np
import pandas as pd
from datetime import datetime, timedelta
from pathlib import Path
import joblib
import warnings
warnings.filterwarnings('ignore')

# Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ù…Ø³Ø§Ø± Ù„Ù„Ù…Ø´Ø±ÙˆØ¹
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from loguru import logger
from sklearn.model_selection import train_test_split, TimeSeriesSplit
from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.ensemble import VotingClassifier
import lightgbm as lgb
import xgboost as xgb
from catboost import CatBoostClassifier
import optuna

# Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ø£Ù†Ø¸Ù…Ø© Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø©
from src.feature_engineer_fixed_v2 import FeatureEngineer
from support_resistance import SupportResistanceCalculator
from src.dynamic_sl_tp_system import DynamicSLTPSystem
from src.advanced_learner import AdvancedHistoricalLearner
from src.continuous_learner import ContinuousLearner

# Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ³Ø¬ÙŠÙ„
logger.remove()
logger.add(sys.stdout, format="<green>{time:YYYY-MM-DD HH:mm:ss}</green> | <level>{level: <8}</level> | <level>{message}</level>")
logger.add("logs/ultimate_training.log", rotation="1 day", retention="30 days")

class UltimateModelTrainer:
    """Ù†Ø¸Ø§Ù… Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ø´Ø§Ù…Ù„ Ø§Ù„Ù…ØªÙƒØ§Ù…Ù„"""
    
    def __init__(self):
        self.feature_engineer = FeatureEngineer()
        self.sr_calculator = SupportResistanceCalculator()
        self.sltp_system = DynamicSLTPSystem()
        self.advanced_learner = AdvancedHistoricalLearner()
        self.continuous_learner = ContinuousLearner()
        
        # Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨
        self.min_data_points = 5000
        self.test_size = 0.2
        self.validation_split = 0.1
        self.random_state = 42
        
        # Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø£Ù‡Ø¯Ø§Ù Ø§Ù„Ù…ØªØ¹Ø¯Ø¯Ø©
        self.target_configs = [
            {'name': 'target_5m', 'minutes': 5, 'min_pips': 5},
            {'name': 'target_15m', 'minutes': 15, 'min_pips': 10},
            {'name': 'target_30m', 'minutes': 30, 'min_pips': 15},
            {'name': 'target_1h', 'minutes': 60, 'min_pips': 20},
            {'name': 'target_4h', 'minutes': 240, 'min_pips': 40},
        ]
        
        # Ù…Ø³ØªÙˆÙŠØ§Øª SL/TP Ù…Ø®ØªÙ„ÙØ©
        self.sltp_strategies = [
            {'name': 'conservative', 'risk_reward': 1.5, 'atr_multiplier': 1.0},
            {'name': 'balanced', 'risk_reward': 2.0, 'atr_multiplier': 1.5},
            {'name': 'aggressive', 'risk_reward': 3.0, 'atr_multiplier': 2.0},
            {'name': 'scalping', 'risk_reward': 1.0, 'atr_multiplier': 0.5},
            {'name': 'swing', 'risk_reward': 4.0, 'atr_multiplier': 3.0},
        ]
        
        # Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ù†Ù…Ø§Ø°Ø¬
        self.model_configs = {
            'lightgbm': {
                'objective': 'multiclass',
                'num_class': 3,
                'metric': 'multi_logloss',
                'boosting_type': 'gbdt',
                'num_leaves': 64,
                'learning_rate': 0.05,
                'feature_fraction': 0.8,
                'bagging_fraction': 0.8,
                'bagging_freq': 5,
                'verbose': -1
            },
            'xgboost': {
                'objective': 'multi:softprob',
                'num_class': 3,
                'max_depth': 8,
                'learning_rate': 0.05,
                'subsample': 0.8,
                'colsample_bytree': 0.8,
                'eval_metric': 'mlogloss',
                'use_label_encoder': False
            },
            'catboost': {
                'loss_function': 'MultiClass',
                'classes_count': 3,
                'depth': 8,
                'learning_rate': 0.05,
                'l2_leaf_reg': 5,
                'random_state': 42,
                'verbose': False
            }
        }
        
    def get_all_symbols_from_db(self):
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø¹Ù…Ù„Ø§Øª Ù…Ù† Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª"""
        try:
            conn = sqlite3.connect("data/forex_data.db")
            query = """
                SELECT DISTINCT symbol, timeframe, COUNT(*) as count
                FROM price_data
                GROUP BY symbol, timeframe
                HAVING count >= ?
                ORDER BY symbol, timeframe
            """
            df = pd.read_sql_query(query, conn, params=(self.min_data_points,))
            conn.close()
            
            logger.info(f"âœ… ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ {len(df)} Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ù…ØªØ§Ø­Ø©")
            return df
            
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ø¹Ù…Ù„Ø§Øª: {e}")
            return pd.DataFrame()
    
    def load_data_with_sr_levels(self, symbol, timeframe, limit=100000):
        """ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ø¹ Ù…Ø³ØªÙˆÙŠØ§Øª Ø§Ù„Ø¯Ø¹Ù… ÙˆØ§Ù„Ù…Ù‚Ø§ÙˆÙ…Ø©"""
        try:
            conn = sqlite3.connect("data/forex_data.db")
            query = """
                SELECT * FROM price_data 
                WHERE symbol = ? AND timeframe = ?
                ORDER BY time DESC
                LIMIT ?
            """
            df = pd.read_sql_query(query, conn, params=(symbol, timeframe, limit))
            conn.close()
            
            if df.empty:
                return None
                
            df = df.sort_values('time').reset_index(drop=True)
            
            # ØªØ­ÙˆÙŠÙ„ Ø§Ù„ÙˆÙ‚Øª
            df['time'] = pd.to_datetime(df['time'], unit='s')
            
            # Ø­Ø³Ø§Ø¨ Ù…Ø³ØªÙˆÙŠØ§Øª Ø§Ù„Ø¯Ø¹Ù… ÙˆØ§Ù„Ù…Ù‚Ø§ÙˆÙ…Ø©
            sr_levels = self.sr_calculator.calculate_all_levels(df, symbol)
            
            # Ø¥Ø¶Ø§ÙØ© Ù…Ø³ØªÙˆÙŠØ§Øª Ø§Ù„Ø¯Ø¹Ù… ÙˆØ§Ù„Ù…Ù‚Ø§ÙˆÙ…Ø© ÙƒÙ…ÙŠØ²Ø§Øª
            df = self._add_sr_features(df, sr_levels)
            
            logger.info(f"âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ {len(df)} Ø³Ø¬Ù„ Ù„Ù€ {symbol} {timeframe} Ù…Ø¹ Ù…Ø³ØªÙˆÙŠØ§Øª S/R")
            return df
            
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: {e}")
            return None
    
    def _add_sr_features(self, df, sr_levels):
        """Ø¥Ø¶Ø§ÙØ© Ù…ÙŠØ²Ø§Øª Ø§Ù„Ø¯Ø¹Ù… ÙˆØ§Ù„Ù…Ù‚Ø§ÙˆÙ…Ø©"""
        # Ø£Ù‚Ø±Ø¨ Ù…Ø³ØªÙˆÙ‰ Ø¯Ø¹Ù… ÙˆÙ…Ù‚Ø§ÙˆÙ…Ø©
        df['nearest_support'] = 0.0
        df['nearest_resistance'] = 0.0
        df['distance_to_support'] = 0.0
        df['distance_to_resistance'] = 0.0
        df['sr_strength'] = 0.0
        
        all_levels = []
        
        # Ø¬Ù…Ø¹ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ø³ØªÙˆÙŠØ§Øª
        for method, levels in sr_levels.items():
            for level in levels:
                all_levels.append({
                    'price': level['price'],
                    'strength': level['strength'],
                    'type': level['type']
                })
        
        # Ù„ÙƒÙ„ Ø´Ù…Ø¹Ø©ØŒ Ø­Ø³Ø§Ø¨ Ø£Ù‚Ø±Ø¨ Ù…Ø³ØªÙˆÙŠØ§Øª
        for idx in range(len(df)):
            current_price = df.loc[idx, 'close']
            
            supports = [l for l in all_levels if l['price'] < current_price]
            resistances = [l for l in all_levels if l['price'] > current_price]
            
            if supports:
                nearest_sup = max(supports, key=lambda x: x['price'])
                df.loc[idx, 'nearest_support'] = nearest_sup['price']
                df.loc[idx, 'distance_to_support'] = (current_price - nearest_sup['price']) / current_price
                df.loc[idx, 'sr_strength'] = nearest_sup['strength']
            
            if resistances:
                nearest_res = min(resistances, key=lambda x: x['price'])
                df.loc[idx, 'nearest_resistance'] = nearest_res['price']
                df.loc[idx, 'distance_to_resistance'] = (nearest_res['price'] - current_price) / current_price
        
        return df
    
    def create_multiple_targets(self, df, symbol):
        """Ø¥Ù†Ø´Ø§Ø¡ Ø£Ù‡Ø¯Ø§Ù Ù…ØªØ¹Ø¯Ø¯Ø© Ø¨Ø£ÙˆÙ‚Ø§Øª ÙˆØ£Ø­Ø¬Ø§Ù… Ù…Ø®ØªÙ„ÙØ©"""
        targets = {}
        
        # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø¹Ù…Ù„Ø©
        symbol_info = self.sltp_system.get_symbol_info(symbol)
        pip_value = symbol_info['pip_size']
        
        for config in self.target_configs:
            target_name = config['name']
            minutes = config['minutes']
            min_pips = config['min_pips']
            
            # Ø­Ø³Ø§Ø¨ Ø§Ù„Ù‡Ø¯Ù
            target = []
            for i in range(len(df) - minutes):
                future_prices = df.iloc[i:i+minutes]['close'].values
                current_price = df.iloc[i]['close']
                
                max_price = future_prices.max()
                min_price = future_prices.min()
                
                # Ø­Ø³Ø§Ø¨ Ø§Ù„ØªØºÙŠØ± Ø¨Ø§Ù„Ù†Ù‚Ø§Ø·
                max_change_pips = (max_price - current_price) / pip_value
                min_change_pips = (current_price - min_price) / pip_value
                
                if max_change_pips >= min_pips:
                    target.append(2)  # ØµØ¹ÙˆØ¯ Ù‚ÙˆÙŠ
                elif min_change_pips >= min_pips:
                    target.append(0)  # Ù‡Ø¨ÙˆØ· Ù‚ÙˆÙŠ
                else:
                    target.append(1)  # Ù…Ø­Ø§ÙŠØ¯
            
            # Ù…Ù„Ø¡ Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ø£Ø®ÙŠØ±Ø©
            target.extend([1] * minutes)
            targets[target_name] = target
        
        return targets
    
    def create_advanced_features(self, df, symbol):
        """Ø¥Ù†Ø´Ø§Ø¡ Ù…ÙŠØ²Ø§Øª Ù…ØªÙ‚Ø¯Ù…Ø© Ø´Ø§Ù…Ù„Ø©"""
        # Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© Ù…Ù† FeatureEngineer
        features = self.feature_engineer.create_features(df)
        
        # Ø¥Ø¶Ø§ÙØ© Ù…ÙŠØ²Ø§Øª Ø§Ù„ÙˆÙ‚Øª
        features['hour'] = df['time'].dt.hour
        features['day_of_week'] = df['time'].dt.dayofweek
        features['day_of_month'] = df['time'].dt.day
        features['is_london_session'] = ((features['hour'] >= 8) & (features['hour'] <= 16)).astype(int)
        features['is_ny_session'] = ((features['hour'] >= 13) & (features['hour'] <= 22)).astype(int)
        features['is_asian_session'] = ((features['hour'] >= 0) & (features['hour'] <= 8)).astype(int)
        
        # Ù…ÙŠØ²Ø§Øª Ø§Ù„ØªÙ‚Ù„Ø¨ Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø©
        for period in [5, 10, 20, 50]:
            features[f'volatility_{period}'] = df['close'].pct_change().rolling(period).std()
            features[f'range_{period}'] = (df['high'] - df['low']).rolling(period).mean()
            features[f'true_range_{period}'] = self._calculate_true_range(df).rolling(period).mean()
        
        # Ù…ÙŠØ²Ø§Øª Ø§Ù„Ø­Ø¬Ù… Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø©
        features['volume_sma_ratio'] = df['volume'] / df['volume'].rolling(20).mean()
        features['volume_trend'] = df['volume'].rolling(10).mean() - df['volume'].rolling(30).mean()
        
        # Ù…ÙŠØ²Ø§Øª Ø§Ù„Ø³Ø¹Ø± Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø©
        features['price_position'] = (df['close'] - df['low']) / (df['high'] - df['low'] + 0.0001)
        features['body_size'] = abs(df['close'] - df['open']) / (df['high'] - df['low'] + 0.0001)
        features['upper_shadow'] = (df['high'] - df[['close', 'open']].max(axis=1)) / (df['high'] - df['low'] + 0.0001)
        features['lower_shadow'] = (df[['close', 'open']].min(axis=1) - df['low']) / (df['high'] - df['low'] + 0.0001)
        
        # Ù…ÙŠØ²Ø§Øª Ø§Ù„Ø§ØªØ¬Ø§Ù‡
        for period in [10, 20, 50, 100]:
            # Ù…ÙŠÙ„ Ø§Ù„Ù…ØªÙˆØ³Ø· Ø§Ù„Ù…ØªØ­Ø±Ùƒ
            sma = df['close'].rolling(period).mean()
            features[f'sma_{period}_slope'] = (sma - sma.shift(5)) / 5
            
            # Ø§Ù„Ù…Ø³Ø§ÙØ© Ù…Ù† Ø§Ù„Ù…ØªÙˆØ³Ø·
            features[f'distance_from_sma_{period}'] = (df['close'] - sma) / sma
        
        # Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ø´Ù…ÙˆØ¹
        features['doji'] = (features['body_size'] < 0.1).astype(int)
        features['hammer'] = ((features['lower_shadow'] > 2 * features['body_size']) & 
                             (features['upper_shadow'] < features['body_size'])).astype(int)
        features['shooting_star'] = ((features['upper_shadow'] > 2 * features['body_size']) & 
                                    (features['lower_shadow'] < features['body_size'])).astype(int)
        
        # Ø¯Ù…Ø¬ Ù…ÙŠØ²Ø§Øª S/R Ù…Ù† DataFrame Ø§Ù„Ø£ØµÙ„ÙŠ
        sr_columns = ['nearest_support', 'nearest_resistance', 'distance_to_support', 
                     'distance_to_resistance', 'sr_strength']
        for col in sr_columns:
            if col in df.columns:
                features[col] = df[col]
        
        return features
    
    def _calculate_true_range(self, df):
        """Ø­Ø³Ø§Ø¨ True Range"""
        high_low = df['high'] - df['low']
        high_close = abs(df['high'] - df['close'].shift(1))
        low_close = abs(df['low'] - df['close'].shift(1))
        
        return pd.concat([high_low, high_close, low_close], axis=1).max(axis=1)
    
    def simulate_trading_patterns(self, df, features, symbol):
        """Ù…Ø­Ø§ÙƒØ§Ø© Ø£Ù†Ù…Ø§Ø· ØªØ¯Ø§ÙˆÙ„ Ù…Ø®ØªÙ„ÙØ© Ù„ÙƒÙ„ ÙŠÙˆÙ…"""
        patterns = []
        
        # Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø¹Ù…Ù„Ø©
        symbol_info = self.sltp_system.get_symbol_info(symbol)
        
        # Ù„ÙƒÙ„ ØµÙ ÙÙŠ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        for idx in range(len(df)):
            row_patterns = []
            
            # Ù„ÙƒÙ„ Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© SL/TP
            for strategy in self.sltp_strategies:
                # Ø­Ø³Ø§Ø¨ SL/TP Ø¯ÙŠÙ†Ø§Ù…ÙŠÙƒÙŠ
                sl_tp = self.sltp_system.calculate_sl_tp(
                    symbol=symbol,
                    entry_price=df.iloc[idx]['close'],
                    position_type='BUY',  # Ø³Ù†Ø­Ø§ÙƒÙŠ Ø§Ù„Ø´Ø±Ø§Ø¡ ÙˆØ§Ù„Ø¨ÙŠØ¹
                    lot_size=0.1,
                    df=df.iloc[:idx+1]  # Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø­ØªÙ‰ Ù‡Ø°Ù‡ Ø§Ù„Ù†Ù‚Ø·Ø©
                )
                
                # Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ù†Ù…Ø·
                pattern = {
                    'strategy': strategy['name'],
                    'sl': sl_tp['sl'],
                    'tp': sl_tp['tp'],
                    'risk_reward': sl_tp.get('risk_reward_ratio', strategy['risk_reward']),
                    'confidence': self._calculate_pattern_confidence(features.iloc[idx])
                }
                row_patterns.append(pattern)
            
            patterns.append(row_patterns)
        
        return patterns
    
    def _calculate_pattern_confidence(self, features):
        """Ø­Ø³Ø§Ø¨ Ø«Ù‚Ø© Ø§Ù„Ù†Ù…Ø· Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø¤Ø´Ø±Ø§Øª"""
        confidence = 0.5  # Ù‚ÙŠÙ…Ø© Ø£Ø³Ø§Ø³ÙŠØ©
        
        # RSI
        if 'rsi_14' in features:
            if features['rsi_14'] < 30:
                confidence += 0.1  # oversold
            elif features['rsi_14'] > 70:
                confidence += 0.1  # overbought
        
        # Bollinger Bands
        if 'bb_position_20' in features:
            if features['bb_position_20'] < 0.2 or features['bb_position_20'] > 0.8:
                confidence += 0.1
        
        # Volume
        if 'volume_sma_ratio' in features and features['volume_sma_ratio'] > 1.5:
            confidence += 0.1
        
        # ADX
        if 'adx_14' in features and features['adx_14'] > 25:
            confidence += 0.1
        
        return min(confidence, 0.95)
    
    def train_ensemble_model(self, X_train, y_train, X_val, y_val, target_name):
        """ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬ Ù…Ø¬Ù…Ø¹ Ù…ØªÙ‚Ø¯Ù…"""
        logger.info(f"ğŸ¯ ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬ Ù…Ø¬Ù…Ø¹ Ù„Ù€ {target_name}")
        
        # ØªØ­Ø³ÙŠÙ† Ù…Ø¹Ø§Ù…Ù„Ø§Øª LightGBM Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Optuna
        best_lgb_params = self.optimize_lightgbm(X_train, y_train, X_val, y_val)
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬
        models = []
        
        # LightGBM Ù…Ø¹ Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø§Ù„Ù…Ø­Ø³Ù†Ø©
        lgb_model = lgb.LGBMClassifier(**best_lgb_params, n_estimators=500)
        models.append(('lightgbm', lgb_model))
        
        # XGBoost
        xgb_model = xgb.XGBClassifier(**self.model_configs['xgboost'], n_estimators=500)
        models.append(('xgboost', xgb_model))
        
        # CatBoost
        cat_model = CatBoostClassifier(**self.model_configs['catboost'], iterations=500)
        models.append(('catboost', cat_model))
        
        # Ù†Ù…ÙˆØ°Ø¬ Ù…Ø¬Ù…Ø¹
        ensemble = VotingClassifier(models, voting='soft', n_jobs=-1)
        ensemble.fit(X_train, y_train)
        
        # ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ø£Ø¯Ø§Ø¡
        train_score = ensemble.score(X_train, y_train)
        val_score = ensemble.score(X_val, y_val)
        
        logger.info(f"âœ… Ø¯Ù‚Ø© Ø§Ù„ØªØ¯Ø±ÙŠØ¨: {train_score:.4f}")
        logger.info(f"âœ… Ø¯Ù‚Ø© Ø§Ù„ØªØ­Ù‚Ù‚: {val_score:.4f}")
        
        return ensemble, {'train_score': train_score, 'val_score': val_score}
    
    def optimize_lightgbm(self, X_train, y_train, X_val, y_val, n_trials=50):
        """ØªØ­Ø³ÙŠÙ† Ù…Ø¹Ø§Ù…Ù„Ø§Øª LightGBM Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Optuna"""
        def objective(trial):
            params = {
                'objective': 'multiclass',
                'num_class': 3,
                'metric': 'multi_logloss',
                'boosting_type': 'gbdt',
                'num_leaves': trial.suggest_int('num_leaves', 20, 300),
                'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),
                'feature_fraction': trial.suggest_float('feature_fraction', 0.5, 1.0),
                'bagging_fraction': trial.suggest_float('bagging_fraction', 0.5, 1.0),
                'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),
                'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),
                'verbosity': -1
            }
            
            model = lgb.LGBMClassifier(**params, n_estimators=100)
            model.fit(X_train, y_train, eval_set=[(X_val, y_val)], 
                     eval_metric='multi_logloss', callbacks=[lgb.early_stopping(50)])
            
            return model.score(X_val, y_val)
        
        study = optuna.create_study(direction='maximize', study_name='lightgbm_optimization')
        study.optimize(objective, n_trials=n_trials, show_progress_bar=True)
        
        best_params = study.best_params
        best_params.update(self.model_configs['lightgbm'])
        
        return best_params
    
    def train_symbol(self, symbol, timeframe):
        """ØªØ¯Ø±ÙŠØ¨ Ù†Ù…Ø§Ø°Ø¬ Ù„Ø¹Ù…Ù„Ø© ÙˆØ§Ø­Ø¯Ø©"""
        logger.info(f"\n{'='*80}")
        logger.info(f"ğŸš€ Ø¨Ø¯Ø¡ ØªØ¯Ø±ÙŠØ¨ {symbol} {timeframe}")
        logger.info(f"{'='*80}")
        
        # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ø¹ S/R
        df = self.load_data_with_sr_levels(symbol, timeframe)
        if df is None or len(df) < self.min_data_points:
            logger.warning(f"âš ï¸ Ø¨ÙŠØ§Ù†Ø§Øª ØºÙŠØ± ÙƒØ§ÙÙŠØ© Ù„Ù€ {symbol} {timeframe}")
            return None
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø©
        features = self.create_advanced_features(df, symbol)
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ø£Ù‡Ø¯Ø§Ù Ù…ØªØ¹Ø¯Ø¯Ø©
        targets = self.create_multiple_targets(df, symbol)
        
        # Ù…Ø­Ø§ÙƒØ§Ø© Ø£Ù†Ù…Ø§Ø· Ø§Ù„ØªØ¯Ø§ÙˆÙ„
        patterns = self.simulate_trading_patterns(df, features, symbol)
        
        # Ø§Ù„ØªØ¹Ù„Ù… Ù…Ù† Ø§Ù„Ø£Ù†Ù…Ø§Ø· Ø§Ù„ØªØ§Ø±ÙŠØ®ÙŠØ©
        self.advanced_learner.analyze_historical_opportunities(
            df, features, targets, patterns, symbol, timeframe
        )
        
        # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        features = features.fillna(0)
        features = features.replace([np.inf, -np.inf], 0)
        
        # Ø§Ù„Ù†ØªØ§Ø¦Ø¬
        results = {
            'symbol': symbol,
            'timeframe': timeframe,
            'total_samples': len(features),
            'models': {},
            'patterns_learned': len(patterns),
            'timestamp': datetime.now()
        }
        
        # ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬ Ù„ÙƒÙ„ Ù‡Ø¯Ù
        for target_name, target_values in targets.items():
            logger.info(f"\nğŸ“Š ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬ {target_name}")
            
            # ØªØ­Ø¶ÙŠØ± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
            X = features.values
            y = np.array(target_values)
            
            # ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
            X_temp, X_test, y_temp, y_test = train_test_split(
                X, y, test_size=self.test_size, shuffle=False
            )
            X_train, X_val, y_train, y_val = train_test_split(
                X_temp, y_temp, test_size=self.validation_split, shuffle=False
            )
            
            # Ù…Ø¹Ø§ÙŠØ±Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
            scaler = RobustScaler()
            X_train_scaled = scaler.fit_transform(X_train)
            X_val_scaled = scaler.transform(X_val)
            X_test_scaled = scaler.transform(X_test)
            
            # ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¬Ù…Ø¹
            model, scores = self.train_ensemble_model(
                X_train_scaled, y_train, X_val_scaled, y_val, target_name
            )
            
            # ØªÙ‚ÙŠÙŠÙ… Ø¹Ù„Ù‰ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±
            y_pred = model.predict(X_test_scaled)
            test_accuracy = accuracy_score(y_test, y_pred)
            
            # Ø­Ø³Ø§Ø¨ Ù…Ù‚Ø§ÙŠÙŠØ³ Ù…ØªÙ‚Ø¯Ù…Ø©
            precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)
            recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)
            f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)
            
            logger.info(f"ğŸ“ˆ Ø¯Ù‚Ø© Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±: {test_accuracy:.4f}")
            logger.info(f"ğŸ“Š Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}")
            
            # Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
            model_dir = Path(f"models/{symbol}_{timeframe}/{target_name}")
            model_dir.mkdir(parents=True, exist_ok=True)
            
            model_data = {
                'model': model,
                'scaler': scaler,
                'feature_names': list(features.columns),
                'scores': {
                    'train': scores['train_score'],
                    'validation': scores['val_score'],
                    'test': test_accuracy,
                    'precision': precision,
                    'recall': recall,
                    'f1': f1
                },
                'target_config': next(c for c in self.target_configs if c['name'] == target_name),
                'timestamp': datetime.now()
            }
            
            joblib.dump(model_data, model_dir / 'model.pkl')
            
            results['models'][target_name] = model_data['scores']
            
            # ØªØ­Ø¯ÙŠØ« Ù†Ø¸Ø§Ù… Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ù…Ø³ØªÙ…Ø±
            self.continuous_learner.learn_from_predictions(
                y_test, y_pred, features.iloc[-len(y_test):], symbol, timeframe
            )
        
        # Ø­ÙØ¸ ØªÙ‚Ø±ÙŠØ± Ø´Ø§Ù…Ù„
        self._save_training_report(results, symbol, timeframe)
        
        return results
    
    def _save_training_report(self, results, symbol, timeframe):
        """Ø­ÙØ¸ ØªÙ‚Ø±ÙŠØ± Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ø´Ø§Ù…Ù„"""
        report_dir = Path(f"models/{symbol}_{timeframe}")
        report_dir.mkdir(parents=True, exist_ok=True)
        
        report = {
            'training_summary': results,
            'feature_importance': self._get_feature_importance(symbol, timeframe),
            'pattern_analysis': self.advanced_learner.get_pattern_summary(symbol, timeframe),
            'recommendations': self.continuous_learner.get_improvement_suggestions()
        }
        
        with open(report_dir / 'training_report.json', 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, default=str, ensure_ascii=False)
        
        logger.info(f"ğŸ“„ ØªÙ… Ø­ÙØ¸ ØªÙ‚Ø±ÙŠØ± Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙÙŠ {report_dir}")
    
    def _get_feature_importance(self, symbol, timeframe):
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø£Ù‡Ù…ÙŠØ© Ø§Ù„Ù…ÙŠØ²Ø§Øª Ù…Ù† Ø§Ù„Ù†Ù…Ø§Ø°Ø¬"""
        importance_dict = {}
        
        for target_name in self.target_configs:
            model_path = Path(f"models/{symbol}_{timeframe}/{target_name['name']}/model.pkl")
            if model_path.exists():
                model_data = joblib.load(model_path)
                model = model_data['model']
                
                # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø£Ù‡Ù…ÙŠØ© Ø§Ù„Ù…ÙŠØ²Ø§Øª Ù…Ù† LightGBM
                if hasattr(model, 'estimators_'):
                    lgb_model = next((est for name, est in model.estimators_ if name == 'lightgbm'), None)
                    if lgb_model and hasattr(lgb_model, 'feature_importances_'):
                        importance = lgb_model.feature_importances_
                        feature_names = model_data['feature_names']
                        
                        importance_dict[target_name['name']] = dict(
                            zip(feature_names, importance.tolist())
                        )
        
        return importance_dict
    
    def train_all_symbols(self):
        """ØªØ¯Ø±ÙŠØ¨ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø¹Ù…Ù„Ø§Øª Ø§Ù„Ù…ØªØ§Ø­Ø©"""
        logger.info("\n" + "="*100)
        logger.info("ğŸš€ Ø¨Ø¯Ø¡ ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ø´Ø§Ù…Ù„Ø© Ù„Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø¹Ù…Ù„Ø§Øª")
        logger.info("="*100)
        
        # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø¹Ù…Ù„Ø§Øª
        available_data = self.get_all_symbols_from_db()
        
        if available_data.empty:
            logger.error("âŒ Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¨ÙŠØ§Ù†Ø§Øª Ù…ØªØ§Ø­Ø© Ù„Ù„ØªØ¯Ø±ÙŠØ¨")
            return
        
        successful_trainings = []
        failed_trainings = []
        
        # ØªØ¯Ø±ÙŠØ¨ ÙƒÙ„ Ø¹Ù…Ù„Ø©
        for idx, row in available_data.iterrows():
            symbol = row['symbol']
            timeframe = row['timeframe']
            
            try:
                logger.info(f"\nğŸ“Š Ù…Ø¹Ø§Ù„Ø¬Ø© {idx+1}/{len(available_data)}: {symbol} {timeframe}")
                
                results = self.train_symbol(symbol, timeframe)
                
                if results:
                    successful_trainings.append({
                        'symbol': symbol,
                        'timeframe': timeframe,
                        'models': len(results['models']),
                        'best_accuracy': max(m['test'] for m in results['models'].values())
                    })
                    logger.info(f"âœ… Ù†Ø¬Ø­ ØªØ¯Ø±ÙŠØ¨ {symbol} {timeframe}")
                else:
                    failed_trainings.append({'symbol': symbol, 'timeframe': timeframe})
                    
            except Exception as e:
                logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªØ¯Ø±ÙŠØ¨ {symbol} {timeframe}: {e}")
                failed_trainings.append({'symbol': symbol, 'timeframe': timeframe, 'error': str(e)})
        
        # Ø·Ø¨Ø§Ø¹Ø© Ø§Ù„Ù…Ù„Ø®Øµ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ
        self._print_final_summary(successful_trainings, failed_trainings)
        
        # Ø­ÙØ¸ Ù…Ù„Ø®Øµ Ø´Ø§Ù…Ù„
        self._save_overall_summary(successful_trainings, failed_trainings)
    
    def _print_final_summary(self, successful, failed):
        """Ø·Ø¨Ø§Ø¹Ø© Ø§Ù„Ù…Ù„Ø®Øµ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ"""
        logger.info("\n" + "="*100)
        logger.info("ğŸ“Š Ø§Ù„Ù…Ù„Ø®Øµ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ Ù„Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ø´Ø§Ù…Ù„")
        logger.info("="*100)
        
        logger.info(f"\nâœ… Ù†Ø¬Ø­ Ø§Ù„ØªØ¯Ø±ÙŠØ¨: {len(successful)} Ø¹Ù…Ù„Ø©/ÙØ±ÙŠÙ…")
        logger.info(f"âŒ ÙØ´Ù„ Ø§Ù„ØªØ¯Ø±ÙŠØ¨: {len(failed)} Ø¹Ù…Ù„Ø©/ÙØ±ÙŠÙ…")
        
        if successful:
            logger.info("\nğŸ† Ø£ÙØ¶Ù„ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬:")
            sorted_models = sorted(successful, key=lambda x: x['best_accuracy'], reverse=True)[:10]
            for model in sorted_models:
                logger.info(f"  â€¢ {model['symbol']} {model['timeframe']}: "
                          f"Ø¯Ù‚Ø© {model['best_accuracy']:.4f} ({model['models']} Ù†Ù…Ø§Ø°Ø¬)")
        
        if failed:
            logger.info("\nâš ï¸ Ø§Ù„Ø¹Ù…Ù„Ø§Øª Ø§Ù„ØªÙŠ ÙØ´Ù„Øª:")
            for fail in failed[:10]:
                error_msg = fail.get('error', 'Ø¨ÙŠØ§Ù†Ø§Øª ØºÙŠØ± ÙƒØ§ÙÙŠØ©')
                logger.info(f"  â€¢ {fail['symbol']} {fail['timeframe']}: {error_msg}")
    
    def _save_overall_summary(self, successful, failed):
        """Ø­ÙØ¸ Ù…Ù„Ø®Øµ Ø´Ø§Ù…Ù„ Ù„Ù„ØªØ¯Ø±ÙŠØ¨"""
        summary = {
            'training_date': datetime.now().isoformat(),
            'total_attempted': len(successful) + len(failed),
            'successful': len(successful),
            'failed': len(failed),
            'success_rate': len(successful) / (len(successful) + len(failed)) if successful or failed else 0,
            'successful_models': successful,
            'failed_models': failed,
            'configuration': {
                'min_data_points': self.min_data_points,
                'target_configs': self.target_configs,
                'sltp_strategies': self.sltp_strategies,
                'features_used': 'Advanced with S/R, ATR, Time, Patterns'
            }
        }
        
        summary_path = Path("models/training_summary_ultimate.json")
        with open(summary_path, 'w', encoding='utf-8') as f:
            json.dump(summary, f, indent=2, ensure_ascii=False)
        
        logger.info(f"\nğŸ’¾ ØªÙ… Ø­ÙØ¸ Ø§Ù„Ù…Ù„Ø®Øµ Ø§Ù„Ø´Ø§Ù…Ù„ ÙÙŠ: {summary_path}")

def main():
    """Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©"""
    trainer = UltimateModelTrainer()
    trainer.train_all_symbols()

if __name__ == "__main__":
    main()