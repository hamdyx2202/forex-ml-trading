#!/usr/bin/env python3
"""
Advanced Complete Training System - Ù†Ø¸Ø§Ù… Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù…ØªÙ‚Ø¯Ù… Ø§Ù„ÙƒØ§Ù…Ù„
Ù‡Ø¯Ù: ØªØ­Ù‚ÙŠÙ‚ Ù†Ø³Ø¨Ø© Ù†Ø¬Ø§Ø­ 95%+ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø¬Ù…ÙŠØ¹ Ø§Ù„ØªÙ‚Ù†ÙŠØ§Øª Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø©
"""

import os
import sys
import json
import sqlite3
import numpy as np
import pandas as pd
from datetime import datetime, timedelta
from pathlib import Path
import joblib
import warnings
warnings.filterwarnings('ignore')

from loguru import logger
from sklearn.model_selection import train_test_split, TimeSeriesSplit, GridSearchCV
from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report
from sklearn.ensemble import VotingClassifier, RandomForestClassifier, ExtraTreesClassifier
from sklearn.neural_network import MLPClassifier
import lightgbm as lgb
import xgboost as xgb
from catboost import CatBoostClassifier
import optuna
from ta import add_all_ta_features
from ta.utils import dropna
import talib

# Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ³Ø¬ÙŠÙ„
logger.remove()
logger.add(sys.stdout, format="<green>{time:YYYY-MM-DD HH:mm:ss}</green> | <level>{level: <8}</level> | <level>{message}</level>")
logger.add("logs/advanced_training.log", rotation="1 day", retention="30 days")

class AdvancedCompleteTrainer:
    """Ù†Ø¸Ø§Ù… Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù…ØªÙ‚Ø¯Ù… Ø§Ù„ÙƒØ§Ù…Ù„ Ù„Ù„ÙˆØµÙˆÙ„ Ù„Ù†Ø³Ø¨Ø© Ù†Ø¬Ø§Ø­ 95%+"""
    
    def __init__(self):
        self.min_data_points = 10000  # Ø¨ÙŠØ§Ù†Ø§Øª Ø£ÙƒØ«Ø± Ù„Ù„Ø¯Ù‚Ø© Ø§Ù„Ø¹Ø§Ù„ÙŠØ©
        self.test_size = 0.2
        self.validation_split = 0.15
        self.random_state = 42
        
        # Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ§Øª Ù…ØªØ¹Ø¯Ø¯Ø© Ù„Ù„ØªØ¯Ø±ÙŠØ¨
        self.training_strategies = {
            'ultra_short': {'lookahead': 5, 'min_pips': 3, 'confidence_threshold': 0.95},
            'scalping': {'lookahead': 15, 'min_pips': 5, 'confidence_threshold': 0.92},
            'short_term': {'lookahead': 30, 'min_pips': 10, 'confidence_threshold': 0.90},
            'medium_term': {'lookahead': 60, 'min_pips': 20, 'confidence_threshold': 0.88},
            'long_term': {'lookahead': 240, 'min_pips': 40, 'confidence_threshold': 0.85}
        }
        
        # Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Stop Loss Ùˆ Take Profit Ø§Ù„Ø¯ÙŠÙ†Ø§Ù…ÙŠÙƒÙŠØ©
        self.sl_tp_settings = {
            'ultra_short': {
                'stop_loss_atr': 0.5,      # Ù†ØµÙ ATR Ù„Ù„Ø³ÙƒØ§Ù„Ø¨ÙŠÙ†Ø¬ Ø§Ù„Ø³Ø±ÙŠØ¹
                'take_profit_ratios': [1.0, 1.5, 2.0],  # TP Ù…ØªØ¹Ø¯Ø¯
                'trailing_stop_atr': 0.3,
                'breakeven_pips': 5
            },
            'scalping': {
                'stop_loss_atr': 1.0,
                'take_profit_ratios': [1.0, 2.0, 3.0],
                'trailing_stop_atr': 0.5,
                'breakeven_pips': 10
            },
            'short_term': {
                'stop_loss_atr': 1.5,
                'take_profit_ratios': [1.5, 2.5, 3.5],
                'trailing_stop_atr': 0.7,
                'breakeven_pips': 15
            },
            'medium_term': {
                'stop_loss_atr': 2.0,
                'take_profit_ratios': [2.0, 3.0, 4.0],
                'trailing_stop_atr': 1.0,
                'breakeven_pips': 20
            },
            'long_term': {
                'stop_loss_atr': 2.5,
                'take_profit_ratios': [2.5, 4.0, 6.0],
                'trailing_stop_atr': 1.5,
                'breakeven_pips': 30
            }
        }

        
        # Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø©
        self.ensemble_models = {
            'lightgbm_primary': {
                'objective': 'multiclass',
                'num_class': 3,
                'metric': 'multi_logloss',
                'boosting_type': 'dart',  # DART Ù„Ù„Ø¯Ù‚Ø© Ø§Ù„Ø¹Ø§Ù„ÙŠØ©
                'num_leaves': 127,
                'max_depth': -1,
                'learning_rate': 0.01,
                'n_estimators': 2000,
                'feature_fraction': 0.9,
                'bagging_fraction': 0.9,
                'bagging_freq': 5,
                'lambda_l1': 0.1,
                'lambda_l2': 0.1,
                'min_child_samples': 20,
                'subsample_for_bin': 200000,
                'class_weight': 'balanced',
                'verbosity': -1
            },
            'xgboost_primary': {
                'objective': 'multi:softprob',
                'num_class': 3,
                'max_depth': 10,
                'learning_rate': 0.01,
                'n_estimators': 2000,
                'subsample': 0.9,
                'colsample_bytree': 0.9,
                'gamma': 0.1,
                'reg_alpha': 0.1,
                'reg_lambda': 1,
                'scale_pos_weight': 1,
                'eval_metric': 'mlogloss',
                'use_label_encoder': False,
                'tree_method': 'hist'
            },
            'catboost_primary': {
                'loss_function': 'MultiClass',
                'classes_count': 3,
                'iterations': 2000,
                'depth': 10,
                'learning_rate': 0.01,
                'l2_leaf_reg': 5,
                'border_count': 254,
                'random_strength': 0.1,
                'bagging_temperature': 0.1,
                'od_type': 'Iter',
                'od_wait': 50,
                'class_weights': [1, 1, 1],
                'verbose': False
            },
            'extra_trees': {
                'n_estimators': 500,
                'max_depth': 20,
                'min_samples_split': 5,
                'min_samples_leaf': 2,
                'max_features': 'sqrt',
                'bootstrap': True,
                'class_weight': 'balanced',
                'n_jobs': -1
            },
            'neural_network': {
                'hidden_layer_sizes': (200, 150, 100, 50),
                'activation': 'relu',
                'solver': 'adam',
                'alpha': 0.001,
                'learning_rate': 'adaptive',
                'learning_rate_init': 0.001,
                'max_iter': 1000,
                'early_stopping': True,
                'validation_fraction': 0.1,
                'n_iter_no_change': 20
            }
        }
        
    def check_database(self):
        """Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙˆØ§Ù„Ø¨ÙŠØ§Ù†Ø§Øª"""
        db_path = Path("data/forex_ml.db")
        
        if not db_path.exists():
            logger.error("âŒ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯Ø©!")
            logger.info("ğŸ”§ ÙŠØ±Ø¬Ù‰ ØªØ´ØºÙŠÙ„: python setup_database.py")
            return False
        
        try:
            conn = sqlite3.connect(db_path)
            cursor = conn.cursor()
            cursor.execute("SELECT COUNT(*) FROM price_data")
            count = cursor.fetchone()[0]
            conn.close()
            
            if count == 0:
                logger.error("âŒ Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¨ÙŠØ§Ù†Ø§Øª ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª!")
                logger.info("ğŸ“Š ÙŠØ±Ø¬Ù‰ Ø¬Ù…Ø¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù† MT5 Ø£ÙˆÙ„Ø§Ù‹")
                return False
            
            logger.info(f"âœ… Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¬Ø§Ù‡Ø²Ø©: {count:,} Ø³Ø¬Ù„")
            return True
            
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: {e}")
            return False
    
    def load_data_advanced(self, symbol, timeframe, limit=100000):
        """ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ø¹ Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…ØªÙ‚Ø¯Ù…Ø©"""
        try:
            conn = sqlite3.connect("data/forex_ml.db")
            query = """
                SELECT * FROM price_data 
                WHERE symbol = ? AND timeframe = ?
                ORDER BY time ASC
                LIMIT ?
            """
            df = pd.read_sql_query(query, conn, params=(symbol, timeframe, limit))
            conn.close()
            
            if len(df) < self.min_data_points:
                logger.warning(f"âš ï¸ Ø¨ÙŠØ§Ù†Ø§Øª ØºÙŠØ± ÙƒØ§ÙÙŠØ© Ù„Ù€ {symbol} {timeframe}: {len(df)} Ø³Ø¬Ù„")
                return None
            
            # ØªØ­ÙˆÙŠÙ„ Ø§Ù„ÙˆÙ‚Øª
            df['time'] = pd.to_datetime(df['time'], unit='s')
            df = df.set_index('time')
            
            # Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø¹Ø¯Ù… ÙˆØ¬ÙˆØ¯ Ù‚ÙŠÙ… Ù…ÙÙ‚ÙˆØ¯Ø©
            df = df.fillna(method='ffill').fillna(method='bfill')
            
            logger.info(f"âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ {len(df)} Ø³Ø¬Ù„ Ù„Ù€ {symbol} {timeframe}")
            return df
            
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: {e}")
            return None
    
    def create_ultra_advanced_features(self, df, symbol):
        """Ø¥Ù†Ø´Ø§Ø¡ Ù…ÙŠØ²Ø§Øª Ù…ØªÙ‚Ø¯Ù…Ø© Ø¬Ø¯Ø§Ù‹ - 200+ Ù…ÙŠØ²Ø©"""
        logger.info("ğŸ”¬ Ø¥Ù†Ø´Ø§Ø¡ Ù…ÙŠØ²Ø§Øª Ù…ØªÙ‚Ø¯Ù…Ø©...")
        
        features = pd.DataFrame(index=df.index)
        
        # 1. Ù…ÙŠØ²Ø§Øª Ø§Ù„Ø£Ø³Ø¹Ø§Ø± Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
        features['returns'] = df['close'].pct_change()
        features['log_returns'] = np.log(df['close'] / df['close'].shift(1))
        features['price_range'] = (df['high'] - df['low']) / df['close']
        features['body_size'] = abs(df['close'] - df['open']) / df['close']
        features['upper_shadow'] = (df['high'] - df[['close', 'open']].max(axis=1)) / df['close']
        features['lower_shadow'] = (df[['close', 'open']].min(axis=1) - df['low']) / df['close']
        
        # 2. Ø§Ù„Ù…ØªÙˆØ³Ø·Ø§Øª Ø§Ù„Ù…ØªØ­Ø±ÙƒØ© Ø§Ù„Ù…ØªØ¹Ø¯Ø¯Ø©
        ma_periods = [5, 8, 10, 13, 20, 21, 34, 50, 55, 89, 100, 144, 200, 233]
        for period in ma_periods:
            sma = df['close'].rolling(period).mean()
            ema = df['close'].ewm(span=period, adjust=False).mean()
            
            features[f'sma_{period}'] = (df['close'] - sma) / sma
            features[f'ema_{period}'] = (df['close'] - ema) / ema
            features[f'sma_{period}_slope'] = sma.pct_change(5)
            features[f'ema_{period}_slope'] = ema.pct_change(5)
        
        # 3. Ù…Ø¤Ø´Ø±Ø§Øª TA-Lib Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø©
        try:
            # RSI Ù…ØªØ¹Ø¯Ø¯
            for period in [5, 7, 9, 14, 21, 28]:
                features[f'rsi_{period}'] = talib.RSI(df['close'].values, timeperiod=period)
            
            # Stochastic
            for period in [5, 9, 14]:
                slowk, slowd = talib.STOCH(df['high'].values, df['low'].values, df['close'].values,
                                          fastk_period=period, slowk_period=3, slowd_period=3)
                features[f'stoch_k_{period}'] = slowk
                features[f'stoch_d_{period}'] = slowd
            
            # MACD Ù…ØªØ¹Ø¯Ø¯
            for fast, slow, signal in [(12, 26, 9), (5, 35, 5), (3, 10, 16)]:
                macd, macdsignal, macdhist = talib.MACD(df['close'].values, 
                                                        fastperiod=fast, 
                                                        slowperiod=slow, 
                                                        signalperiod=signal)
                features[f'macd_{fast}_{slow}'] = macd
                features[f'macd_signal_{fast}_{slow}'] = macdsignal
                features[f'macd_hist_{fast}_{slow}'] = macdhist
            
            # Bollinger Bands Ù…ØªØ¹Ø¯Ø¯
            for period in [10, 20, 30]:
                for dev in [1.5, 2.0, 2.5]:
                    upper, middle, lower = talib.BBANDS(df['close'].values,
                                                       timeperiod=period,
                                                       nbdevup=dev,
                                                       nbdevdn=dev)
                    features[f'bb_upper_{period}_{dev}'] = (upper - df['close']) / df['close']
                    features[f'bb_lower_{period}_{dev}'] = (df['close'] - lower) / df['close']
                    features[f'bb_width_{period}_{dev}'] = (upper - lower) / middle
            
            # ATR ÙˆVolatility
            for period in [7, 10, 14, 20, 30]:
                features[f'atr_{period}'] = talib.ATR(df['high'].values, 
                                                     df['low'].values, 
                                                     df['close'].values, 
                                                     timeperiod=period) / df['close']
                features[f'natr_{period}'] = talib.NATR(df['high'].values, 
                                                       df['low'].values, 
                                                       df['close'].values, 
                                                       timeperiod=period)
            
            # ADX ÙˆDI
            for period in [7, 14, 21]:
                features[f'adx_{period}'] = talib.ADX(df['high'].values, 
                                                     df['low'].values, 
                                                     df['close'].values, 
                                                     timeperiod=period)
                features[f'plus_di_{period}'] = talib.PLUS_DI(df['high'].values, 
                                                              df['low'].values, 
                                                              df['close'].values, 
                                                              timeperiod=period)
                features[f'minus_di_{period}'] = talib.MINUS_DI(df['high'].values, 
                                                               df['low'].values, 
                                                               df['close'].values, 
                                                               timeperiod=period)
            
            # Ù…Ø¤Ø´Ø±Ø§Øª Ø£Ø®Ø±Ù‰
            features['cci_14'] = talib.CCI(df['high'].values, df['low'].values, 
                                          df['close'].values, timeperiod=14)
            features['mfi_14'] = talib.MFI(df['high'].values, df['low'].values, 
                                          df['close'].values, df['volume'].values, timeperiod=14)
            features['roc_10'] = talib.ROC(df['close'].values, timeperiod=10)
            features['williams_r'] = talib.WILLR(df['high'].values, df['low'].values, 
                                                df['close'].values, timeperiod=14)
            features['ultimate_osc'] = talib.ULTOSC(df['high'].values, df['low'].values, 
                                                   df['close'].values)
            
            # Pattern Recognition
            patterns = {
                'cdl_doji': talib.CDLDOJI,
                'cdl_hammer': talib.CDLHAMMER,
                'cdl_shooting_star': talib.CDLSHOOTINGSTAR,
                'cdl_engulfing': talib.CDLENGULFING,
                'cdl_morning_star': talib.CDLMORNINGSTAR,
                'cdl_evening_star': talib.CDLEVENINGSTAR,
                'cdl_3_black_crows': talib.CDL3BLACKCROWS,
                'cdl_3_white_soldiers': talib.CDL3WHITESOLDIERS
            }
            
            for name, func in patterns.items():
                features[name] = func(df['open'].values, df['high'].values, 
                                    df['low'].values, df['close'].values)
                
        except Exception as e:
            logger.warning(f"âš ï¸ Ø¨Ø¹Ø¶ Ù…Ø¤Ø´Ø±Ø§Øª TA-Lib ÙØ´Ù„Øª: {e}")
        
        # 4. Ù…ÙŠØ²Ø§Øª Ø§Ù„Ø­Ø¬Ù… Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø©
        features['volume_sma_ratio'] = df['volume'] / df['volume'].rolling(20).mean()
        features['volume_ema_ratio'] = df['volume'] / df['volume'].ewm(span=20).mean()
        features['obv'] = (np.sign(df['close'].diff()) * df['volume']).cumsum()
        features['obv_sma_ratio'] = features['obv'] / features['obv'].rolling(20).mean()
        
        # Force Index
        features['force_index'] = df['close'].diff() * df['volume']
        features['force_index_ema'] = features['force_index'].ewm(span=13).mean()
        
        # 5. Ù…ÙŠØ²Ø§Øª Ø§Ù„ØªÙ‚Ù„Ø¨ Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø©
        for period in [5, 10, 20, 30, 50]:
            returns = df['close'].pct_change()
            features[f'volatility_{period}'] = returns.rolling(period).std()
            features[f'volatility_ratio_{period}'] = features[f'volatility_{period}'] / returns.rolling(period * 2).std()
            
            # Parkinson volatility
            features[f'parkinson_vol_{period}'] = np.sqrt(
                np.log(df['high']/df['low']).pow(2).rolling(period).mean() / (4 * np.log(2))
            )
            
            # Garman-Klass volatility
            features[f'gk_vol_{period}'] = np.sqrt(
                0.5 * np.log(df['high']/df['low']).pow(2).rolling(period).mean() -
                (2*np.log(2)-1) * np.log(df['close']/df['open']).pow(2).rolling(period).mean()
            )
        
        # 6. Ù…ÙŠØ²Ø§Øª Ø§Ù„Ø³ÙˆÙ‚ ÙˆØ§Ù„ÙˆÙ‚Øª
        features['hour'] = df.index.hour
        features['day_of_week'] = df.index.dayofweek
        features['day_of_month'] = df.index.day
        features['month'] = df.index.month
        features['quarter'] = df.index.quarter
        
        # Ø¬Ù„Ø³Ø§Øª Ø§Ù„ØªØ¯Ø§ÙˆÙ„ Ø¨ØªÙˆÙ‚ÙŠØª Ø£ÙƒØ«Ø± Ø¯Ù‚Ø©
        features['asian_session'] = ((features['hour'] >= 0) & (features['hour'] < 9)).astype(int)
        features['london_session'] = ((features['hour'] >= 8) & (features['hour'] < 17)).astype(int)
        features['ny_session'] = ((features['hour'] >= 13) & (features['hour'] < 22)).astype(int)
        features['sydney_session'] = ((features['hour'] >= 22) | (features['hour'] < 7)).astype(int)
        
        # ØªØ¯Ø§Ø®Ù„ Ø§Ù„Ø¬Ù„Ø³Ø§Øª
        features['london_ny_overlap'] = ((features['hour'] >= 13) & (features['hour'] < 17)).astype(int)
        features['asian_london_overlap'] = ((features['hour'] >= 8) & (features['hour'] < 9)).astype(int)
        
        # 7. Ù…ÙŠØ²Ø§Øª Ø§Ù„Ø§Ø±ØªØ¨Ø§Ø· ÙˆØ§Ù„Ø¹Ù„Ø§Ù‚Ø§Øª
        # Ø¹Ù„Ø§Ù‚Ø§Øª Ø§Ù„Ø£Ø³Ø¹Ø§Ø±
        for period in [5, 10, 20]:
            features[f'high_low_ratio_{period}'] = df['high'].rolling(period).max() / df['low'].rolling(period).min()
            features[f'close_to_high_{period}'] = df['close'] / df['high'].rolling(period).max()
            features[f'close_to_low_{period}'] = df['close'] / df['low'].rolling(period).min()
        
        # 8. Ù…ÙŠØ²Ø§Øª Ø§Ù„Ø²Ø®Ù… Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø©
        for period in [5, 10, 20, 50]:
            features[f'momentum_{period}'] = df['close'] / df['close'].shift(period) - 1
            features[f'acceleration_{period}'] = features[f'momentum_{period}'].diff()
        
        # 9. Ù…Ø³ØªÙˆÙŠØ§Øª Ø§Ù„Ø¯Ø¹Ù… ÙˆØ§Ù„Ù…Ù‚Ø§ÙˆÙ…Ø© Ø§Ù„Ø¯ÙŠÙ†Ø§Ù…ÙŠÙƒÙŠØ©
        for period in [20, 50, 100]:
            rolling_high = df['high'].rolling(period).max()
            rolling_low = df['low'].rolling(period).min()
            
            features[f'distance_from_high_{period}'] = (rolling_high - df['close']) / df['close']
            features[f'distance_from_low_{period}'] = (df['close'] - rolling_low) / df['close']
            features[f'position_in_range_{period}'] = (df['close'] - rolling_low) / (rolling_high - rolling_low)
        
        # 10. Ù…ÙŠØ²Ø§Øª Ø¥Ø­ØµØ§Ø¦ÙŠØ© Ù…ØªÙ‚Ø¯Ù…Ø©
        for period in [10, 20, 50]:
            rolling_returns = df['close'].pct_change().rolling(period)
            features[f'skewness_{period}'] = rolling_returns.skew()
            features[f'kurtosis_{period}'] = rolling_returns.kurt()
            features[f'mean_return_{period}'] = rolling_returns.mean()
            features[f'median_return_{period}'] = rolling_returns.median()
        
        # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        features = features.fillna(method='ffill').fillna(0)
        features = features.replace([np.inf, -np.inf], 0)
        
        logger.info(f"âœ… ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ {len(features.columns)} Ù…ÙŠØ²Ø© Ù…ØªÙ‚Ø¯Ù…Ø©")
        
        return features
    
    
    def calculate_pip_value(self, symbol):
        """Ø­Ø³Ø§Ø¨ Ù‚ÙŠÙ…Ø© Ø§Ù„Ù†Ù‚Ø·Ø© Ø­Ø³Ø¨ Ø§Ù„Ø¹Ù…Ù„Ø©"""
        if 'JPY' in symbol or 'XAG' in symbol:
            return 0.01
        elif 'XAU' in symbol:
            return 0.1
        else:
            return 0.0001
    
    def calculate_dynamic_sl_tp(self, df, position_type, entry_idx, strategy_name):
        """Ø­Ø³Ø§Ø¨ Stop Loss Ùˆ Take Profit Ø¯ÙŠÙ†Ø§Ù…ÙŠÙƒÙŠ Ù…ØªÙ‚Ø¯Ù…"""
        
        sl_tp_config = self.sl_tp_settings.get(strategy_name, self.sl_tp_settings['medium_term'])
        
        # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ ATR
        atr_period = 14
        if f'atr_{atr_period}' in df.columns:
            current_atr = df.iloc[entry_idx][f'atr_{atr_period}']
        else:
            # Ø­Ø³Ø§Ø¨ ATR
            high_low = df['high'] - df['low']
            high_close = abs(df['high'] - df['close'].shift())
            low_close = abs(df['low'] - df['close'].shift())
            true_range = pd.concat([high_low, high_close, low_close], axis=1).max(axis=1)
            current_atr = true_range.rolling(atr_period).mean().iloc[entry_idx]
        
        current_price = df['close'].iloc[entry_idx]
        pip_value = self.calculate_pip_value(df.index.name if hasattr(df.index, 'name') else 'EURUSD')
        
        # Stop Loss Ø¯ÙŠÙ†Ø§Ù…ÙŠÙƒÙŠ
        sl_distance = current_atr * sl_tp_config['stop_loss_atr']
        
        # Take Profit Ù…ØªØ¹Ø¯Ø¯ Ø§Ù„Ù…Ø³ØªÙˆÙŠØ§Øª
        tp_distances = [sl_distance * ratio for ratio in sl_tp_config['take_profit_ratios']]
        
        # Support/Resistance
        support_resistance = self.find_support_resistance_levels(df, entry_idx)
        
        if position_type == 'long':
            # Stop Loss
            stop_loss = current_price - sl_distance
            
            # ØªØ¹Ø¯ÙŠÙ„ SL Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø¯Ø¹Ù…
            nearest_support = min([s for s in support_resistance['support'] 
                                 if s < current_price and s > stop_loss], 
                                default=stop_loss)
            if nearest_support > stop_loss:
                stop_loss = nearest_support - (2 * pip_value)
            
            # Take Profit levels
            take_profits = []
            for tp_distance in tp_distances:
                tp = current_price + tp_distance
                
                # ØªØ¹Ø¯ÙŠÙ„ TP Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ù‚Ø§ÙˆÙ…Ø©
                nearest_resistance = min([r for r in support_resistance['resistance'] 
                                        if r > current_price and r < tp], 
                                       default=tp)
                if nearest_resistance < tp:
                    tp = nearest_resistance - (2 * pip_value)
                
                take_profits.append(tp)
            
            # Trailing Stop
            trailing_stop_distance = current_atr * sl_tp_config['trailing_stop_atr']
            
        else:  # Short
            # Stop Loss
            stop_loss = current_price + sl_distance
            
            # ØªØ¹Ø¯ÙŠÙ„ SL Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ù‚Ø§ÙˆÙ…Ø©
            nearest_resistance = max([r for r in support_resistance['resistance'] 
                                    if r > current_price and r < stop_loss], 
                                   default=stop_loss)
            if nearest_resistance < stop_loss:
                stop_loss = nearest_resistance + (2 * pip_value)
            
            # Take Profit levels
            take_profits = []
            for tp_distance in tp_distances:
                tp = current_price - tp_distance
                
                # ØªØ¹Ø¯ÙŠÙ„ TP Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø¯Ø¹Ù…
                nearest_support = max([s for s in support_resistance['support'] 
                                     if s < current_price and s > tp], 
                                    default=tp)
                if nearest_support > tp:
                    tp = nearest_support + (2 * pip_value)
                
                take_profits.append(tp)
            
            # Trailing Stop
            trailing_stop_distance = current_atr * sl_tp_config['trailing_stop_atr']
        
        return {
            'stop_loss': stop_loss,
            'take_profits': take_profits,
            'trailing_stop_distance': trailing_stop_distance,
            'breakeven_pips': sl_tp_config['breakeven_pips'],
            'risk_amount': abs(current_price - stop_loss),
            'reward_ratios': sl_tp_config['take_profit_ratios'],
            'atr_used': current_atr
        }
    
    def find_support_resistance_levels(self, df, current_idx, lookback=150):
        """Ø¥ÙŠØ¬Ø§Ø¯ Ù…Ø³ØªÙˆÙŠØ§Øª Ø§Ù„Ø¯Ø¹Ù… ÙˆØ§Ù„Ù…Ù‚Ø§ÙˆÙ…Ø© Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø©"""
        start_idx = max(0, current_idx - lookback)
        price_data = df[['high', 'low', 'close']].iloc[start_idx:current_idx]
        
        # Ø¥ÙŠØ¬Ø§Ø¯ Ø§Ù„Ù‚Ù…Ù… ÙˆØ§Ù„Ù‚ÙŠØ¹Ø§Ù†
        highs = []
        lows = []
        
        # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ© Ù…ØªÙ‚Ø¯Ù…Ø© Ù„Ù„ÙƒØ´Ù Ø¹Ù† Ø§Ù„Ù‚Ù…Ù… ÙˆØ§Ù„Ù‚ÙŠØ¹Ø§Ù†
        window = 5
        for i in range(window, len(price_data) - window):
            # Ù‚Ù…Ø© Ù…Ø­Ù„ÙŠØ©
            if (price_data['high'].iloc[i] == price_data['high'].iloc[i-window:i+window+1].max()):
                highs.append(price_data['high'].iloc[i])
            
            # Ù‚Ø§Ø¹ Ù…Ø­Ù„ÙŠ
            if (price_data['low'].iloc[i] == price_data['low'].iloc[i-window:i+window+1].min()):
                lows.append(price_data['low'].iloc[i])
        
        # Ø¥Ø¶Ø§ÙØ© Ù…Ø³ØªÙˆÙŠØ§Øª Ù…Ù† Fibonacci
        if len(price_data) > 0:
            recent_high = price_data['high'].max()
            recent_low = price_data['low'].min()
            fib_levels = [0.236, 0.382, 0.5, 0.618, 0.786]
            
            for level in fib_levels:
                fib_price = recent_low + (recent_high - recent_low) * level
                if fib_price > price_data['close'].iloc[-1]:
                    highs.append(fib_price)
                else:
                    lows.append(fib_price)
        
        # ØªØ¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ø³ØªÙˆÙŠØ§Øª Ø§Ù„Ù‚Ø±ÙŠØ¨Ø©
        def cluster_levels(levels, threshold=0.002):
            if not levels:
                return []
            
            levels = sorted(levels)
            clusters = [[levels[0]]]
            
            for level in levels[1:]:
                if abs(level - clusters[-1][-1]) / clusters[-1][-1] < threshold:
                    clusters[-1].append(level)
                else:
                    clusters.append([level])
            
            # Ù…ØªÙˆØ³Ø· ÙƒÙ„ Ù…Ø¬Ù…ÙˆØ¹Ø© Ù…Ø¹ ÙˆØ²Ù† Ù„Ù„ØªÙƒØ±Ø§Ø±
            result = []
            for cluster in clusters:
                weight = len(cluster)  # ÙƒÙ„Ù…Ø§ Ø²Ø§Ø¯ Ø§Ù„ØªÙƒØ±Ø§Ø± Ø²Ø§Ø¯Øª Ø§Ù„Ù‚ÙˆØ©
                avg_level = sum(cluster) / len(cluster)
                result.append({'level': avg_level, 'strength': weight})
            
            # ØªØ±ØªÙŠØ¨ Ø­Ø³Ø¨ Ø§Ù„Ù‚ÙˆØ©
            result.sort(key=lambda x: x['strength'], reverse=True)
            
            # Ø¥Ø±Ø¬Ø§Ø¹ Ø£Ù‚ÙˆÙ‰ Ø§Ù„Ù…Ø³ØªÙˆÙŠØ§Øª ÙÙ‚Ø·
            return [item['level'] for item in result[:10]]
        
        resistance_levels = cluster_levels(highs)
        support_levels = cluster_levels(lows)
        
        return {
            'resistance': resistance_levels,
            'support': support_levels
        }
    
    def create_advanced_targets_with_sl_tp(self, df, strategy_name):
        """Ø¥Ù†Ø´Ø§Ø¡ Ø£Ù‡Ø¯Ø§Ù Ù…ØªÙ‚Ø¯Ù…Ø© Ù…Ø¹ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª SL/TP ÙƒØ§Ù…Ù„Ø©"""
        strategy = self.training_strategies[strategy_name]
        lookahead = strategy['lookahead']
        min_pips = strategy['min_pips']
        
        pip_value = self.calculate_pip_value(df.index.name if hasattr(df.index, 'name') else 'EURUSD')
        
        targets = []
        confidences = []
        sl_tp_info = []
        trade_quality = []
        
        for i in range(len(df) - lookahead):
            future_prices = df['close'].iloc[i+1:i+lookahead+1].values
            future_highs = df['high'].iloc[i+1:i+lookahead+1].values
            future_lows = df['low'].iloc[i+1:i+lookahead+1].values
            current_price = df['close'].iloc[i]
            
            # Ø­Ø³Ø§Ø¨ Ø£Ù‚ØµÙ‰ Ø­Ø±ÙƒØ© Ù…Ø¹ Ù…Ø±Ø§Ø¹Ø§Ø© Ø§Ù„Ù€ wicks
            max_up = (future_highs.max() - current_price) / pip_value
            max_down = (current_price - future_lows.min()) / pip_value
            
            # Ø­Ø³Ø§Ø¨ Ø§Ù„Ø­Ø±ÙƒØ© Ø§Ù„ÙØ¹Ù„ÙŠØ© Ù„Ù„Ø¥ØºÙ„Ø§Ù‚
            close_up = (future_prices.max() - current_price) / pip_value
            close_down = (current_price - future_prices.min()) / pip_value
            
            # ØªØ­Ø¯ÙŠØ¯ Ù†ÙˆØ¹ Ø§Ù„ØµÙÙ‚Ø© Ù…Ø¹ Ù…Ø¹Ø§ÙŠÙŠØ± Ù…Ø­Ø³Ù†Ø©
            if max_up >= min_pips * 2 and close_up >= min_pips * 1.5:
                # Long signal
                targets.append(2)
                
                # Ø­Ø³Ø§Ø¨ Ø§Ù„Ø«Ù‚Ø© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø¹ÙˆØ§Ù…Ù„ Ù…ØªØ¹Ø¯Ø¯Ø©
                confidence = min(
                    0.5 + (close_up / (min_pips * 4)) * 0.3 +  # Ù‚ÙˆØ© Ø§Ù„Ø­Ø±ÙƒØ©
                    (1 - max_down / max_up) * 0.2,  # Ù†Ø³Ø¨Ø© Ø§Ù„ØµØ¹ÙˆØ¯ Ù„Ù„Ù‡Ø¨ÙˆØ·
                    1.0
                )
                confidences.append(confidence)
                
                # Ø­Ø³Ø§Ø¨ SL/TP
                sl_tp = self.calculate_dynamic_sl_tp(df, 'long', i, strategy_name)
                sl_tp_info.append(sl_tp)
                
                # ØªÙ‚ÙŠÙŠÙ… Ø¬ÙˆØ¯Ø© Ø§Ù„ØµÙÙ‚Ø©
                quality = self.evaluate_trade_quality(df, i, 'long', sl_tp, max_up, max_down)
                trade_quality.append(quality)
                
            elif max_down >= min_pips * 2 and close_down >= min_pips * 1.5:
                # Short signal
                targets.append(0)
                
                # Ø­Ø³Ø§Ø¨ Ø§Ù„Ø«Ù‚Ø©
                confidence = min(
                    0.5 + (close_down / (min_pips * 4)) * 0.3 +
                    (1 - max_up / max_down) * 0.2,
                    1.0
                )
                confidences.append(confidence)
                
                # Ø­Ø³Ø§Ø¨ SL/TP
                sl_tp = self.calculate_dynamic_sl_tp(df, 'short', i, strategy_name)
                sl_tp_info.append(sl_tp)
                
                # ØªÙ‚ÙŠÙŠÙ… Ø¬ÙˆØ¯Ø© Ø§Ù„ØµÙÙ‚Ø©
                quality = self.evaluate_trade_quality(df, i, 'short', sl_tp, max_up, max_down)
                trade_quality.append(quality)
                
            else:
                # No trade
                targets.append(1)
                confidences.append(0.5)
                sl_tp_info.append(None)
                trade_quality.append(0)
        
        # Ù…Ù„Ø¡ Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ø£Ø®ÙŠØ±Ø©
        targets.extend([1] * lookahead)
        confidences.extend([0] * lookahead)
        sl_tp_info.extend([None] * lookahead)
        trade_quality.extend([0] * lookahead)
        
        return (np.array(targets), np.array(confidences), 
                sl_tp_info, np.array(trade_quality))
    
    def evaluate_trade_quality(self, df, idx, position_type, sl_tp, max_up, max_down):
        """ØªÙ‚ÙŠÙŠÙ… Ø¬ÙˆØ¯Ø© Ø§Ù„ØµÙÙ‚Ø© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ù…Ø¹Ø§ÙŠÙŠØ± Ù…ØªØ¹Ø¯Ø¯Ø©"""
        quality_score = 0.5  # Ø§Ù„Ø¨Ø¯Ø§ÙŠØ© Ù…Ù† Ø§Ù„Ù…Ù†ØªØµÙ
        
        # 1. Risk/Reward Ratio
        if sl_tp and sl_tp['take_profits']:
            avg_rr = np.mean([abs(tp - df['close'].iloc[idx]) / sl_tp['risk_amount'] 
                            for tp in sl_tp['take_profits']])
            if avg_rr >= 3:
                quality_score += 0.2
            elif avg_rr >= 2:
                quality_score += 0.1
        
        # 2. ÙˆØ¶ÙˆØ­ Ø§Ù„Ø§ØªØ¬Ø§Ù‡
        if position_type == 'long':
            trend_clarity = max_up / (max_down + 1)  # ØªØ¬Ù†Ø¨ Ø§Ù„Ù‚Ø³Ù…Ø© Ø¹Ù„Ù‰ ØµÙØ±
        else:
            trend_clarity = max_down / (max_up + 1)
        
        if trend_clarity >= 3:
            quality_score += 0.15
        elif trend_clarity >= 2:
            quality_score += 0.1
        
        # 3. Ù‚ÙˆØ© Ø§Ù„Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ù„ÙÙ†ÙŠØ©
        rsi_col = 'rsi_14'
        if rsi_col in df.columns:
            rsi = df[rsi_col].iloc[idx]
            if position_type == 'long' and rsi < 70 and rsi > 30:
                quality_score += 0.1
            elif position_type == 'short' and rsi > 30 and rsi < 70:
                quality_score += 0.1
        
        # 4. Ø§Ù„ØªÙ‚Ù„Ø¨ Ø§Ù„Ù…Ù†Ø§Ø³Ø¨
        if sl_tp and 'atr_used' in sl_tp:
            atr_ratio = sl_tp['atr_used'] / df['close'].iloc[idx]
            if 0.001 < atr_ratio < 0.02:  # ØªÙ‚Ù„Ø¨ Ù…Ø¹ØªØ¯Ù„
                quality_score += 0.05
        
        return min(quality_score, 1.0)

    def create_advanced_targets(self, df, strategy):
        """Ø¥Ù†Ø´Ø§Ø¡ Ø£Ù‡Ø¯Ø§Ù Ù…ØªÙ‚Ø¯Ù…Ø© Ù…ØªØ¹Ø¯Ø¯Ø© Ø§Ù„Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ§Øª"""
        lookahead = strategy['lookahead']
        min_pips = strategy['min_pips']
        
        # Ø­Ø³Ø§Ø¨ pip value Ø­Ø³Ø¨ Ø§Ù„Ø¹Ù…Ù„Ø©
        if 'JPY' in df.index.name or 'JPY' in str(df.index.name):
            pip_value = 0.01
        else:
            pip_value = 0.0001
        
        targets = []
        confidences = []
        
        for i in range(len(df) - lookahead):
            future_prices = df['close'].iloc[i+1:i+lookahead+1].values
            current_price = df['close'].iloc[i]
            
            # Ø­Ø³Ø§Ø¨ Ø£Ù‚ØµÙ‰ Ø­Ø±ÙƒØ© ØµØ¹ÙˆØ¯ÙŠØ© ÙˆÙ‡Ø¨ÙˆØ·ÙŠØ©
            max_up = (future_prices.max() - current_price) / pip_value
            max_down = (current_price - future_prices.min()) / pip_value
            
            # Ø­Ø³Ø§Ø¨ Ø§Ù„Ù‡Ø¯Ù ÙˆØ§Ù„Ø«Ù‚Ø©
            if max_up >= min_pips * 2:  # ØµØ¹ÙˆØ¯ Ù‚ÙˆÙŠ
                targets.append(2)
                confidences.append(min(max_up / (min_pips * 3), 1.0))
            elif max_down >= min_pips * 2:  # Ù‡Ø¨ÙˆØ· Ù‚ÙˆÙŠ
                targets.append(0)
                confidences.append(min(max_down / (min_pips * 3), 1.0))
            else:  # Ù…Ø­Ø§ÙŠØ¯ Ø£Ùˆ Ø­Ø±ÙƒØ© Ø¶Ø¹ÙŠÙØ©
                targets.append(1)
                confidences.append(0.5)
        
        # Ù…Ù„Ø¡ Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ø£Ø®ÙŠØ±Ø©
        targets.extend([1] * lookahead)
        confidences.extend([0] * lookahead)
        
        return np.array(targets), np.array(confidences)
    
    def balance_dataset(self, X, y, confidence):
        """Ù…ÙˆØ§Ø²Ù†Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ ØªÙˆØ²ÙŠØ¹ Ù…ØªØ³Ø§ÙˆÙŠ"""
        # Ø§Ù„Ø§Ø­ØªÙØ§Ø¸ Ø¨Ø§Ù„Ø¹ÙŠÙ†Ø§Øª Ø¹Ø§Ù„ÙŠØ© Ø§Ù„Ø«Ù‚Ø© ÙÙ‚Ø·
        high_conf_mask = confidence > 0.7
        X_high = X[high_conf_mask]
        y_high = y[high_conf_mask]
        
        # Ù…ÙˆØ§Ø²Ù†Ø© Ø§Ù„ÙØ¦Ø§Øª
        from imblearn.over_sampling import SMOTE
        from imblearn.under_sampling import RandomUnderSampler
        from imblearn.pipeline import Pipeline
        
        # Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© Ø§Ù„Ù…ÙˆØ§Ø²Ù†Ø©
        over = SMOTE(sampling_strategy=0.8, random_state=42)
        under = RandomUnderSampler(sampling_strategy=1.0, random_state=42)
        
        steps = [('o', over), ('u', under)]
        pipeline = Pipeline(steps=steps)
        
        try:
            X_balanced, y_balanced = pipeline.fit_resample(X_high, y_high)
            logger.info(f"âœ… ØªÙ…Øª Ù…ÙˆØ§Ø²Ù†Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: {len(X_balanced)} Ø¹ÙŠÙ†Ø©")
            return X_balanced, y_balanced
        except:
            logger.warning("âš ï¸ ÙØ´Ù„Øª Ø§Ù„Ù…ÙˆØ§Ø²Ù†Ø©ØŒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø£ØµÙ„ÙŠØ©")
            return X_high, y_high
    
    def optimize_hyperparameters(self, X_train, y_train, X_val, y_val, model_type='lightgbm'):
        """ØªØ­Ø³ÙŠÙ† Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Optuna Ù„Ù„ÙˆØµÙˆÙ„ Ù„Ø£ÙØ¶Ù„ Ø£Ø¯Ø§Ø¡"""
        logger.info(f"ğŸ”§ ØªØ­Ø³ÙŠÙ† Ù…Ø¹Ø§Ù…Ù„Ø§Øª {model_type}...")
        
        def objective(trial):
            if model_type == 'lightgbm':
                params = {
                    'objective': 'multiclass',
                    'num_class': 3,
                    'metric': 'multi_logloss',
                    'boosting_type': trial.suggest_categorical('boosting_type', ['gbdt', 'dart', 'goss']),
                    'num_leaves': trial.suggest_int('num_leaves', 50, 300),
                    'max_depth': trial.suggest_int('max_depth', 5, 15),
                    'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.1),
                    'n_estimators': trial.suggest_int('n_estimators', 500, 3000),
                    'feature_fraction': trial.suggest_float('feature_fraction', 0.7, 1.0),
                    'bagging_fraction': trial.suggest_float('bagging_fraction', 0.7, 1.0),
                    'bagging_freq': trial.suggest_int('bagging_freq', 1, 10),
                    'min_child_samples': trial.suggest_int('min_child_samples', 10, 50),
                    'lambda_l1': trial.suggest_float('lambda_l1', 0, 1),
                    'lambda_l2': trial.suggest_float('lambda_l2', 0, 1),
                    'verbosity': -1
                }
                
                model = lgb.LGBMClassifier(**params)
                
            elif model_type == 'xgboost':
                params = {
                    'objective': 'multi:softprob',
                    'num_class': 3,
                    'max_depth': trial.suggest_int('max_depth', 5, 15),
                    'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.1),
                    'n_estimators': trial.suggest_int('n_estimators', 500, 3000),
                    'subsample': trial.suggest_float('subsample', 0.7, 1.0),
                    'colsample_bytree': trial.suggest_float('colsample_bytree', 0.7, 1.0),
                    'gamma': trial.suggest_float('gamma', 0, 0.5),
                    'reg_alpha': trial.suggest_float('reg_alpha', 0, 1),
                    'reg_lambda': trial.suggest_float('reg_lambda', 0, 1),
                    'use_label_encoder': False,
                    'eval_metric': 'mlogloss'
                }
                
                model = xgb.XGBClassifier(**params)
            
            # ØªØ¯Ø±ÙŠØ¨ ÙˆØªÙ‚ÙŠÙŠÙ…
            model.fit(X_train, y_train, eval_set=[(X_val, y_val)], 
                     early_stopping_rounds=50, verbose=False)
            
            y_pred = model.predict(X_val)
            accuracy = accuracy_score(y_val, y_pred)
            
            # Ù†Ø±ÙŠØ¯ Ø£ÙŠØ¶Ø§Ù‹ precision Ø¹Ø§Ù„ÙŠØ©
            precision = precision_score(y_val, y_pred, average='weighted', zero_division=0)
            
            # Ø§Ù„Ù‡Ø¯Ù: Ù…Ø²ÙŠØ¬ Ù…Ù† Ø§Ù„Ø¯Ù‚Ø© ÙˆØ§Ù„precision
            return accuracy * 0.7 + precision * 0.3
        
        # ØªØ´ØºÙŠÙ„ Ø§Ù„ØªØ­Ø³ÙŠÙ†
        study = optuna.create_study(direction='maximize')
        study.optimize(objective, n_trials=50, n_jobs=1, show_progress_bar=True)
        
        logger.info(f"âœ… Ø£ÙØ¶Ù„ Ø£Ø¯Ø§Ø¡: {study.best_value:.4f}")
        
        return study.best_params
    
    def train_ensemble_95plus(self, X_train, y_train, X_val, y_val, optimized_params):
        """ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬ Ù…Ø¬Ù…Ø¹ Ù„Ù„ÙˆØµÙˆÙ„ Ù„Ø¯Ù‚Ø© 95%+"""
        logger.info("ğŸ¯ ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¬Ù…Ø¹ Ø§Ù„Ù…ØªÙ‚Ø¯Ù…...")
        
        models = []
        
        # 1. LightGBM Ù…Ø­Ø³Ù†
        lgb_params = self.ensemble_models['lightgbm_primary'].copy()
        lgb_params.update(optimized_params.get('lightgbm', {}))
        lgb_model = lgb.LGBMClassifier(**lgb_params)
        models.append(('lightgbm', lgb_model))
        
        # 2. XGBoost Ù…Ø­Ø³Ù†
        xgb_params = self.ensemble_models['xgboost_primary'].copy()
        xgb_params.update(optimized_params.get('xgboost', {}))
        xgb_model = xgb.XGBClassifier(**xgb_params)
        models.append(('xgboost', xgb_model))
        
        # 3. CatBoost
        cat_model = CatBoostClassifier(**self.ensemble_models['catboost_primary'])
        models.append(('catboost', cat_model))
        
        # 4. Extra Trees
        et_model = ExtraTreesClassifier(**self.ensemble_models['extra_trees'])
        models.append(('extra_trees', et_model))
        
        # 5. Neural Network
        nn_model = MLPClassifier(**self.ensemble_models['neural_network'])
        models.append(('neural_net', nn_model))
        
        # Ù†Ù…ÙˆØ°Ø¬ Ù…Ø¬Ù…Ø¹ Ù…ØªÙ‚Ø¯Ù…
        ensemble = VotingClassifier(models, voting='soft', n_jobs=-1)
        
        # ØªØ¯Ø±ÙŠØ¨ Ù…Ø¹ ØªØªØ¨Ø¹ Ø§Ù„Ø£Ø¯Ø§Ø¡
        logger.info("â³ Ø¬Ø§Ø±ÙŠ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ (Ù‚Ø¯ ÙŠØ³ØªØºØ±Ù‚ ÙˆÙ‚ØªØ§Ù‹)...")
        ensemble.fit(X_train, y_train)
        
        # ØªÙ‚ÙŠÙŠÙ… Ù…ÙØµÙ„
        train_pred = ensemble.predict(X_train)
        val_pred = ensemble.predict(X_val)
        
        train_acc = accuracy_score(y_train, train_pred)
        val_acc = accuracy_score(y_val, val_pred)
        
        # ØªÙ‚ÙŠÙŠÙ… ÙƒÙ„ Ù†Ù…ÙˆØ°Ø¬ Ù…Ù†ÙØ±Ø¯
        individual_scores = {}
        for name, model in models:
            model.fit(X_train, y_train)
            score = model.score(X_val, y_val)
            individual_scores[name] = score
            logger.info(f"  â€¢ {name}: {score:.4f}")
        
        logger.info(f"âœ… Ø¯Ù‚Ø© Ø§Ù„ØªØ¯Ø±ÙŠØ¨: {train_acc:.4f}")
        logger.info(f"âœ… Ø¯Ù‚Ø© Ø§Ù„ØªØ­Ù‚Ù‚: {val_acc:.4f}")
        
        return ensemble, {
            'train_accuracy': train_acc,
            'val_accuracy': val_acc,
            'individual_scores': individual_scores
        }
    
    def train_symbol_advanced(self, symbol, timeframe):
        """ØªØ¯Ø±ÙŠØ¨ Ù…ØªÙ‚Ø¯Ù… Ù„Ø¹Ù…Ù„Ø© ÙˆØ§Ø­Ø¯Ø©"""
        logger.info(f"\n{'='*80}")
        logger.info(f"ğŸš€ ØªØ¯Ø±ÙŠØ¨ Ù…ØªÙ‚Ø¯Ù… Ù„Ù€ {symbol} {timeframe}")
        logger.info(f"{'='*80}")
        
        # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        df = self.load_data_advanced(symbol, timeframe)
        if df is None:
            return None
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø©
        features = self.create_ultra_advanced_features(df, symbol)
        
        # Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù„ØªØ¯Ø±ÙŠØ¨
        X = features.values
        feature_names = features.columns.tolist()
        
        results = {
            'symbol': symbol,
            'timeframe': timeframe,
            'strategies': {},
            'best_accuracy': 0,
            'best_strategy': None
        }
        
        # ØªØ¯Ø±ÙŠØ¨ Ù„ÙƒÙ„ Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ©
        for strategy_name, strategy in self.training_strategies.items():
            logger.info(f"\nğŸ“Š Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ©: {strategy_name}")
            
            # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ø£Ù‡Ø¯Ø§Ù
            # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ø£Ù‡Ø¯Ø§Ù Ù…Ø¹ SL/TP
            if hasattr(self, 'create_advanced_targets_with_sl_tp'):
                y, confidence, sl_tp_info, quality = self.create_advanced_targets_with_sl_tp(df, strategy_name)
                
                # ÙÙ„ØªØ±Ø© Ø§Ù„ØµÙÙ‚Ø§Øª Ø¹Ø§Ù„ÙŠØ© Ø§Ù„Ø¬ÙˆØ¯Ø© ÙÙ‚Ø·
                high_quality_mask = quality > 0.7
                X_balanced, y_balanced = self.balance_dataset(X[high_quality_mask], 
                                                             y[high_quality_mask], 
                                                             confidence[high_quality_mask])
            else:
                # Ø§Ù„Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø© ÙƒÙ€ fallback
                y, confidence = self.create_advanced_targets(df, strategy)
                X_balanced, y_balanced = self.balance_dataset(X, y, confidence)

            
            # Ù…ÙˆØ§Ø²Ù†Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
            X_balanced, y_balanced = self.balance_dataset(X, y, confidence)
            
            # ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
            split_idx = int(len(X_balanced) * 0.7)
            X_temp = X_balanced[:split_idx]
            X_test = X_balanced[split_idx:]
            y_temp = y_balanced[:split_idx]
            y_test = y_balanced[split_idx:]
            
            split_idx2 = int(len(X_temp) * 0.85)
            X_train = X_temp[:split_idx2]
            X_val = X_temp[split_idx2:]
            y_train = y_temp[:split_idx2]
            y_val = y_temp[split_idx2:]
            
            # Ù…Ø¹Ø§ÙŠØ±Ø© Ù…ØªÙ‚Ø¯Ù…Ø©
            scaler = RobustScaler()
            X_train_scaled = scaler.fit_transform(X_train)
            X_val_scaled = scaler.transform(X_val)
            X_test_scaled = scaler.transform(X_test)
            
            # ØªØ­Ø³ÙŠÙ† Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª
            optimized_params = {}
            if strategy_name in ['short_term', 'medium_term']:  # ØªØ­Ø³ÙŠÙ† Ù„Ù„Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ§Øª Ø§Ù„Ù…Ù‡Ù…Ø©
                lgb_params = self.optimize_hyperparameters(
                    X_train_scaled, y_train, X_val_scaled, y_val, 'lightgbm'
                )
                optimized_params['lightgbm'] = lgb_params
            
            # ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¬Ù…Ø¹
            model, scores = self.train_ensemble_95plus(
                X_train_scaled, y_train, X_val_scaled, y_val, optimized_params
            )
            
            # ØªÙ‚ÙŠÙŠÙ… Ø¹Ù„Ù‰ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±
            y_pred = model.predict(X_test_scaled)
            test_accuracy = accuracy_score(y_test, y_pred)
            test_precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)
            test_recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)
            test_f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)
            
            # ØªÙ‚ÙŠÙŠÙ… Ø¥Ø¶Ø§ÙÙŠ - Ù…Ø¹Ø¯Ù„ Ø§Ù„Ù†Ø¬Ø§Ø­ Ù„Ù„ØµÙÙ‚Ø§Øª ÙÙ‚Ø·
            trade_mask = y_pred != 1  # ØºÙŠØ± Ù…Ø­Ø§ÙŠØ¯
            if trade_mask.sum() > 0:
                trade_accuracy = accuracy_score(y_test[trade_mask], y_pred[trade_mask])
            else:
                trade_accuracy = 0
            
            logger.info(f"ğŸ“ˆ Ø¯Ù‚Ø© Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±: {test_accuracy:.4f}")
            logger.info(f"ğŸ“Š Ø¯Ù‚Ø© Ø§Ù„ØµÙÙ‚Ø§Øª: {trade_accuracy:.4f}")
            logger.info(f"ğŸ¯ Precision: {test_precision:.4f}, Recall: {test_recall:.4f}")
            
            # Ø­ÙØ¸ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
            strategy_results = {
                'accuracy': test_accuracy,
                'trade_accuracy': trade_accuracy,
                'precision': test_precision,
                'recall': test_recall,
                'f1': test_f1,
                'confidence_threshold': strategy['confidence_threshold'],
                'scores': scores
            }
            
            results['strategies'][strategy_name] = strategy_results
            
            # ØªØ­Ø¯ÙŠØ« Ø£ÙØ¶Ù„ Ù†ØªÙŠØ¬Ø©
            if test_accuracy > results['best_accuracy']:
                results['best_accuracy'] = test_accuracy
                results['best_strategy'] = strategy_name
            
            # Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø£Ø¯Ø§Ø¡ Ù…Ù…ØªØ§Ø²
            if test_accuracy >= 0.85:  # 85%+ 
                model_dir = Path(f"models/{symbol}_{timeframe}/{strategy_name}")
                model_dir.mkdir(parents=True, exist_ok=True)
                
                model_data = {
                    'model': model,
                    'scaler': scaler,
                    'feature_names': feature_names,
                    'strategy': strategy,
                    'results': strategy_results,
                    'training_date': datetime.now()
                }
                
                joblib.dump(model_data, model_dir / 'model_advanced.pkl')
                logger.info(f"ğŸ’¾ ØªÙ… Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬: {model_dir}")
        
        # Ø·Ø¨Ø§Ø¹Ø© Ø§Ù„Ù…Ù„Ø®Øµ
        logger.info(f"\n{'='*60}")
        logger.info(f"ğŸ“Š Ù…Ù„Ø®Øµ Ù†ØªØ§Ø¦Ø¬ {symbol} {timeframe}")
        logger.info(f"ğŸ† Ø£ÙØ¶Ù„ Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ©: {results['best_strategy']}")
        logger.info(f"ğŸ¯ Ø£ÙØ¶Ù„ Ø¯Ù‚Ø©: {results['best_accuracy']:.4f}")
        
        return results
    
    def train_all_advanced(self):
        """ØªØ¯Ø±ÙŠØ¨ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø¹Ù…Ù„Ø§Øª Ø§Ù„Ù…ØªØ§Ø­Ø©"""
        logger.info("\n" + "="*100)
        logger.info("ğŸš€ Ø¨Ø¯Ø¡ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù…ØªÙ‚Ø¯Ù… Ø§Ù„Ø´Ø§Ù…Ù„ - Ù‡Ø¯Ù 95%+ Ø¯Ù‚Ø©")
        logger.info("="*100)
        
        # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        if not self.check_database():
            return
        
        # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ø¹Ù…Ù„Ø§Øª Ø§Ù„Ù…ØªØ§Ø­Ø©
        try:
            conn = sqlite3.connect("data/forex_ml.db")
            query = """
                SELECT symbol, timeframe, COUNT(*) as count
                FROM price_data
                GROUP BY symbol, timeframe
                HAVING count >= ?
                ORDER BY symbol, timeframe
            """
            available = pd.read_sql_query(query, conn, params=(self.min_data_points,))
            conn.close()
            
            logger.info(f"ğŸ“Š Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ù…ØªØ§Ø­Ø©: {len(available)}")
            
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: {e}")
            return
        
        # Ù†ØªØ§Ø¦Ø¬ Ø§Ù„ØªØ¯Ø±ÙŠØ¨
        excellent_models = []  # 90%+
        good_models = []       # 85-90%
        acceptable_models = [] # 80-85%
        failed_models = []
        
        # ØªØ¯Ø±ÙŠØ¨ ÙƒÙ„ Ø¹Ù…Ù„Ø©
        for idx, row in available.iterrows():
            symbol = row['symbol']
            timeframe = row['timeframe']
            
            try:
                logger.info(f"\nğŸ“Š Ù…Ø¹Ø§Ù„Ø¬Ø© {idx+1}/{len(available)}: {symbol} {timeframe}")
                
                results = self.train_symbol_advanced(symbol, timeframe)
                
                if results:
                    best_acc = results['best_accuracy']
                    
                    model_info = {
                        'symbol': symbol,
                        'timeframe': timeframe,
                        'accuracy': best_acc,
                        'strategy': results['best_strategy']
                    }
                    
                    if best_acc >= 0.90:
                        excellent_models.append(model_info)
                        logger.info(f"ğŸŒŸ Ù…Ù…ØªØ§Ø²! Ø¯Ù‚Ø© {best_acc:.4f}")
                    elif best_acc >= 0.85:
                        good_models.append(model_info)
                        logger.info(f"âœ… Ø¬ÙŠØ¯ Ø¬Ø¯Ø§Ù‹! Ø¯Ù‚Ø© {best_acc:.4f}")
                    elif best_acc >= 0.80:
                        acceptable_models.append(model_info)
                        logger.info(f"ğŸ‘ Ù…Ù‚Ø¨ÙˆÙ„! Ø¯Ù‚Ø© {best_acc:.4f}")
                    else:
                        failed_models.append(f"{symbol} {timeframe}")
                else:
                    failed_models.append(f"{symbol} {timeframe}")
                    
            except Exception as e:
                logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªØ¯Ø±ÙŠØ¨ {symbol} {timeframe}: {str(e)}")
                failed_models.append(f"{symbol} {timeframe}: {str(e)}")
        
        # Ø·Ø¨Ø§Ø¹Ø© Ø§Ù„ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ
        self._print_final_report(excellent_models, good_models, acceptable_models, failed_models)
    
    def _print_final_report(self, excellent, good, acceptable, failed):
        """Ø·Ø¨Ø§Ø¹Ø© Ø§Ù„ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ Ø§Ù„Ù…ÙØµÙ„"""
        logger.info("\n" + "="*100)
        logger.info("ğŸ“Š Ø§Ù„ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ Ù„Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù…ØªÙ‚Ø¯Ù…")
        logger.info("="*100)
        
        total = len(excellent) + len(good) + len(acceptable) + len(failed)
        
        logger.info(f"\nğŸ“ˆ Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø£Ø¯Ø§Ø¡:")
        logger.info(f"  ğŸŒŸ Ù…Ù…ØªØ§Ø² (90%+): {len(excellent)} Ù†Ù…ÙˆØ°Ø¬")
        logger.info(f"  âœ… Ø¬ÙŠØ¯ Ø¬Ø¯Ø§Ù‹ (85-90%): {len(good)} Ù†Ù…ÙˆØ°Ø¬")
        logger.info(f"  ğŸ‘ Ù…Ù‚Ø¨ÙˆÙ„ (80-85%): {len(acceptable)} Ù†Ù…ÙˆØ°Ø¬")
        logger.info(f"  âŒ ÙØ´Ù„ (<80%): {len(failed)} Ù†Ù…ÙˆØ°Ø¬")
        
        success_rate = (len(excellent) + len(good)) / total * 100 if total > 0 else 0
        logger.info(f"\nğŸ¯ Ù…Ø¹Ø¯Ù„ Ø§Ù„Ù†Ø¬Ø§Ø­ (85%+): {success_rate:.1f}%")
        
        if excellent:
            logger.info(f"\nğŸŒŸ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…Ù…ØªØ§Ø²Ø© (90%+):")
            for model in sorted(excellent, key=lambda x: x['accuracy'], reverse=True)[:10]:
                logger.info(f"  â€¢ {model['symbol']} {model['timeframe']}: "
                          f"{model['accuracy']:.4f} ({model['strategy']})")
        
        if good:
            logger.info(f"\nâœ… Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ø¬ÙŠØ¯Ø© Ø¬Ø¯Ø§Ù‹ (85-90%):")
            for model in sorted(good, key=lambda x: x['accuracy'], reverse=True)[:10]:
                logger.info(f"  â€¢ {model['symbol']} {model['timeframe']}: "
                          f"{model['accuracy']:.4f} ({model['strategy']})")
        
        # Ø­ÙØ¸ Ø§Ù„ØªÙ‚Ø±ÙŠØ±
        report = {
            'training_date': datetime.now().isoformat(),
            'total_models': total,
            'excellent_count': len(excellent),
            'good_count': len(good),
            'acceptable_count': len(acceptable),
            'failed_count': len(failed),
            'success_rate': success_rate,
            'excellent_models': excellent,
            'good_models': good,
            'acceptable_models': acceptable,
            'configuration': {
                'min_data_points': self.min_data_points,
                'strategies': list(self.training_strategies.keys()),
                'ensemble_models': list(self.ensemble_models.keys())
            }
        }
        
        report_path = Path("models/advanced_training_report.json")
        report_path.parent.mkdir(exist_ok=True)
        
        with open(report_path, 'w') as f:
            json.dump(report, f, indent=2, default=str)
        
        logger.info(f"\nğŸ’¾ ØªÙ… Ø­ÙØ¸ Ø§Ù„ØªÙ‚Ø±ÙŠØ±: {report_path}")
        
        if success_rate >= 70:
            logger.info("\nğŸ‰ ØªÙ‡Ø§Ù†ÙŠÙ†Ø§! ØªÙ… ØªØ­Ù‚ÙŠÙ‚ Ù†ØªØ§Ø¦Ø¬ Ù…Ù…ØªØ§Ø²Ø©!")
        else:
            logger.info("\nğŸ’¡ Ù†ØµØ§Ø¦Ø­ Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø£Ø¯Ø§Ø¡:")
            logger.info("  â€¢ Ø¬Ù…Ø¹ Ø§Ù„Ù…Ø²ÙŠØ¯ Ù…Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª (3+ Ø³Ù†ÙˆØ§Øª)")
            logger.info("  â€¢ ØªØ¬Ø±Ø¨Ø© ÙØªØ±Ø§Øª Ø²Ù…Ù†ÙŠØ© Ø£Ø·ÙˆÙ„")
            logger.info("  â€¢ Ø¶Ø¨Ø· Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨")

def main():
    """Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©"""
    trainer = AdvancedCompleteTrainer()
    
    # Ù„Ù„Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø³Ø±ÙŠØ¹
    if len(sys.argv) > 1 and sys.argv[1] == '--quick':
        # ØªØ¯Ø±ÙŠØ¨ Ø¹Ù…Ù„Ø© ÙˆØ§Ø­Ø¯Ø© ÙÙ‚Ø·
        trainer.train_symbol_advanced("EURUSD", "H1")
    else:
        # ØªØ¯Ø±ÙŠØ¨ Ø´Ø§Ù…Ù„
        trainer.train_all_advanced()

if __name__ == "__main__":
    main()