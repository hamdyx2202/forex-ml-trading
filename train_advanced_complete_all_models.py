#!/usr/bin/env python3
"""
Advanced Complete Training System with All Models - Ù†Ø¸Ø§Ù… Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„ÙƒØ§Ù…Ù„ Ø¨Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬
ÙŠØ³ØªØ®Ø¯Ù… 5 Ù†Ù…Ø§Ø°Ø¬ Ø°ÙƒØ§Ø¡ Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ù…Ø®ØªÙ„ÙØ© Ù…Ø¹ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¢Ù…Ù†Ø©
"""

import os
import sys
import json
import sqlite3
import numpy as np
import pandas as pd
from datetime import datetime, timedelta
from pathlib import Path
import joblib
import warnings
warnings.filterwarnings('ignore')

from loguru import logger
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import RobustScaler
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.ensemble import VotingClassifier, RandomForestClassifier, ExtraTreesClassifier
from sklearn.neural_network import MLPClassifier
import lightgbm as lgb
import xgboost as xgb
from catboost import CatBoostClassifier
import optuna
import talib
import time
import gc
import psutil

# Ù…ÙƒØªØ¨Ø§Øª Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…ØªÙˆØ§Ø²ÙŠØ©
from concurrent.futures import ThreadPoolExecutor, as_completed
from multiprocessing import cpu_count
import threading

# Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ³Ø¬ÙŠÙ„
logger.remove()
logger.add(sys.stdout, format="<green>{time:YYYY-MM-DD HH:mm:ss}</green> | <level>{level: <8}</level> | <level>{message}</level>")
logger.add("logs/advanced_training_all_models.log", rotation="1 day", retention="30 days")

class AdvancedCompleteTrainerAllModels:
    """Ù†Ø¸Ø§Ù… Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù…ØªÙ‚Ø¯Ù… Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø¬Ù…ÙŠØ¹ Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ"""
    
    def __init__(self, use_all_models=True):
        self.min_data_points = 10000
        self.test_size = 0.2
        self.validation_split = 0.15
        self.random_state = 42
        self.use_all_models = use_all_models
        
        # Ø¹Ø¯Ø¯ Ø§Ù„Ø¹Ù…Ù„ÙŠØ§Øª Ø§Ù„Ù…ØªÙˆØ§Ø²ÙŠØ©
        self.max_workers = min(cpu_count() - 1, 4)
        logger.info(f"ğŸš€ Ø§Ø³ØªØ®Ø¯Ø§Ù… {self.max_workers} Ø¹Ù…Ù„ÙŠØ§Øª Ù…ØªÙˆØ§Ø²ÙŠØ©")
        
        # Ø¥Ø¸Ù‡Ø§Ø± Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…Ø©
        if self.use_all_models:
            logger.info("ğŸ¤– Ø§Ø³ØªØ®Ø¯Ø§Ù… 5 Ù†Ù…Ø§Ø°Ø¬ Ø°ÙƒØ§Ø¡ Ø§ØµØ·Ù†Ø§Ø¹ÙŠ:")
            logger.info("  1ï¸âƒ£ LightGBM")
            logger.info("  2ï¸âƒ£ XGBoost") 
            logger.info("  3ï¸âƒ£ CatBoost")
            logger.info("  4ï¸âƒ£ Extra Trees")
            logger.info("  5ï¸âƒ£ Neural Network")
        else:
            logger.info("ğŸ¤– Ø§Ø³ØªØ®Ø¯Ø§Ù… LightGBM ÙÙ‚Ø· (Ø§Ù„ÙˆØ¶Ø¹ Ø§Ù„Ø³Ø±ÙŠØ¹)")
        
        # Ù…Ø±Ø§Ù‚Ø¨Ø© Ø§Ù„Ø°Ø§ÙƒØ±Ø©
        self.monitor_memory()
        
        # Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨
        self.training_strategies = {
            'ultra_short': {'lookahead': 5, 'min_pips': 3, 'confidence_threshold': 0.95},
            'scalping': {'lookahead': 15, 'min_pips': 5, 'confidence_threshold': 0.92},
            'short_term': {'lookahead': 30, 'min_pips': 10, 'confidence_threshold': 0.90},
            'medium_term': {'lookahead': 60, 'min_pips': 20, 'confidence_threshold': 0.88},
            'long_term': {'lookahead': 240, 'min_pips': 40, 'confidence_threshold': 0.85}
        }
        
        # Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ù†Ù…Ø§Ø°Ø¬
        self.model_configs = {
            'lightgbm': {
                'objective': 'multiclass',
                'num_class': 3,
                'metric': 'multi_logloss',
                'boosting_type': 'gbdt',
                'num_leaves': 100,
                'learning_rate': 0.01,
                'n_estimators': 1000,
                'feature_fraction': 0.9,
                'bagging_fraction': 0.9,
                'bagging_freq': 5,
                'lambda_l1': 0.1,
                'lambda_l2': 0.1,
                'verbosity': -1,
                'n_jobs': 1
            },
            'xgboost': {
                'objective': 'multi:softprob',
                'num_class': 3,
                'max_depth': 10,
                'learning_rate': 0.01,
                'n_estimators': 1000,
                'subsample': 0.9,
                'colsample_bytree': 0.9,
                'gamma': 0.1,
                'eval_metric': 'mlogloss',
                'use_label_encoder': False,
                'n_jobs': 1
            },
            'catboost': {
                'loss_function': 'MultiClass',
                'classes_count': 3,
                'iterations': 1000,
                'depth': 8,
                'learning_rate': 0.01,
                'l2_leaf_reg': 5,
                'verbose': False,
                'thread_count': 1
            },
            'extra_trees': {
                'n_estimators': 300,
                'max_depth': 20,
                'min_samples_split': 5,
                'min_samples_leaf': 2,
                'max_features': 'sqrt',
                'class_weight': 'balanced',
                'n_jobs': 1
            },
            'neural_network': {
                'hidden_layer_sizes': (150, 100, 50),
                'activation': 'relu',
                'solver': 'adam',
                'alpha': 0.001,
                'learning_rate': 'adaptive',
                'max_iter': 500,
                'early_stopping': True,
                'validation_fraction': 0.1
            }
        }
    
    def monitor_memory(self):
        """Ù…Ø±Ø§Ù‚Ø¨Ø© Ø§Ù„Ø°Ø§ÙƒØ±Ø©"""
        memory = psutil.virtual_memory()
        logger.info(f"ğŸ’¾ Ø§Ù„Ø°Ø§ÙƒØ±Ø©: {memory.percent}% ({memory.used / 1024**3:.1f}GB / {memory.total / 1024**3:.1f}GB)")
    
    def check_database(self):
        """Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª"""
        db_path = Path("data/forex_ml.db")
        
        if not db_path.exists():
            logger.error("âŒ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯Ø©!")
            return False
        
        try:
            conn = sqlite3.connect(db_path)
            cursor = conn.cursor()
            cursor.execute("SELECT COUNT(*) FROM price_data")
            count = cursor.fetchone()[0]
            conn.close()
            
            logger.info(f"âœ… Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¬Ø§Ù‡Ø²Ø©: {count:,} Ø³Ø¬Ù„")
            return count > 0
            
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø£: {e}")
            return False
    
    def load_data_advanced(self, symbol, timeframe, limit=100000):
        """ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª"""
        try:
            conn = sqlite3.connect("data/forex_ml.db")
            query = """
                SELECT * FROM price_data 
                WHERE symbol = ? AND timeframe = ?
                ORDER BY time ASC
                LIMIT ?
            """
            df = pd.read_sql_query(query, conn, params=(symbol, timeframe, limit))
            conn.close()
            
            if len(df) < self.min_data_points:
                return None
            
            df['time'] = pd.to_datetime(df['time'], unit='s')
            df = df.set_index('time')
            df = df.fillna(method='ffill').fillna(method='bfill')
            
            logger.info(f"âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ {len(df)} Ø³Ø¬Ù„")
            return df
            
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø£: {e}")
            return None
    
    def create_features(self, df):
        """Ø¥Ù†Ø´Ø§Ø¡ Ù…ÙŠØ²Ø§Øª Ù…ØªÙ‚Ø¯Ù…Ø©"""
        logger.info("ğŸ”¬ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…ÙŠØ²Ø§Øª...")
        features = pd.DataFrame(index=df.index)
        
        # 1. Ù…ÙŠØ²Ø§Øª Ø§Ù„Ø£Ø³Ø¹Ø§Ø±
        features['returns'] = df['close'].pct_change()
        features['log_returns'] = np.log(df['close'] / df['close'].shift(1))
        features['price_range'] = (df['high'] - df['low']) / df['close']
        features['body_size'] = abs(df['close'] - df['open']) / df['close']
        
        # 2. Ø§Ù„Ù…ØªÙˆØ³Ø·Ø§Øª Ø§Ù„Ù…ØªØ­Ø±ÙƒØ©
        for period in [5, 10, 20, 50, 100, 200]:
            sma = df['close'].rolling(period).mean()
            ema = df['close'].ewm(span=period, adjust=False).mean()
            features[f'sma_{period}'] = (df['close'] - sma) / sma
            features[f'ema_{period}'] = (df['close'] - ema) / ema
            features[f'sma_slope_{period}'] = sma.pct_change(5)
        
        # 3. Ù…Ø¤Ø´Ø±Ø§Øª ÙÙ†ÙŠØ©
        try:
            # RSI
            for period in [7, 14, 21]:
                features[f'rsi_{period}'] = talib.RSI(df['close'].values, timeperiod=period)
            
            # MACD
            macd, signal, hist = talib.MACD(df['close'].values)
            features['macd'] = macd
            features['macd_signal'] = signal
            features['macd_hist'] = hist
            
            # Bollinger Bands
            upper, middle, lower = talib.BBANDS(df['close'].values)
            features['bb_upper'] = (upper - df['close']) / df['close']
            features['bb_lower'] = (df['close'] - lower) / df['close']
            
            # ATR
            features['atr_14'] = talib.ATR(df['high'].values, df['low'].values, 
                                          df['close'].values, timeperiod=14) / df['close']
            
            # ADX
            features['adx_14'] = talib.ADX(df['high'].values, df['low'].values,
                                          df['close'].values, timeperiod=14)
            
            # Stochastic
            slowk, slowd = talib.STOCH(df['high'].values, df['low'].values, df['close'].values)
            features['stoch_k'] = slowk
            features['stoch_d'] = slowd
            
        except Exception as e:
            logger.warning(f"âš ï¸ Ø¨Ø¹Ø¶ Ø§Ù„Ù…Ø¤Ø´Ø±Ø§Øª ÙØ´Ù„Øª: {e}")
        
        # 4. Ù…ÙŠØ²Ø§Øª Ø§Ù„Ø­Ø¬Ù…
        features['volume_sma'] = df['volume'] / df['volume'].rolling(20).mean()
        features['obv'] = (np.sign(df['close'].diff()) * df['volume']).cumsum()
        
        # 5. Ù…ÙŠØ²Ø§Øª Ø§Ù„ÙˆÙ‚Øª
        features['hour'] = df.index.hour
        features['day_of_week'] = df.index.dayofweek
        features['month'] = df.index.month
        
        # 6. Ø§Ù„ØªÙ‚Ù„Ø¨
        for period in [10, 20, 30]:
            returns = df['close'].pct_change()
            features[f'volatility_{period}'] = returns.rolling(period).std()
        
        # ØªÙ†Ø¸ÙŠÙ
        features = features.fillna(method='ffill').fillna(0)
        features = features.replace([np.inf, -np.inf], 0)
        
        logger.info(f"âœ… ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ {len(features.columns)} Ù…ÙŠØ²Ø©")
        return features
    
    def create_targets(self, df, strategy):
        """Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ø£Ù‡Ø¯Ø§Ù"""
        lookahead = strategy['lookahead']
        min_pips = strategy['min_pips']
        pip_value = 0.0001 if 'JPY' not in str(df.index.name) else 0.01
        
        targets = []
        confidences = []
        
        for i in range(len(df) - lookahead):
            future_prices = df['close'].iloc[i+1:i+lookahead+1].values
            current_price = df['close'].iloc[i]
            
            max_up = (future_prices.max() - current_price) / pip_value
            max_down = (current_price - future_prices.min()) / pip_value
            
            if max_up >= min_pips * 2:
                targets.append(2)  # Buy
                confidences.append(min(max_up / (min_pips * 3), 1.0))
            elif max_down >= min_pips * 2:
                targets.append(0)  # Sell
                confidences.append(min(max_down / (min_pips * 3), 1.0))
            else:
                targets.append(1)  # Hold
                confidences.append(0.5)
        
        targets.extend([1] * lookahead)
        confidences.extend([0] * lookahead)
        
        return np.array(targets), np.array(confidences)
    
    def balance_data(self, X, y, confidence):
        """Ù…ÙˆØ§Ø²Ù†Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª"""
        from imblearn.over_sampling import SMOTE
        from imblearn.under_sampling import RandomUnderSampler
        from imblearn.pipeline import Pipeline
        
        # ÙÙ„ØªØ±Ø© Ø§Ù„Ø¹ÙŠÙ†Ø§Øª Ø¹Ø§Ù„ÙŠØ© Ø§Ù„Ø«Ù‚Ø©
        high_conf = confidence > 0.7
        X_high = X[high_conf]
        y_high = y[high_conf]
        
        try:
            # Ù…ÙˆØ§Ø²Ù†Ø©
            over = SMOTE(sampling_strategy=0.8, random_state=42)
            under = RandomUnderSampler(sampling_strategy=1.0, random_state=42)
            pipeline = Pipeline([('o', over), ('u', under)])
            
            X_balanced, y_balanced = pipeline.fit_resample(X_high, y_high)
            logger.info(f"âœ… ØªÙ…Øª Ø§Ù„Ù…ÙˆØ§Ø²Ù†Ø©: {len(X_balanced)} Ø¹ÙŠÙ†Ø©")
            return X_balanced, y_balanced
        except:
            return X_high, y_high
    
    def train_single_model(self, model_name, model_config, X_train, y_train, X_val, y_val):
        """ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬ ÙˆØ§Ø­Ø¯"""
        logger.info(f"  â€¢ ØªØ¯Ø±ÙŠØ¨ {model_name}...")
        
        try:
            if model_name == 'lightgbm':
                model = lgb.LGBMClassifier(**model_config)
                model.fit(X_train, y_train,
                         eval_set=[(X_val, y_val)],
                         callbacks=[lgb.early_stopping(50), lgb.log_evaluation(0)])
                         
            elif model_name == 'xgboost':
                model = xgb.XGBClassifier(**model_config)
                model.fit(X_train, y_train,
                         eval_set=[(X_val, y_val)],
                         early_stopping_rounds=50,
                         verbose=False)
                         
            elif model_name == 'catboost':
                model = CatBoostClassifier(**model_config)
                model.fit(X_train, y_train,
                         eval_set=(X_val, y_val),
                         early_stopping_rounds=50,
                         verbose=False)
                         
            elif model_name == 'extra_trees':
                model = ExtraTreesClassifier(**model_config)
                model.fit(X_train, y_train)
                
            elif model_name == 'neural_network':
                model = MLPClassifier(**model_config)
                model.fit(X_train, y_train)
            
            # ØªÙ‚ÙŠÙŠÙ…
            val_score = model.score(X_val, y_val)
            logger.info(f"    âœ“ {model_name}: Ø¯Ù‚Ø© {val_score:.4f}")
            
            return model_name, model, val_score
            
        except Exception as e:
            logger.error(f"    âœ— ÙØ´Ù„ {model_name}: {e}")
            return model_name, None, 0
    
    def train_ensemble_models(self, X_train, y_train, X_val, y_val, X_test, y_test):
        """ØªØ¯Ø±ÙŠØ¨ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ ÙˆØ¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù…Ø¬Ù…Ø¹"""
        logger.info("ğŸ¯ ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬...")
        
        if not self.use_all_models:
            # Ø§Ø³ØªØ®Ø¯Ø§Ù… LightGBM ÙÙ‚Ø·
            model = lgb.LGBMClassifier(**self.model_configs['lightgbm'])
            model.fit(X_train, y_train,
                     eval_set=[(X_val, y_val)],
                     callbacks=[lgb.early_stopping(50), lgb.log_evaluation(0)])
            
            y_pred = model.predict(X_test)
            accuracy = accuracy_score(y_test, y_pred)
            
            return model, {'accuracy': accuracy}
        
        # ØªØ¯Ø±ÙŠØ¨ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø¨Ø§Ù„ØªÙˆØ§Ø²ÙŠ
        trained_models = []
        
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            futures = []
            
            for model_name, config in self.model_configs.items():
                future = executor.submit(
                    self.train_single_model,
                    model_name, config,
                    X_train, y_train,
                    X_val, y_val
                )
                futures.append(future)
            
            for future in as_completed(futures):
                model_name, model, score = future.result()
                if model is not None:
                    trained_models.append((model_name, model))
        
        if len(trained_models) < 2:
            logger.warning("âš ï¸ Ø¹Ø¯Ø¯ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù†Ø§Ø¬Ø­Ø© Ù‚Ù„ÙŠÙ„ØŒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£ÙØ¶Ù„ Ù†Ù…ÙˆØ°Ø¬")
            if trained_models:
                best_model = trained_models[0][1]
                y_pred = best_model.predict(X_test)
                accuracy = accuracy_score(y_test, y_pred)
                return best_model, {'accuracy': accuracy}
            else:
                return None, {'accuracy': 0}
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù…Ø¬Ù…Ø¹
        logger.info("ğŸ”— Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¬Ù…Ø¹...")
        ensemble = VotingClassifier(trained_models, voting='soft')
        ensemble.fit(X_train, y_train)
        
        # ØªÙ‚ÙŠÙŠÙ…
        y_pred = ensemble.predict(X_test)
        accuracy = accuracy_score(y_test, y_pred)
        precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)
        recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)
        f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)
        
        # Ø¯Ù‚Ø© Ø§Ù„ØµÙÙ‚Ø§Øª
        trade_mask = y_pred != 1
        trade_accuracy = accuracy_score(y_test[trade_mask], y_pred[trade_mask]) if trade_mask.sum() > 0 else 0
        
        results = {
            'accuracy': accuracy,
            'trade_accuracy': trade_accuracy,
            'precision': precision,
            'recall': recall,
            'f1': f1,
            'models_count': len(trained_models)
        }
        
        logger.info(f"âœ… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¬Ù…Ø¹: Ø¯Ù‚Ø© {accuracy:.4f} ({len(trained_models)} Ù†Ù…Ø§Ø°Ø¬)")
        
        return ensemble, results
    
    def train_strategy(self, X, y, confidence, feature_names, strategy_name, strategy, symbol, timeframe):
        """ØªØ¯Ø±ÙŠØ¨ Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© ÙˆØ§Ø­Ø¯Ø©"""
        logger.info(f"\nğŸ“Š ØªØ¯Ø±ÙŠØ¨ Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© {strategy_name}...")
        
        # Ù…ÙˆØ§Ø²Ù†Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        X_balanced, y_balanced = self.balance_data(X, y, confidence)
        
        # ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        split1 = int(len(X_balanced) * 0.7)
        split2 = int(len(X_balanced) * 0.85)
        
        X_train = X_balanced[:split1]
        X_val = X_balanced[split1:split2]
        X_test = X_balanced[split2:]
        
        y_train = y_balanced[:split1]
        y_val = y_balanced[split1:split2]
        y_test = y_balanced[split2:]
        
        # Ù…Ø¹Ø§ÙŠØ±Ø©
        scaler = RobustScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        X_val_scaled = scaler.transform(X_val)
        X_test_scaled = scaler.transform(X_test)
        
        # ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬
        model, results = self.train_ensemble_models(
            X_train_scaled, y_train,
            X_val_scaled, y_val,
            X_test_scaled, y_test
        )
        
        if model is None:
            return None
        
        # Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¥Ø°Ø§ ÙƒØ§Ù† Ø¬ÙŠØ¯
        if results['accuracy'] >= 0.85:
            model_dir = Path(f"models/{symbol}_{timeframe}/{strategy_name}")
            model_dir.mkdir(parents=True, exist_ok=True)
            
            model_data = {
                'model': model,
                'scaler': scaler,
                'feature_names': feature_names,
                'strategy': strategy,
                'results': results,
                'training_date': datetime.now(),
                'use_all_models': self.use_all_models
            }
            
            joblib.dump(model_data, model_dir / 'model_advanced.pkl')
            logger.info(f"ğŸ’¾ ØªÙ… Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬")
        
        return results
    
    def train_symbol(self, symbol, timeframe):
        """ØªØ¯Ø±ÙŠØ¨ Ø¹Ù…Ù„Ø© ÙˆØ§Ø­Ø¯Ø©"""
        logger.info(f"\n{'='*80}")
        logger.info(f"ğŸš€ ØªØ¯Ø±ÙŠØ¨ {symbol} {timeframe}")
        logger.info(f"{'='*80}")
        
        # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        df = self.load_data_advanced(symbol, timeframe)
        if df is None:
            return None
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…ÙŠØ²Ø§Øª
        features = self.create_features(df)
        X = features.values
        feature_names = features.columns.tolist()
        
        results = {
            'symbol': symbol,
            'timeframe': timeframe,
            'strategies': {},
            'best_accuracy': 0,
            'best_strategy': None
        }
        
        # ØªØ¯Ø±ÙŠØ¨ ÙƒÙ„ Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ©
        for strategy_name, strategy in self.training_strategies.items():
            try:
                # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ø£Ù‡Ø¯Ø§Ù
                y, confidence = self.create_targets(df, strategy)
                
                # ØªØ¯Ø±ÙŠØ¨
                strategy_results = self.train_strategy(
                    X, y, confidence, feature_names,
                    strategy_name, strategy, symbol, timeframe
                )
                
                if strategy_results:
                    results['strategies'][strategy_name] = strategy_results
                    
                    if strategy_results['accuracy'] > results['best_accuracy']:
                        results['best_accuracy'] = strategy_results['accuracy']
                        results['best_strategy'] = strategy_name
                    
                    logger.info(f"âœ… {strategy_name}: Ø¯Ù‚Ø© {strategy_results['accuracy']:.4f}")
                
                # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø°Ø§ÙƒØ±Ø©
                gc.collect()
                
            except Exception as e:
                logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ {strategy_name}: {e}")
        
        # Ù…Ù„Ø®Øµ
        logger.info(f"\nğŸ“Š Ù…Ù„Ø®Øµ {symbol} {timeframe}:")
        logger.info(f"ğŸ† Ø£ÙØ¶Ù„ Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ©: {results['best_strategy']}")
        logger.info(f"ğŸ¯ Ø£ÙØ¶Ù„ Ø¯Ù‚Ø©: {results['best_accuracy']:.4f}")
        
        return results
    
    def train_all(self):
        """ØªØ¯Ø±ÙŠØ¨ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø¹Ù…Ù„Ø§Øª"""
        logger.info("\n" + "="*100)
        logger.info("ğŸš€ Ø¨Ø¯Ø¡ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ø´Ø§Ù…Ù„")
        if self.use_all_models:
            logger.info("ğŸ“Š Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø¬Ù…ÙŠØ¹ Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ (5 Ù†Ù…Ø§Ø°Ø¬)")
        else:
            logger.info("ğŸ“Š Ø§Ù„ÙˆØ¶Ø¹ Ø§Ù„Ø³Ø±ÙŠØ¹ - LightGBM ÙÙ‚Ø·")
        logger.info("="*100)
        
        if not self.check_database():
            return
        
        # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ø¹Ù…Ù„Ø§Øª
        try:
            conn = sqlite3.connect("data/forex_ml.db")
            query = """
                SELECT symbol, timeframe, COUNT(*) as count
                FROM price_data
                GROUP BY symbol, timeframe
                HAVING count >= ?
                ORDER BY symbol, timeframe
            """
            available = pd.read_sql_query(query, conn, params=(self.min_data_points,))
            conn.close()
            
            logger.info(f"ğŸ“Š Ø¹Ø¯Ø¯ Ø§Ù„Ø£Ø²ÙˆØ§Ø¬: {len(available)}")
            
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø£: {e}")
            return
        
        # Ø§Ù„Ù†ØªØ§Ø¦Ø¬
        excellent = []  # 90%+
        good = []       # 85-90%
        acceptable = [] # 80-85%
        failed = []
        
        # ØªØ¯Ø±ÙŠØ¨ ÙƒÙ„ Ø¹Ù…Ù„Ø©
        for idx, row in available.iterrows():
            symbol = row['symbol']
            timeframe = row['timeframe']
            
            logger.info(f"\nğŸ“ˆ Ù…Ø¹Ø§Ù„Ø¬Ø© {idx+1}/{len(available)}: {symbol} {timeframe}")
            
            try:
                result = self.train_symbol(symbol, timeframe)
                
                if result:
                    acc = result['best_accuracy']
                    
                    model_info = {
                        'symbol': symbol,
                        'timeframe': timeframe,
                        'accuracy': acc,
                        'strategy': result['best_strategy']
                    }
                    
                    if acc >= 0.90:
                        excellent.append(model_info)
                    elif acc >= 0.85:
                        good.append(model_info)
                    elif acc >= 0.80:
                        acceptable.append(model_info)
                    else:
                        failed.append(f"{symbol} {timeframe}")
                
                # Ù…Ø±Ø§Ù‚Ø¨Ø© Ø§Ù„Ø°Ø§ÙƒØ±Ø©
                self.monitor_memory()
                gc.collect()
                
            except Exception as e:
                logger.error(f"âŒ Ø®Ø·Ø£: {e}")
                failed.append(f"{symbol} {timeframe}")
        
        # Ø§Ù„ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ
        self._print_report(excellent, good, acceptable, failed)
    
    def _print_report(self, excellent, good, acceptable, failed):
        """Ø·Ø¨Ø§Ø¹Ø© Ø§Ù„ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ"""
        logger.info("\n" + "="*100)
        logger.info("ğŸ“Š Ø§Ù„ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ")
        logger.info("="*100)
        
        total = len(excellent) + len(good) + len(acceptable) + len(failed)
        
        logger.info(f"\nğŸ“ˆ Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª:")
        logger.info(f"  ğŸŒŸ Ù…Ù…ØªØ§Ø² (90%+): {len(excellent)}")
        logger.info(f"  âœ… Ø¬ÙŠØ¯ (85-90%): {len(good)}")
        logger.info(f"  ğŸ‘ Ù…Ù‚Ø¨ÙˆÙ„ (80-85%): {len(acceptable)}")
        logger.info(f"  âŒ ÙØ´Ù„ (<80%): {len(failed)}")
        
        if total > 0:
            success_rate = (len(excellent) + len(good)) / total * 100
            logger.info(f"\nğŸ¯ Ù…Ø¹Ø¯Ù„ Ø§Ù„Ù†Ø¬Ø§Ø­: {success_rate:.1f}%")
        
        # Ø£ÙØ¶Ù„ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬
        if excellent:
            logger.info(f"\nğŸŒŸ Ø£ÙØ¶Ù„ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬:")
            for m in sorted(excellent, key=lambda x: x['accuracy'], reverse=True)[:5]:
                logger.info(f"  â€¢ {m['symbol']} {m['timeframe']}: {m['accuracy']:.4f}")
        
        # Ø­ÙØ¸ Ø§Ù„ØªÙ‚Ø±ÙŠØ±
        report = {
            'date': datetime.now().isoformat(),
            'use_all_models': self.use_all_models,
            'total': total,
            'excellent': excellent,
            'good': good,
            'acceptable': acceptable,
            'failed': failed
        }
        
        report_path = Path("models/training_report_all_models.json")
        report_path.parent.mkdir(exist_ok=True)
        
        with open(report_path, 'w') as f:
            json.dump(report, f, indent=2, default=str)
        
        logger.info(f"\nğŸ’¾ ØªÙ… Ø­ÙØ¸ Ø§Ù„ØªÙ‚Ø±ÙŠØ±: {report_path}")

def main():
    """Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©"""
    import argparse
    
    parser = argparse.ArgumentParser(description='Advanced Training with All Models')
    parser.add_argument('--quick', action='store_true', help='Quick mode (LightGBM only)')
    parser.add_argument('--symbol', type=str, help='Train specific symbol only')
    parser.add_argument('--timeframe', type=str, default='H1', help='Timeframe')
    args = parser.parse_args()
    
    # ØªØ­Ø¯ÙŠØ¯ ÙˆØ¶Ø¹ Ø§Ù„ØªØ¯Ø±ÙŠØ¨
    use_all_models = not args.quick
    
    trainer = AdvancedCompleteTrainerAllModels(use_all_models=use_all_models)
    
    if args.symbol:
        # ØªØ¯Ø±ÙŠØ¨ Ø¹Ù…Ù„Ø© ÙˆØ§Ø­Ø¯Ø©
        trainer.train_symbol(args.symbol, args.timeframe)
    else:
        # ØªØ¯Ø±ÙŠØ¨ Ø´Ø§Ù…Ù„
        trainer.train_all()

if __name__ == "__main__":
    main()