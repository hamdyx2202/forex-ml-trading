#!/usr/bin/env python3
"""
Retrain Models with Unified Feature Engineering
Ø¥Ø¹Ø§Ø¯Ø© ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù‡Ù†Ø¯Ø³Ø© Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…ÙˆØ­Ø¯Ø©
"""

import pandas as pd
import numpy as np
import joblib
import json
import os
from datetime import datetime
from pathlib import Path
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import RobustScaler
from sklearn.ensemble import RandomForestClassifier, VotingClassifier
from sklearn.metrics import accuracy_score, classification_report
import lightgbm as lgb
import xgboost as xgb
from catboost import CatBoostClassifier
from loguru import logger
import warnings
warnings.filterwarnings('ignore')

# Import unified feature engineer
from feature_engineering_unified import UnifiedFeatureEngineer

class UnifiedModelTrainer:
    """Ù…Ø¯Ø±Ø¨ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…ÙˆØ­Ø¯"""
    
    def __init__(self):
        self.feature_engineer = UnifiedFeatureEngineer()
        self.models = {}
        self.scalers = {}
        self.feature_info = {}
        
    def load_data(self, symbol: str, timeframe: str, db_path: str = 'forex_data.db'):
        """ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù† Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª"""
        import sqlite3
        
        conn = sqlite3.connect(db_path)
        query = """
            SELECT time, open, high, low, close, volume
            FROM forex_data
            WHERE symbol = ? AND timeframe = ?
            ORDER BY time
        """
        
        df = pd.read_sql_query(query, conn, params=(symbol, timeframe))
        conn.close()
        
        if len(df) < 1000:
            logger.warning(f"Not enough data for {symbol} {timeframe}: {len(df)} rows")
            return None
            
        # ØªØ­ÙˆÙŠÙ„ Ø§Ù„ÙˆÙ‚Øª
        df['datetime'] = pd.to_datetime(df['time'], unit='s')
        df.set_index('datetime', inplace=True)
        
        return df
    
    def prepare_data(self, df: pd.DataFrame, lookahead: int = 5, threshold: float = 0.001):
        """ØªØ­Ø¶ÙŠØ± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù„ØªØ¯Ø±ÙŠØ¨"""
        # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù…ÙˆØ­Ø¯
        X, feature_names = self.feature_engineer.create_features(df)
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØªØ³Ù…ÙŠØ§Øª
        future_returns = df['close'].shift(-lookahead) / df['close'] - 1
        y = np.where(future_returns > threshold, 1, 0)  # 1 for UP, 0 for DOWN
        
        # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØµÙÙˆÙ Ø§Ù„Ø£Ø®ÙŠØ±Ø© Ø¨Ø¯ÙˆÙ† ØªØ³Ù…ÙŠØ§Øª
        X = X[:-lookahead]
        y = y[:-lookahead]
        
        # Ø¥Ø²Ø§Ù„Ø© NaN
        mask = ~np.isnan(y)
        X = X[mask]
        y = y[mask]
        
        logger.info(f"Prepared {len(X)} samples with {len(feature_names)} features")
        
        return X, y, feature_names
    
    def create_ensemble_model(self):
        """Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ ensemble"""
        models = []
        
        # LightGBM
        lgb_model = lgb.LGBMClassifier(
            n_estimators=100,
            learning_rate=0.05,
            max_depth=7,
            num_leaves=31,
            random_state=42,
            n_jobs=-1,
            verbosity=-1
        )
        models.append(('lgb', lgb_model))
        
        # XGBoost
        xgb_model = xgb.XGBClassifier(
            n_estimators=100,
            learning_rate=0.05,
            max_depth=7,
            random_state=42,
            n_jobs=-1,
            verbosity=0
        )
        models.append(('xgb', xgb_model))
        
        # CatBoost
        cat_model = CatBoostClassifier(
            iterations=100,
            learning_rate=0.05,
            depth=7,
            random_state=42,
            verbose=False
        )
        models.append(('cat', cat_model))
        
        # Random Forest
        rf_model = RandomForestClassifier(
            n_estimators=100,
            max_depth=10,
            random_state=42,
            n_jobs=-1
        )
        models.append(('rf', rf_model))
        
        # Voting Classifier
        ensemble = VotingClassifier(
            estimators=models,
            voting='soft',
            n_jobs=-1
        )
        
        return ensemble
    
    def train_model(self, symbol: str, timeframe: str, df: pd.DataFrame):
        """ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù…Ø¹ Ø­ÙØ¸ ÙƒÙ„ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª"""
        logger.info(f"Training model for {symbol} {timeframe}")
        
        # ØªØ­Ø¶ÙŠØ± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        X, y, feature_names = self.prepare_data(df)
        
        if len(X) < 100:
            logger.error(f"Not enough samples for {symbol} {timeframe}")
            return None
        
        # ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42, stratify=y
        )
        
        # Ø§Ù„ØªØ·Ø¨ÙŠØ¹
        scaler = RobustScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.transform(X_test)
        
        # Ø¥Ù†Ø´Ø§Ø¡ ÙˆØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
        model = self.create_ensemble_model()
        model.fit(X_train_scaled, y_train)
        
        # Ø§Ù„ØªÙ‚ÙŠÙŠÙ…
        y_pred = model.predict(X_test_scaled)
        accuracy = accuracy_score(y_test, y_pred)
        
        # Ø­Ø³Ø§Ø¨ Ø§Ù„Ø¯Ù‚Ø© Ù„Ù„Ø«Ù‚Ø© Ø§Ù„Ø¹Ø§Ù„ÙŠØ©
        y_proba = model.predict_proba(X_test_scaled)
        high_conf_mask = np.max(y_proba, axis=1) > 0.7
        if high_conf_mask.sum() > 0:
            high_conf_accuracy = accuracy_score(
                y_test[high_conf_mask], 
                y_pred[high_conf_mask]
            )
        else:
            high_conf_accuracy = 0
        
        logger.info(f"Model accuracy: {accuracy:.4f}")
        logger.info(f"High confidence accuracy: {high_conf_accuracy:.4f}")
        
        # Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù…Ø¹ ÙƒÙ„ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª
        model_key = f"{symbol}_{timeframe}"
        
        # Ø­Ø²Ù…Ø© Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ÙƒØ§Ù…Ù„Ø©
        model_package = {
            'model': model,
            'scaler': scaler,
            'feature_names': feature_names,
            'n_features': len(feature_names),
            'feature_version': UnifiedFeatureEngineer.VERSION,
            'feature_config': UnifiedFeatureEngineer.INDICATORS_CONFIG,
            'metrics': {
                'accuracy': float(accuracy),
                'high_confidence_accuracy': float(high_conf_accuracy),
                'high_confidence_trades': int(high_conf_mask.sum()),
                'total_test_samples': len(y_test),
                'class_distribution': {
                    'train': dict(zip(*np.unique(y_train, return_counts=True))),
                    'test': dict(zip(*np.unique(y_test, return_counts=True)))
                }
            },
            'training_metadata': {
                'symbol': symbol,
                'timeframe': timeframe,
                'training_date': datetime.now().isoformat(),
                'n_samples': len(X),
                'model_version': '2.0',
                'unified_features': True
            }
        }
        
        # Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
        os.makedirs('models/unified', exist_ok=True)
        model_filename = f'models/unified/{model_key}_unified_v2.pkl'
        joblib.dump(model_package, model_filename)
        logger.info(f"âœ… Model saved: {model_filename}")
        
        # Ø­ÙØ¸ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙÙŠ JSON Ù„Ù„Ù…Ø±Ø§Ø¬Ø¹Ø©
        config_filename = f'models/unified/{model_key}_config.json'
        config_data = {
            'feature_names': feature_names,
            'n_features': len(feature_names),
            'feature_version': UnifiedFeatureEngineer.VERSION,
            'feature_config': UnifiedFeatureEngineer.INDICATORS_CONFIG,
            'metrics': model_package['metrics'],
            'metadata': model_package['training_metadata']
        }
        
        with open(config_filename, 'w') as f:
            json.dump(config_data, f, indent=2)
        logger.info(f"ğŸ“‹ Config saved: {config_filename}")
        
        return model_package
    
    def train_all_models(self, symbols: list, timeframes: list):
        """ØªØ¯Ø±ÙŠØ¨ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬"""
        logger.info(f"Training {len(symbols) * len(timeframes)} models...")
        
        successful = 0
        failed = 0
        
        for symbol in symbols:
            for timeframe in timeframes:
                try:
                    # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
                    df = self.load_data(symbol, timeframe)
                    if df is None:
                        failed += 1
                        continue
                    
                    # ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
                    model_package = self.train_model(symbol, timeframe, df)
                    if model_package:
                        successful += 1
                        self.models[f"{symbol}_{timeframe}"] = model_package
                    else:
                        failed += 1
                        
                except Exception as e:
                    logger.error(f"Error training {symbol} {timeframe}: {e}")
                    failed += 1
        
        logger.info(f"âœ… Training completed: {successful} successful, {failed} failed")
        
        # Ø­ÙØ¸ Ù…Ù„Ø®Øµ Ø§Ù„ØªØ¯Ø±ÙŠØ¨
        summary = {
            'training_date': datetime.now().isoformat(),
            'total_models': successful + failed,
            'successful': successful,
            'failed': failed,
            'feature_version': UnifiedFeatureEngineer.VERSION,
            'models': {}
        }
        
        for key, package in self.models.items():
            summary['models'][key] = {
                'accuracy': package['metrics']['accuracy'],
                'high_conf_accuracy': package['metrics']['high_confidence_accuracy'],
                'n_features': package['n_features']
            }
        
        with open('models/unified/training_summary.json', 'w') as f:
            json.dump(summary, f, indent=2)
        
        return summary

# Ù„Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù…Ø¨Ø§Ø´Ø±
if __name__ == "__main__":
    # Ø§Ù„Ø£Ø²ÙˆØ§Ø¬ ÙˆØ§Ù„Ø£Ø·Ø± Ø§Ù„Ø²Ù…Ù†ÙŠØ©
    symbols = ['EURUSDm', 'GBPUSDm', 'USDJPYm', 'USDCHFm', 
               'AUDUSDm', 'USDCADm', 'NZDUSDm', 'XAUUSDm',
               'EURJPYm', 'GBPJPYm']  # Ø£Ø¶Ù EURJPYm Ùˆ GBPJPYm
    
    timeframes = ['PERIOD_M5', 'PERIOD_M15', 'PERIOD_H1', 'PERIOD_H4']
    
    # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…Ø¯Ø±Ø¨
    trainer = UnifiedModelTrainer()
    
    # ØªØ¯Ø±ÙŠØ¨ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬
    summary = trainer.train_all_models(symbols, timeframes)
    
    print("\n" + "="*60)
    print("ğŸ¯ Training Summary")
    print("="*60)
    print(f"âœ… Successful: {summary['successful']}")
    print(f"âŒ Failed: {summary['failed']}")
    print(f"ğŸ“Š Feature Version: {summary['feature_version']}")
    print("\nModel Performance:")
    
    for model_key, metrics in summary['models'].items():
        print(f"\n{model_key}:")
        print(f"  â€¢ Accuracy: {metrics['accuracy']:.2%}")
        print(f"  â€¢ High Conf Accuracy: {metrics['high_conf_accuracy']:.2%}")
        print(f"  â€¢ Features: {metrics['n_features']}")