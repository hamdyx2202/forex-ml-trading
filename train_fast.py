#!/usr/bin/env python3
"""
Ù†Ø¸Ø§Ù… Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ø³Ø±ÙŠØ¹ - Ù†Ø³Ø®Ø© Ù…Ø­Ø³Ù†Ø© Ù„Ù„Ø£Ø¯Ø§Ø¡
ÙŠØ±ÙƒØ² Ø¹Ù„Ù‰ Ø§Ù„Ø³Ø±Ø¹Ø© Ù…Ø¹ Ø§Ù„Ø­ÙØ§Ø¸ Ø¹Ù„Ù‰ Ø§Ù„Ø¬ÙˆØ¯Ø©
"""

import warnings
warnings.filterwarnings('ignore')

import pandas as pd
import numpy as np
from datetime import datetime
import sqlite3
from pathlib import Path
import joblib
import json
import logging
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report
import xgboost as xgb
import lightgbm as lgb
from concurrent.futures import ProcessPoolExecutor
import multiprocessing
import time
import ta

# Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø³Ø¬Ù„Ø§Øª
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s | %(levelname)-8s | %(message)s',
    handlers=[
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class FastTrainer:
    def __init__(self):
        self.models = {}
        self.scaler = StandardScaler()
        self.results = []
        
        # Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ù…Ø­Ø³Ù†Ø© Ù„Ù„Ø³Ø±Ø¹Ø©
        self.config = {
            'min_data_points': 5000,  # Ø£Ù‚Ù„ Ù…Ù† Ø§Ù„Ø³Ø§Ø¨Ù‚
            'max_features': 50,  # Ù…ÙŠØ²Ø§Øª Ø£Ù‚Ù„ ÙˆÙ„ÙƒÙ† Ø£ÙƒØ«Ø± ÙØ¹Ø§Ù„ÙŠØ©
            'test_size': 0.2,
            'n_jobs': multiprocessing.cpu_count() - 1,
            'quick_mode': True,  # ÙˆØ¶Ø¹ Ø³Ø±ÙŠØ¹
        }
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…Ø¬Ù„Ø¯Ø§Øª
        Path("models").mkdir(exist_ok=True)
        Path("results").mkdir(exist_ok=True)
        Path("logs").mkdir(exist_ok=True)
    
    def get_available_data(self):
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªØ§Ø­Ø©"""
        try:
            conn = sqlite3.connect('data/forex_ml.db')
            query = """
                SELECT symbol, timeframe, COUNT(*) as count
                FROM price_data
                GROUP BY symbol, timeframe
                HAVING count >= ?
                ORDER BY count DESC
            """
            df = pd.read_sql_query(query, conn, params=(self.config['min_data_points'],))
            conn.close()
            
            logger.info(f"âœ… ÙˆØ¬Ø¯Øª {len(df)} Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª")
            return df.to_records(index=False).tolist()
            
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: {e}")
            return []
    
    def load_data(self, symbol, timeframe, limit=10000):
        """ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ø´ÙƒÙ„ Ø³Ø±ÙŠØ¹"""
        try:
            conn = sqlite3.connect('data/forex_ml.db')
            query = """
                SELECT * FROM price_data 
                WHERE symbol = ? AND timeframe = ?
                ORDER BY timestamp DESC
                LIMIT ?
            """
            df = pd.read_sql_query(query, conn, params=(symbol, timeframe, limit))
            conn.close()
            
            if len(df) < self.config['min_data_points']:
                return None
            
            # ØªØ±ØªÙŠØ¨ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
            df = df.sort_values('timestamp')
            df['timestamp'] = pd.to_datetime(df['timestamp'])
            df = df.set_index('timestamp')
            
            logger.info(f"âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ {len(df)} Ø³Ø¬Ù„ Ù„Ù€ {symbol} {timeframe}")
            return df
            
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: {e}")
            return None
    
    def create_fast_features(self, df):
        """Ø¥Ù†Ø´Ø§Ø¡ Ù…ÙŠØ²Ø§Øª Ø³Ø±ÙŠØ¹Ø© ÙˆÙØ¹Ø§Ù„Ø©"""
        features = pd.DataFrame(index=df.index)
        
        # 1. Ù…ÙŠØ²Ø§Øª Ø§Ù„Ø³Ø¹Ø± Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© (10 Ù…ÙŠØ²Ø§Øª)
        features['returns'] = df['close'].pct_change()
        features['log_returns'] = np.log(df['close'] / df['close'].shift(1))
        features['price_range'] = (df['high'] - df['low']) / df['close']
        features['body_size'] = abs(df['close'] - df['open']) / df['close']
        features['upper_shadow'] = (df['high'] - df[['close', 'open']].max(axis=1)) / df['close']
        features['lower_shadow'] = (df[['close', 'open']].min(axis=1) - df['low']) / df['close']
        features['volume_ratio'] = df['volume'] / df['volume'].rolling(20).mean()
        
        # 2. Ø§Ù„Ù…ØªÙˆØ³Ø·Ø§Øª Ø§Ù„Ù…ØªØ­Ø±ÙƒØ© Ø§Ù„Ø³Ø±ÙŠØ¹Ø© (15 Ù…ÙŠØ²Ø©)
        for period in [5, 10, 20, 50, 100]:
            sma = df['close'].rolling(period).mean()
            features[f'sma_{period}_ratio'] = df['close'] / sma
            features[f'sma_{period}_slope'] = sma.pct_change(5)
            
            if period <= 20:
                ema = df['close'].ewm(span=period).mean()
                features[f'ema_{period}_ratio'] = df['close'] / ema
        
        # 3. Ù…Ø¤Ø´Ø±Ø§Øª ÙÙ†ÙŠØ© Ø£Ø³Ø§Ø³ÙŠØ© (20 Ù…ÙŠØ²Ø©)
        # RSI
        features['rsi_14'] = ta.momentum.RSIIndicator(df['close'], window=14).rsi()
        features['rsi_7'] = ta.momentum.RSIIndicator(df['close'], window=7).rsi()
        
        # MACD
        macd = ta.trend.MACD(df['close'])
        features['macd'] = macd.macd()
        features['macd_signal'] = macd.macd_signal()
        features['macd_diff'] = macd.macd_diff()
        
        # Bollinger Bands
        bb = ta.volatility.BollingerBands(df['close'])
        features['bb_upper'] = (bb.bollinger_hband() - df['close']) / df['close']
        features['bb_lower'] = (df['close'] - bb.bollinger_lband()) / df['close']
        features['bb_width'] = bb.bollinger_wband() / df['close']
        
        # ATR
        features['atr'] = ta.volatility.AverageTrueRange(df['high'], df['low'], df['close']).average_true_range() / df['close']
        
        # ADX
        adx = ta.trend.ADXIndicator(df['high'], df['low'], df['close'])
        features['adx'] = adx.adx()
        features['adx_pos'] = adx.adx_pos()
        features['adx_neg'] = adx.adx_neg()
        
        # 4. Ù…ÙŠØ²Ø§Øª Ø¥Ø­ØµØ§Ø¦ÙŠØ© (5 Ù…ÙŠØ²Ø§Øª)
        features['volatility_20'] = df['returns'].rolling(20).std()
        features['skew_20'] = df['returns'].rolling(20).skew()
        features['kurt_20'] = df['returns'].rolling(20).kurt()
        features['max_20'] = df['high'].rolling(20).max() / df['close']
        features['min_20'] = df['low'].rolling(20).min() / df['close']
        
        # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ù†Ø§Ù‚ØµØ©
        features = features.fillna(method='ffill').fillna(0)
        
        # Ø§Ø®ØªÙŠØ§Ø± Ø£ÙØ¶Ù„ Ø§Ù„Ù…ÙŠØ²Ø§Øª ÙÙ‚Ø·
        if len(features.columns) > self.config['max_features']:
            # Ø­Ø³Ø§Ø¨ Ø§Ù„Ø§Ø±ØªØ¨Ø§Ø· Ù…Ø¹ Ø§Ù„Ø¹Ø§Ø¦Ø¯Ø§Øª Ø§Ù„Ù…Ø³ØªÙ‚Ø¨Ù„ÙŠØ©
            future_returns = df['close'].shift(-5).pct_change(5)
            correlations = features.corrwith(future_returns).abs()
            top_features = correlations.nlargest(self.config['max_features']).index
            features = features[top_features]
        
        logger.info(f"âœ… ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ {len(features.columns)} Ù…ÙŠØ²Ø©")
        return features
    
    def create_labels(self, df, features):
        """Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØªØ³Ù…ÙŠØ§Øª Ù„Ù„ØªØ¯Ø±ÙŠØ¨"""
        # Ù‡Ø¯Ù Ø¨Ø³ÙŠØ·: Ù‡Ù„ Ø³ÙŠØ±ØªÙØ¹ Ø§Ù„Ø³Ø¹Ø± 0.5% Ø®Ù„Ø§Ù„ 5 Ø´Ù…ÙˆØ¹ØŸ
        future_returns = df['close'].shift(-5) / df['close'] - 1
        labels = (future_returns > 0.005).astype(int)
        
        # Ù…Ø­Ø§Ø°Ø§Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        mask = ~(features.isna().any(axis=1) | labels.isna())
        
        return features[mask], labels[mask]
    
    def train_single_model(self, X_train, X_test, y_train, y_test, model_type='rf'):
        """ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬ ÙˆØ§Ø­Ø¯ Ø¨Ø³Ø±Ø¹Ø©"""
        if model_type == 'rf':
            model = RandomForestClassifier(
                n_estimators=50,  # Ø£Ù‚Ù„ Ù…Ù† Ø§Ù„Ø³Ø§Ø¨Ù‚
                max_depth=10,
                min_samples_split=20,
                n_jobs=self.config['n_jobs'],
                random_state=42
            )
        elif model_type == 'xgb':
            model = xgb.XGBClassifier(
                n_estimators=50,
                max_depth=6,
                learning_rate=0.1,
                n_jobs=self.config['n_jobs'],
                random_state=42,
                use_label_encoder=False,
                eval_metric='logloss'
            )
        elif model_type == 'lgb':
            model = lgb.LGBMClassifier(
                n_estimators=50,
                max_depth=6,
                learning_rate=0.1,
                n_jobs=self.config['n_jobs'],
                random_state=42,
                verbose=-1
            )
        
        # Ø§Ù„ØªØ¯Ø±ÙŠØ¨
        model.fit(X_train, y_train)
        
        # Ø§Ù„ØªÙ‚ÙŠÙŠÙ…
        y_pred = model.predict(X_test)
        accuracy = accuracy_score(y_test, y_pred)
        
        return model, accuracy
    
    def train_models(self, features, labels):
        """ØªØ¯Ø±ÙŠØ¨ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬"""
        # ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        X_train, X_test, y_train, y_test = train_test_split(
            features, labels, 
            test_size=self.config['test_size'], 
            random_state=42,
            stratify=labels if len(np.unique(labels)) > 1 else None
        )
        
        # ØªØ­Ø¬ÙŠÙ… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        X_train_scaled = self.scaler.fit_transform(X_train)
        X_test_scaled = self.scaler.transform(X_test)
        
        results = {}
        
        # ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬
        if self.config['quick_mode']:
            # ÙÙŠ Ø§Ù„ÙˆØ¶Ø¹ Ø§Ù„Ø³Ø±ÙŠØ¹ØŒ Ù†Ø¯Ø±Ø¨ Ù†Ù…ÙˆØ°Ø¬ ÙˆØ§Ø­Ø¯ ÙÙ‚Ø·
            logger.info("ğŸš€ ÙˆØ¶Ø¹ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ø³Ø±ÙŠØ¹ - Random Forest ÙÙ‚Ø·")
            model, accuracy = self.train_single_model(
                X_train_scaled, X_test_scaled, y_train, y_test, 'rf'
            )
            results['rf'] = {'model': model, 'accuracy': accuracy}
        else:
            # ØªØ¯Ø±ÙŠØ¨ 3 Ù†Ù…Ø§Ø°Ø¬
            for model_type in ['rf', 'xgb', 'lgb']:
                logger.info(f"ğŸ¤– ØªØ¯Ø±ÙŠØ¨ {model_type}...")
                model, accuracy = self.train_single_model(
                    X_train_scaled, X_test_scaled, y_train, y_test, model_type
                )
                results[model_type] = {'model': model, 'accuracy': accuracy}
        
        # Ø§Ø®ØªÙŠØ§Ø± Ø£ÙØ¶Ù„ Ù†Ù…ÙˆØ°Ø¬
        best_model_type = max(results, key=lambda x: results[x]['accuracy'])
        best_accuracy = results[best_model_type]['accuracy']
        
        logger.info(f"âœ… Ø£ÙØ¶Ù„ Ù†Ù…ÙˆØ°Ø¬: {best_model_type} Ø¨Ø¯Ù‚Ø© {best_accuracy:.4f}")
        
        return results[best_model_type]['model'], best_accuracy, results
    
    def save_model(self, model, symbol, timeframe, accuracy):
        """Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙˆØ§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
        model_path = f"models/{symbol}_{timeframe}_fast_{timestamp}.pkl"
        joblib.dump({
            'model': model,
            'scaler': self.scaler,
            'feature_names': self.feature_names,
            'accuracy': accuracy,
            'timestamp': timestamp
        }, model_path)
        
        # Ø­ÙØ¸ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª
        info = {
            'symbol': symbol,
            'timeframe': timeframe,
            'accuracy': accuracy,
            'model_path': model_path,
            'timestamp': timestamp,
            'features_count': len(self.feature_names)
        }
        
        self.results.append(info)
        logger.info(f"ğŸ’¾ ØªÙ… Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬: {model_path}")
        
        return model_path
    
    def train_pair(self, symbol, timeframe):
        """ØªØ¯Ø±ÙŠØ¨ Ø²ÙˆØ¬ ÙˆØ§Ø­Ø¯"""
        logger.info(f"\n{'='*60}")
        logger.info(f"ğŸ¯ ØªØ¯Ø±ÙŠØ¨ {symbol} {timeframe}")
        logger.info(f"{'='*60}")
        
        try:
            # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
            df = self.load_data(symbol, timeframe)
            if df is None:
                return None
            
            # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…ÙŠØ²Ø§Øª
            features = self.create_fast_features(df)
            self.feature_names = features.columns.tolist()
            
            # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØªØ³Ù…ÙŠØ§Øª
            X, y = self.create_labels(df, features)
            
            if len(X) < 1000:
                logger.warning(f"âš ï¸ Ø¨ÙŠØ§Ù†Ø§Øª ØºÙŠØ± ÙƒØ§ÙÙŠØ© Ø¨Ø¹Ø¯ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©: {len(X)}")
                return None
            
            # Ø§Ù„ØªØ¯Ø±ÙŠØ¨
            logger.info("ğŸ¤– Ø¨Ø¯Ø¡ Ø§Ù„ØªØ¯Ø±ÙŠØ¨...")
            start_time = time.time()
            
            model, accuracy, all_results = self.train_models(X, y)
            
            training_time = time.time() - start_time
            logger.info(f"â±ï¸ ÙˆÙ‚Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨: {training_time:.1f} Ø«Ø§Ù†ÙŠØ©")
            
            # Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„Ø¯Ù‚Ø© Ø¬ÙŠØ¯Ø©
            if accuracy >= 0.52:  # Ø¹ØªØ¨Ø© Ø£Ù‚Ù„ Ù„Ù„Ø³Ø±Ø¹Ø©
                model_path = self.save_model(model, symbol, timeframe, accuracy)
                return {
                    'symbol': symbol,
                    'timeframe': timeframe,
                    'accuracy': accuracy,
                    'model_path': model_path,
                    'training_time': training_time
                }
            else:
                logger.warning(f"âš ï¸ Ø¯Ù‚Ø© Ù…Ù†Ø®ÙØ¶Ø©: {accuracy:.4f}")
                return None
                
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ØªØ¯Ø±ÙŠØ¨: {e}")
            return None
    
    def train_all(self, max_pairs=None):
        """ØªØ¯Ø±ÙŠØ¨ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø£Ø²ÙˆØ§Ø¬"""
        logger.info("\n" + "="*80)
        logger.info("ğŸš€ Ø¨Ø¯Ø¡ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ø³Ø±ÙŠØ¹")
        logger.info("="*80)
        
        # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªØ§Ø­Ø©
        available_data = self.get_available_data()
        
        if max_pairs:
            available_data = available_data[:max_pairs]
        
        logger.info(f"ğŸ“Š Ø³ÙŠØªÙ… ØªØ¯Ø±ÙŠØ¨ {len(available_data)} Ø²ÙˆØ¬")
        
        start_time = time.time()
        successful = 0
        
        # Ø§Ù„ØªØ¯Ø±ÙŠØ¨
        for i, (symbol, timeframe, count) in enumerate(available_data, 1):
            logger.info(f"\nğŸ“ˆ [{i}/{len(available_data)}] {symbol} {timeframe}")
            
            result = self.train_pair(symbol, timeframe)
            
            if result:
                successful += 1
                logger.info(f"âœ… Ù†Ø¬Ø­ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ - Ø¯Ù‚Ø©: {result['accuracy']:.4f}")
            else:
                logger.warning(f"âŒ ÙØ´Ù„ Ø§Ù„ØªØ¯Ø±ÙŠØ¨")
        
        # Ø­ÙØ¸ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
        self.save_results()
        
        total_time = time.time() - start_time
        logger.info(f"\n{'='*80}")
        logger.info(f"âœ… Ø§ÙƒØªÙ…Ù„ Ø§Ù„ØªØ¯Ø±ÙŠØ¨!")
        logger.info(f"ğŸ“Š Ù†Ø¬Ø­: {successful}/{len(available_data)}")
        logger.info(f"â±ï¸ Ø§Ù„ÙˆÙ‚Øª Ø§Ù„Ø¥Ø¬Ù…Ø§Ù„ÙŠ: {total_time/60:.1f} Ø¯Ù‚ÙŠÙ‚Ø©")
        logger.info(f"âš¡ Ù…ØªÙˆØ³Ø· Ø§Ù„ÙˆÙ‚Øª Ù„ÙƒÙ„ Ø²ÙˆØ¬: {total_time/len(available_data):.1f} Ø«Ø§Ù†ÙŠØ©")
        logger.info(f"{'='*80}")
    
    def save_results(self):
        """Ø­ÙØ¸ Ù†ØªØ§Ø¦Ø¬ Ø§Ù„ØªØ¯Ø±ÙŠØ¨"""
        if not self.results:
            return
        
        # Ø­ÙØ¸ ÙƒÙ€ JSON
        results_path = f"results/training_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(results_path, 'w') as f:
            json.dump(self.results, f, indent=2)
        
        # Ø­ÙØ¸ ÙƒÙ€ CSV
        df = pd.DataFrame(self.results)
        csv_path = results_path.replace('.json', '.csv')
        df.to_csv(csv_path, index=False)
        
        logger.info(f"ğŸ’¾ ØªÙ… Ø­ÙØ¸ Ø§Ù„Ù†ØªØ§Ø¦Ø¬: {results_path}")

def main():
    """Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ"""
    import argparse
    
    parser = argparse.ArgumentParser(description='Ù†Ø¸Ø§Ù… Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ø³Ø±ÙŠØ¹')
    parser.add_argument('--max-pairs', type=int, help='Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ù‚ØµÙ‰ Ù„Ù„Ø£Ø²ÙˆØ§Ø¬')
    parser.add_argument('--full-mode', action='store_true', help='Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„ÙˆØ¶Ø¹ Ø§Ù„ÙƒØ§Ù…Ù„ (3 Ù†Ù…Ø§Ø°Ø¬)')
    
    args = parser.parse_args()
    
    # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…Ø¯Ø±Ø¨
    trainer = FastTrainer()
    
    if args.full_mode:
        trainer.config['quick_mode'] = False
        logger.info("ğŸ”§ ØªÙ… ØªÙØ¹ÙŠÙ„ Ø§Ù„ÙˆØ¶Ø¹ Ø§Ù„ÙƒØ§Ù…Ù„ (3 Ù†Ù…Ø§Ø°Ø¬)")
    
    # Ø¨Ø¯Ø¡ Ø§Ù„ØªØ¯Ø±ÙŠØ¨
    trainer.train_all(max_pairs=args.max_pairs)

if __name__ == "__main__":
    main()