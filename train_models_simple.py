#!/usr/bin/env python3
"""
Simple Model Training - Ù†Ø¸Ø§Ù… ØªØ¯Ø±ÙŠØ¨ Ù…Ø¨Ø³Ø· ÙŠØ¹Ù…Ù„ Ù…Ø¹ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…ØªØ§Ø­Ø©
"""

import os
import sys
import json
import sqlite3
import numpy as np
import pandas as pd
from datetime import datetime
from pathlib import Path
import joblib
import warnings
warnings.filterwarnings('ignore')

from loguru import logger
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.ensemble import RandomForestClassifier, VotingClassifier
import lightgbm as lgb
import xgboost as xgb

# Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ³Ø¬ÙŠÙ„
logger.remove()
logger.add(sys.stdout, format="<green>{time:YYYY-MM-DD HH:mm:ss}</green> | <level>{level: <8}</level> | <level>{message}</level>")

class SimpleModelTrainer:
    """Ù†Ø¸Ø§Ù… ØªØ¯Ø±ÙŠØ¨ Ù…Ø¨Ø³Ø· Ù„Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø¹Ù…Ù„Ø§Øª"""
    
    def __init__(self):
        self.min_data_points = 1000
        self.test_size = 0.2
        self.random_state = 42
        
    def get_all_symbols_from_db(self):
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø¹Ù…Ù„Ø§Øª Ù…Ù† Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª"""
        try:
            conn = sqlite3.connect("data/forex_data.db")
            query = """
                SELECT DISTINCT symbol, timeframe, COUNT(*) as count
                FROM price_data
                GROUP BY symbol, timeframe
                HAVING count >= ?
                ORDER BY symbol, timeframe
            """
            df = pd.read_sql_query(query, conn, params=(self.min_data_points,))
            conn.close()
            
            logger.info(f"âœ… ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ {len(df)} Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª")
            return df
            
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: {e}")
            return pd.DataFrame()
    
    def load_data(self, symbol, timeframe, limit=50000):
        """ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù† Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª"""
        try:
            conn = sqlite3.connect("data/forex_data.db")
            query = """
                SELECT * FROM price_data 
                WHERE symbol = ? AND timeframe = ?
                ORDER BY time DESC
                LIMIT ?
            """
            df = pd.read_sql_query(query, conn, params=(symbol, timeframe, limit))
            conn.close()
            
            if df.empty:
                return None
                
            df = df.sort_values('time').reset_index(drop=True)
            df['time'] = pd.to_datetime(df['time'], unit='s')
            
            logger.info(f"âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ {len(df)} Ø³Ø¬Ù„ Ù„Ù€ {symbol} {timeframe}")
            return df
            
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: {e}")
            return None
    
    def create_features(self, df):
        """Ø¥Ù†Ø´Ø§Ø¡ Ù…ÙŠØ²Ø§Øª Ø¨Ø³ÙŠØ·Ø© ÙˆÙØ¹Ø§Ù„Ø©"""
        features = pd.DataFrame()
        
        # Ù…ÙŠØ²Ø§Øª Ø§Ù„Ø³Ø¹Ø± Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
        features['returns'] = df['close'].pct_change()
        features['log_returns'] = np.log(df['close'] / df['close'].shift(1))
        
        # Ø§Ù„Ù…ØªÙˆØ³Ø·Ø§Øª Ø§Ù„Ù…ØªØ­Ø±ÙƒØ©
        for period in [5, 10, 20, 50, 100]:
            sma = df['close'].rolling(period).mean()
            features[f'sma_{period}'] = df['close'] / sma - 1
            features[f'sma_{period}_slope'] = sma.pct_change(5)
        
        # RSI
        for period in [7, 14, 21]:
            features[f'rsi_{period}'] = self.calculate_rsi(df['close'], period)
        
        # ATR
        for period in [7, 14, 21]:
            features[f'atr_{period}'] = self.calculate_atr(df, period)
        
        # Bollinger Bands
        for period in [10, 20]:
            sma = df['close'].rolling(period).mean()
            std = df['close'].rolling(period).std()
            features[f'bb_upper_{period}'] = (df['close'] - (sma + 2*std)) / df['close']
            features[f'bb_lower_{period}'] = ((sma - 2*std) - df['close']) / df['close']
            features[f'bb_width_{period}'] = (4 * std) / sma
        
        # MACD
        exp1 = df['close'].ewm(span=12, adjust=False).mean()
        exp2 = df['close'].ewm(span=26, adjust=False).mean()
        macd = exp1 - exp2
        signal = macd.ewm(span=9, adjust=False).mean()
        features['macd'] = macd / df['close']
        features['macd_signal'] = signal / df['close']
        features['macd_hist'] = (macd - signal) / df['close']
        
        # Volume
        features['volume_sma_ratio'] = df['volume'] / df['volume'].rolling(20).mean()
        features['volume_change'] = df['volume'].pct_change()
        
        # Ø§Ù„ØªÙ‚Ù„Ø¨
        for period in [5, 10, 20]:
            features[f'volatility_{period}'] = df['close'].pct_change().rolling(period).std()
        
        # Ù…ÙŠØ²Ø§Øª Ø§Ù„ÙˆÙ‚Øª
        features['hour'] = df['time'].dt.hour
        features['day_of_week'] = df['time'].dt.dayofweek
        features['day_of_month'] = df['time'].dt.day
        features['month'] = df['time'].dt.month
        
        # Ø¬Ù„Ø³Ø§Øª Ø§Ù„ØªØ¯Ø§ÙˆÙ„
        features['london_session'] = ((features['hour'] >= 8) & (features['hour'] <= 16)).astype(int)
        features['ny_session'] = ((features['hour'] >= 13) & (features['hour'] <= 22)).astype(int)
        features['asian_session'] = ((features['hour'] >= 0) & (features['hour'] <= 8)).astype(int)
        
        # Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ø´Ù…ÙˆØ¹ Ø§Ù„Ø¨Ø³ÙŠØ·Ø©
        body = abs(df['close'] - df['open'])
        range_hl = df['high'] - df['low']
        features['body_ratio'] = body / (range_hl + 0.0001)
        features['upper_shadow'] = (df['high'] - df[['close', 'open']].max(axis=1)) / (range_hl + 0.0001)
        features['lower_shadow'] = (df[['close', 'open']].min(axis=1) - df['low']) / (range_hl + 0.0001)
        
        # Doji
        features['is_doji'] = (features['body_ratio'] < 0.1).astype(int)
        
        # Pin Bar
        features['is_pin_bar'] = (
            ((features['upper_shadow'] > 2 * features['body_ratio']) | 
             (features['lower_shadow'] > 2 * features['body_ratio']))
        ).astype(int)
        
        return features
    
    def calculate_rsi(self, prices, period=14):
        """Ø­Ø³Ø§Ø¨ RSI"""
        delta = prices.diff()
        gain = (delta.where(delta > 0, 0)).rolling(window=period).mean()
        loss = (-delta.where(delta < 0, 0)).rolling(window=period).mean()
        rs = gain / (loss + 0.0001)
        rsi = 100 - (100 / (1 + rs))
        return rsi
    
    def calculate_atr(self, df, period=14):
        """Ø­Ø³Ø§Ø¨ ATR"""
        high_low = df['high'] - df['low']
        high_close = abs(df['high'] - df['close'].shift(1))
        low_close = abs(df['low'] - df['close'].shift(1))
        ranges = pd.concat([high_low, high_close, low_close], axis=1)
        true_range = ranges.max(axis=1)
        atr = true_range.rolling(period).mean()
        return atr / df['close']
    
    def create_targets(self, df, lookahead_minutes=30, min_change_pct=0.1):
        """Ø¥Ù†Ø´Ø§Ø¡ Ø£Ù‡Ø¯Ø§Ù Ø§Ù„ØªØµÙ†ÙŠÙ"""
        # Ø­Ø³Ø§Ø¨ Ø§Ù„ØªØºÙŠØ± Ø§Ù„Ù…Ø³ØªÙ‚Ø¨Ù„ÙŠ
        future_returns = df['close'].shift(-lookahead_minutes) / df['close'] - 1
        
        # ØªØµÙ†ÙŠÙ Ø«Ù„Ø§Ø«ÙŠ
        targets = pd.Series(index=df.index, dtype=int)
        targets[future_returns > min_change_pct/100] = 2  # ØµØ¹ÙˆØ¯
        targets[future_returns < -min_change_pct/100] = 0  # Ù‡Ø¨ÙˆØ·
        targets[(future_returns >= -min_change_pct/100) & (future_returns <= min_change_pct/100)] = 1  # Ù…Ø­Ø§ÙŠØ¯
        
        return targets
    
    def train_model(self, X_train, y_train, X_test, y_test):
        """ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬ Ù…Ø¬Ù…Ø¹"""
        # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬
        models = []
        
        # LightGBM
        lgb_params = {
            'objective': 'multiclass',
            'num_class': 3,
            'metric': 'multi_logloss',
            'num_leaves': 31,
            'learning_rate': 0.05,
            'feature_fraction': 0.8,
            'bagging_fraction': 0.8,
            'bagging_freq': 5,
            'verbose': -1,
            'n_estimators': 300
        }
        lgb_model = lgb.LGBMClassifier(**lgb_params)
        models.append(('lightgbm', lgb_model))
        
        # XGBoost
        xgb_params = {
            'objective': 'multi:softprob',
            'num_class': 3,
            'max_depth': 6,
            'learning_rate': 0.05,
            'subsample': 0.8,
            'colsample_bytree': 0.8,
            'eval_metric': 'mlogloss',
            'use_label_encoder': False,
            'n_estimators': 300
        }
        xgb_model = xgb.XGBClassifier(**xgb_params)
        models.append(('xgboost', xgb_model))
        
        # Random Forest
        rf_model = RandomForestClassifier(
            n_estimators=300,
            max_depth=10,
            min_samples_split=5,
            min_samples_leaf=2,
            random_state=self.random_state,
            n_jobs=-1
        )
        models.append(('random_forest', rf_model))
        
        # Ù†Ù…ÙˆØ°Ø¬ Ù…Ø¬Ù…Ø¹
        ensemble = VotingClassifier(models, voting='soft', n_jobs=-1)
        
        logger.info("ğŸ¯ Ø¨Ø¯Ø¡ Ø§Ù„ØªØ¯Ø±ÙŠØ¨...")
        ensemble.fit(X_train, y_train)
        
        # ØªÙ‚ÙŠÙŠÙ…
        train_score = ensemble.score(X_train, y_train)
        test_score = ensemble.score(X_test, y_test)
        
        y_pred = ensemble.predict(X_test)
        precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)
        recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)
        f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)
        
        scores = {
            'train_accuracy': train_score,
            'test_accuracy': test_score,
            'precision': precision,
            'recall': recall,
            'f1': f1
        }
        
        logger.info(f"âœ… Ø¯Ù‚Ø© Ø§Ù„ØªØ¯Ø±ÙŠØ¨: {train_score:.4f}")
        logger.info(f"âœ… Ø¯Ù‚Ø© Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±: {test_score:.4f}")
        logger.info(f"ğŸ“Š Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}")
        
        return ensemble, scores
    
    def train_symbol(self, symbol, timeframe):
        """ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬ Ù„Ø¹Ù…Ù„Ø© ÙˆØ§Ø­Ø¯Ø©"""
        logger.info(f"\n{'='*60}")
        logger.info(f"ğŸš€ ØªØ¯Ø±ÙŠØ¨ {symbol} {timeframe}")
        logger.info(f"{'='*60}")
        
        # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        df = self.load_data(symbol, timeframe)
        if df is None or len(df) < self.min_data_points:
            logger.warning(f"âš ï¸ Ø¨ÙŠØ§Ù†Ø§Øª ØºÙŠØ± ÙƒØ§ÙÙŠØ©")
            return None
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…ÙŠØ²Ø§Øª
        features = self.create_features(df)
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ø£Ù‡Ø¯Ø§Ù
        targets = self.create_targets(df)
        
        # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        features = features.fillna(0)
        features = features.replace([np.inf, -np.inf], 0)
        
        # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØµÙÙˆÙ Ø§Ù„Ø£Ø®ÙŠØ±Ø© Ø¨Ø¯ÙˆÙ† Ø£Ù‡Ø¯Ø§Ù
        valid_idx = ~targets.isna()
        features = features[valid_idx]
        targets = targets[valid_idx]
        
        if len(features) < self.min_data_points:
            logger.warning(f"âš ï¸ Ø¨ÙŠØ§Ù†Ø§Øª ØºÙŠØ± ÙƒØ§ÙÙŠØ© Ø¨Ø¹Ø¯ Ø§Ù„ØªÙ†Ø¸ÙŠÙ")
            return None
        
        # ØªØ­Ø¶ÙŠØ± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        X = features.values
        y = targets.values
        
        # ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=self.test_size, shuffle=False
        )
        
        # Ù…Ø¹Ø§ÙŠØ±Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.transform(X_test)
        
        # ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
        model, scores = self.train_model(X_train_scaled, y_train, X_test_scaled, y_test)
        
        # Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
        model_dir = Path(f"models/{symbol}_{timeframe}")
        model_dir.mkdir(parents=True, exist_ok=True)
        
        model_data = {
            'model': model,
            'scaler': scaler,
            'feature_names': list(features.columns),
            'scores': scores,
            'training_date': datetime.now(),
            'symbol': symbol,
            'timeframe': timeframe,
            'samples': len(features)
        }
        
        joblib.dump(model_data, model_dir / 'model_simple.pkl')
        
        # Ø­ÙØ¸ ØªÙ‚Ø±ÙŠØ±
        with open(model_dir / 'training_report_simple.json', 'w') as f:
            report = {
                'symbol': symbol,
                'timeframe': timeframe,
                'training_date': str(datetime.now()),
                'samples': len(features),
                'features': len(features.columns),
                'scores': scores
            }
            json.dump(report, f, indent=2)
        
        logger.info(f"ğŸ’¾ ØªÙ… Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙÙŠ {model_dir}")
        
        return scores
    
    def train_all(self):
        """ØªØ¯Ø±ÙŠØ¨ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø¹Ù…Ù„Ø§Øª Ø§Ù„Ù…ØªØ§Ø­Ø©"""
        logger.info("\n" + "="*80)
        logger.info("ğŸš€ Ø¨Ø¯Ø¡ ØªØ¯Ø±ÙŠØ¨ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø¹Ù…Ù„Ø§Øª")
        logger.info("="*80)
        
        # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø¹Ù…Ù„Ø§Øª
        available_data = self.get_all_symbols_from_db()
        
        if available_data.empty:
            logger.error("âŒ Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¨ÙŠØ§Ù†Ø§Øª Ù…ØªØ§Ø­Ø©")
            return
        
        successful = []
        failed = []
        
        # ØªØ¯Ø±ÙŠØ¨ ÙƒÙ„ Ø¹Ù…Ù„Ø©
        for idx, row in available_data.iterrows():
            symbol = row['symbol']
            timeframe = row['timeframe']
            
            try:
                logger.info(f"\nğŸ“Š Ù…Ø¹Ø§Ù„Ø¬Ø© {idx+1}/{len(available_data)}")
                
                scores = self.train_symbol(symbol, timeframe)
                
                if scores:
                    successful.append({
                        'symbol': symbol,
                        'timeframe': timeframe,
                        'accuracy': scores['test_accuracy']
                    })
                else:
                    failed.append(f"{symbol} {timeframe}")
                    
            except Exception as e:
                logger.error(f"âŒ Ø®Ø·Ø£: {e}")
                failed.append(f"{symbol} {timeframe}")
        
        # Ø§Ù„Ù…Ù„Ø®Øµ
        logger.info("\n" + "="*80)
        logger.info("ğŸ“Š Ø§Ù„Ù…Ù„Ø®Øµ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ")
        logger.info("="*80)
        logger.info(f"âœ… Ù†Ø¬Ø­: {len(successful)}")
        logger.info(f"âŒ ÙØ´Ù„: {len(failed)}")
        
        if successful:
            logger.info("\nğŸ† Ø£ÙØ¶Ù„ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬:")
            sorted_models = sorted(successful, key=lambda x: x['accuracy'], reverse=True)[:10]
            for model in sorted_models:
                logger.info(f"  â€¢ {model['symbol']} {model['timeframe']}: {model['accuracy']:.4f}")

def main():
    """ØªØ´ØºÙŠÙ„ Ø§Ù„ØªØ¯Ø±ÙŠØ¨"""
    trainer = SimpleModelTrainer()
    
    # Ù„Ù„Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø³Ø±ÙŠØ¹ - Ø¹Ù…Ù„Ø§Øª Ù…Ø­Ø¯Ø¯Ø©
    test_symbols = [
        ("EURUSD", "M5"),
        ("GBPUSD", "M15"),
        ("XAUUSD", "H1"),
        ("BTCUSD", "H4")
    ]
    
    logger.info("ğŸš€ Ø¨Ø¯Ø¡ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù…Ø¨Ø³Ø·")
    
    # ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ø¹Ù…Ù„Ø§Øª Ø§Ù„Ù…Ø­Ø¯Ø¯Ø©
    for symbol, timeframe in test_symbols:
        try:
            trainer.train_symbol(symbol, timeframe)
        except Exception as e:
            logger.error(f"Ø®Ø·Ø£ ÙÙŠ {symbol} {timeframe}: {e}")
    
    logger.info("\nâœ… Ø§ÙƒØªÙ…Ù„ Ø§Ù„ØªØ¯Ø±ÙŠØ¨!")

if __name__ == "__main__":
    main()