#!/usr/bin/env python3
"""
ğŸš€ Optimized Training System - Ù†Ø¸Ø§Ù… ØªØ¯Ø±ÙŠØ¨ Ù…Ø­Ø³Ù†
âœ¨ ÙŠØ­Ù„ Ø¬Ù…ÙŠØ¹ Ù…Ø´Ø§ÙƒÙ„ Ø§Ù„Ø£Ø¯Ø§Ø¡ ÙˆØ§Ù„Ø°Ø§ÙƒØ±Ø©
ğŸ“Š ÙŠØ­ØªÙØ¸ Ø¨Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø©
"""

import os
import sys
import gc
import json
import sqlite3
import numpy as np
import pandas as pd
from datetime import datetime, timedelta
from pathlib import Path
import joblib
import logging
from typing import Dict, List, Tuple, Optional
import warnings
warnings.filterwarnings('ignore')

# ML Libraries
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
import lightgbm as lgb
import xgboost as xgb

# Technical Analysis
import talib

# Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ù…Ø³Ø§Ø±
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

# Logging setup
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s | %(levelname)-8s | %(message)s'
)
logger = logging.getLogger(__name__)

class OptimizedTrainer:
    """Ù…Ø¯Ø±Ø¨ Ù…Ø­Ø³Ù† Ù„Ù„Ø£Ø¯Ø§Ø¡ Ø§Ù„Ø¹Ø§Ù„ÙŠ"""
    
    def __init__(self):
        """ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù…Ø¯Ø±Ø¨"""
        logger.info("="*100)
        logger.info("ğŸš€ Optimized Training System - Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ù…Ø­Ø³Ù†")
        logger.info("="*100)
        
        # Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø£Ø¯Ø§Ø¡
        self.chunk_size = 10000  # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¹Ù„Ù‰ Ø¯ÙØ¹Ø§Øª
        self.min_data_points = 1000
        self.max_features = 50  # Ø¹Ø¯Ø¯ Ù…Ø­Ø¯ÙˆØ¯ Ù…Ù† Ø§Ù„Ù…ÙŠØ²Ø§Øª Ù„Ù„Ø£Ø¯Ø§Ø¡
        self.use_gpu = False  # ØªØ¹Ø·ÙŠÙ„ GPU Ù„Ù„Ø«Ø¨Ø§Øª
        
        # Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨
        self.training_config = {
            'test_size': 0.2,
            'validation_size': 0.1,
            'random_state': 42,
            'early_stopping_rounds': 50,
            'n_jobs': 2  # Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…ØªÙˆØ§Ø²ÙŠØ© Ù…Ø­Ø¯ÙˆØ¯Ø©
        }
        
        # Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ§Øª Ø§Ù„ØªØ¯Ø§ÙˆÙ„
        self.strategies = {
            'scalping': {
                'description': 'Ø³ÙƒØ§Ù„Ø¨ÙŠÙ†Ø¬ Ø³Ø±ÙŠØ¹',
                'lookahead': 20,
                'min_pips': 5,
                'take_profit_ratios': [1.5, 2.5],
                'stop_loss_atr': 1.0
            },
            'day_trading': {
                'description': 'ØªØ¯Ø§ÙˆÙ„ ÙŠÙˆÙ…ÙŠ',
                'lookahead': 60,
                'min_pips': 15,
                'take_profit_ratios': [2.0, 3.0],
                'stop_loss_atr': 1.5
            },
            'swing_trading': {
                'description': 'ØªØ¯Ø§ÙˆÙ„ Ù…ØªØ£Ø±Ø¬Ø­',
                'lookahead': 240,
                'min_pips': 30,
                'take_profit_ratios': [2.5, 4.0],
                'stop_loss_atr': 2.0
            }
        }
        
        # Ù†Ù…Ø§Ø°Ø¬ ML
        self.models = {
            'lightgbm': {
                'n_estimators': 500,
                'max_depth': 10,
                'learning_rate': 0.05,
                'num_leaves': 31,
                'min_child_samples': 20,
                'subsample': 0.8,
                'colsample_bytree': 0.8,
                'n_jobs': 2,
                'verbose': -1
            },
            'xgboost': {
                'n_estimators': 500,
                'max_depth': 10,
                'learning_rate': 0.05,
                'subsample': 0.8,
                'colsample_bytree': 0.8,
                'n_jobs': 2,
                'verbosity': 0,
                'use_label_encoder': False
            }
        }
        
        logger.info(f"âœ… System initialized with optimized settings")
    
    def load_data_optimized(self, symbol: str, timeframe: str, limit: int = 50000) -> pd.DataFrame:
        """ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ø·Ø±ÙŠÙ‚Ø© Ù…Ø­Ø³Ù†Ø©"""
        logger.info(f"ğŸ“Š Loading data for {symbol} {timeframe}...")
        
        try:
            conn = sqlite3.connect("data/forex_ml.db")
            
            # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¹Ù„Ù‰ Ø¯ÙØ¹Ø§Øª
            query = """
                SELECT time, open, high, low, close, volume
                FROM price_data 
                WHERE symbol = ? AND timeframe = ?
                ORDER BY time DESC
                LIMIT ?
            """
            
            # Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
            df = pd.read_sql_query(query, conn, params=(symbol, timeframe, limit))
            conn.close()
            
            if len(df) < self.min_data_points:
                logger.warning(f"âš ï¸ Insufficient data: {len(df)} records")
                return None
            
            # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
            df['time'] = pd.to_datetime(df['time'], unit='s')
            df = df.sort_values('time')
            df = df.set_index('time')
            
            # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ù…ÙÙ‚ÙˆØ¯Ø© ÙˆØ§Ù„Ù…ÙƒØ±Ø±Ø©
            df = df.dropna()
            df = df[~df.index.duplicated(keep='first')]
            
            # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø¬ÙˆØ¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
            if df['close'].std() == 0:
                logger.error("âŒ No price variation in data")
                return None
            
            logger.info(f"âœ… Loaded {len(df)} records")
            return df
            
        except Exception as e:
            logger.error(f"âŒ Error loading data: {e}")
            return None
    
    def create_features_optimized(self, df: pd.DataFrame) -> pd.DataFrame:
        """Ø¥Ù†Ø´Ø§Ø¡ Ù…ÙŠØ²Ø§Øª Ù…Ø­Ø³Ù†Ø©"""
        logger.info("ğŸ”§ Creating optimized features...")
        
        features = pd.DataFrame(index=df.index)
        
        try:
            # 1. Ù…ÙŠØ²Ø§Øª Ø§Ù„Ø³Ø¹Ø± Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
            features['returns'] = df['close'].pct_change()
            features['log_returns'] = np.log(df['close'] / df['close'].shift(1))
            features['price_range'] = (df['high'] - df['low']) / df['close']
            features['price_position'] = (df['close'] - df['low']) / (df['high'] - df['low'] + 1e-10)
            
            # 2. Ø§Ù„Ù…ØªÙˆØ³Ø·Ø§Øª Ø§Ù„Ù…ØªØ­Ø±ÙƒØ©
            for period in [5, 10, 20, 50]:
                features[f'sma_{period}'] = df['close'].rolling(period).mean()
                features[f'sma_{period}_slope'] = features[f'sma_{period}'].diff()
            
            # 3. Ù…Ø¤Ø´Ø±Ø§Øª RSI
            for period in [7, 14, 21]:
                features[f'rsi_{period}'] = talib.RSI(df['close'].values, timeperiod=period)
            
            # 4. Bollinger Bands
            for period in [20]:
                upper, middle, lower = talib.BBANDS(df['close'].values, timeperiod=period)
                features[f'bb_upper_{period}'] = upper
                features[f'bb_middle_{period}'] = middle
                features[f'bb_lower_{period}'] = lower
                features[f'bb_width_{period}'] = (upper - lower) / middle
                features[f'bb_position_{period}'] = (df['close'] - lower) / (upper - lower + 1e-10)
            
            # 5. MACD
            macd, signal, hist = talib.MACD(df['close'].values)
            features['macd'] = macd
            features['macd_signal'] = signal
            features['macd_hist'] = hist
            
            # 6. ATR Ù„Ù„ØªÙ‚Ù„Ø¨
            features['atr_14'] = talib.ATR(df['high'].values, df['low'].values, df['close'].values, timeperiod=14)
            features['atr_7'] = talib.ATR(df['high'].values, df['low'].values, df['close'].values, timeperiod=7)
            
            # 7. Volume features
            features['volume_sma'] = df['volume'].rolling(20).mean()
            features['volume_ratio'] = df['volume'] / features['volume_sma']
            
            # 8. Pattern Recognition (Ù…Ø­Ø¯ÙˆØ¯)
            features['doji'] = talib.CDLDOJI(df['open'].values, df['high'].values, df['low'].values, df['close'].values)
            features['hammer'] = talib.CDLHAMMER(df['open'].values, df['high'].values, df['low'].values, df['close'].values)
            
            # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ù…ÙÙ‚ÙˆØ¯Ø©
            features = features.fillna(method='ffill').fillna(0)
            
            # Ø§Ø®ØªÙŠØ§Ø± Ø£ÙØ¶Ù„ Ø§Ù„Ù…ÙŠØ²Ø§Øª ÙÙ‚Ø·
            features = features.iloc[:, :self.max_features]
            
            logger.info(f"âœ… Created {len(features.columns)} optimized features")
            return features
            
        except Exception as e:
            logger.error(f"âŒ Error creating features: {e}")
            return pd.DataFrame()
    
    def create_targets_optimized(self, df: pd.DataFrame, strategy: dict) -> Tuple[np.ndarray, np.ndarray]:
        """Ø¥Ù†Ø´Ø§Ø¡ Ø£Ù‡Ø¯Ø§Ù Ù…Ø­Ø³Ù†Ø©"""
        logger.info(f"ğŸ¯ Creating targets for {strategy['description']}...")
        
        lookahead = strategy['lookahead']
        min_pips = strategy['min_pips']
        
        # Ø­Ø³Ø§Ø¨ Ù‚ÙŠÙ…Ø© Ø§Ù„Ù†Ù‚Ø·Ø©
        pip_value = 0.0001 if 'JPY' not in df.index.name else 0.01
        
        targets = []
        confidences = []
        
        # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¹Ù„Ù‰ Ø¯ÙØ¹Ø§Øª Ù„ØªÙˆÙÙŠØ± Ø§Ù„Ø°Ø§ÙƒØ±Ø©
        batch_size = 1000
        total_samples = len(df) - lookahead
        
        for start_idx in range(0, total_samples, batch_size):
            end_idx = min(start_idx + batch_size, total_samples)
            
            for i in range(start_idx, end_idx):
                try:
                    current_price = df['close'].iloc[i]
                    future_prices = df['close'].iloc[i+1:i+lookahead+1].values
                    
                    if len(future_prices) == 0:
                        continue
                    
                    # Ø­Ø³Ø§Ø¨ Ø§Ù„Ø­Ø±ÙƒØ© Ø§Ù„Ù…Ø³ØªÙ‚Ø¨Ù„ÙŠØ©
                    max_up = (future_prices.max() - current_price) / pip_value
                    max_down = (current_price - future_prices.min()) / pip_value
                    
                    # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù‡Ø¯Ù
                    if max_up >= min_pips * 2:
                        targets.append(2)  # Buy
                        confidence = min(0.5 + (max_up / (min_pips * 4)) * 0.5, 1.0)
                        confidences.append(confidence)
                    elif max_down >= min_pips * 2:
                        targets.append(0)  # Sell
                        confidence = min(0.5 + (max_down / (min_pips * 4)) * 0.5, 1.0)
                        confidences.append(confidence)
                    else:
                        targets.append(1)  # Hold
                        confidences.append(0.5)
                        
                except Exception as e:
                    targets.append(1)
                    confidences.append(0.5)
            
            # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø°Ø§ÙƒØ±Ø©
            if start_idx % 5000 == 0:
                gc.collect()
                logger.info(f"  Progress: {start_idx}/{total_samples}")
        
        # Ù…Ù„Ø¡ Ø§Ù„Ø¨Ø§Ù‚ÙŠ
        remaining = len(df) - len(targets)
        targets.extend([1] * remaining)
        confidences.extend([0.5] * remaining)
        
        # Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
        unique, counts = np.unique(targets, return_counts=True)
        stats = dict(zip(unique, counts))
        logger.info(f"âœ… Targets created - Buy: {stats.get(2, 0)}, Sell: {stats.get(0, 0)}, Hold: {stats.get(1, 0)}")
        
        return np.array(targets), np.array(confidences)
    
    def train_model_optimized(self, X_train, y_train, X_val, y_val, model_type='lightgbm'):
        """ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬ Ù…Ø­Ø³Ù†"""
        logger.info(f"ğŸ¤– Training {model_type} model...")
        
        try:
            if model_type == 'lightgbm':
                model = lgb.LGBMClassifier(**self.models['lightgbm'])
                model.fit(
                    X_train, y_train,
                    eval_set=[(X_val, y_val)],
                    callbacks=[lgb.early_stopping(self.training_config['early_stopping_rounds']),
                             lgb.log_evaluation(0)]
                )
            else:  # xgboost
                model = xgb.XGBClassifier(**self.models['xgboost'])
                model.fit(
                    X_train, y_train,
                    eval_set=[(X_val, y_val)],
                    early_stopping_rounds=self.training_config['early_stopping_rounds'],
                    verbose=False
                )
            
            # ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ø£Ø¯Ø§Ø¡
            y_pred = model.predict(X_val)
            accuracy = accuracy_score(y_val, y_pred)
            precision = precision_score(y_val, y_pred, average='weighted', zero_division=0)
            recall = recall_score(y_val, y_pred, average='weighted', zero_division=0)
            f1 = f1_score(y_val, y_pred, average='weighted', zero_division=0)
            
            results = {
                'model': model,
                'accuracy': accuracy,
                'precision': precision,
                'recall': recall,
                'f1': f1
            }
            
            logger.info(f"âœ… Model trained - Accuracy: {accuracy:.4f}, F1: {f1:.4f}")
            return results
            
        except Exception as e:
            logger.error(f"âŒ Error training model: {e}")
            return None
    
    def train_symbol(self, symbol: str, timeframe: str) -> dict:
        """ØªØ¯Ø±ÙŠØ¨ Ø±Ù…Ø² ÙˆØ§Ø­Ø¯"""
        logger.info(f"\n{'='*80}")
        logger.info(f"ğŸ¯ Training {symbol} {timeframe}")
        logger.info(f"{'='*80}")
        
        # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        df = self.load_data_optimized(symbol, timeframe)
        if df is None:
            return None
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…ÙŠØ²Ø§Øª
        features = self.create_features_optimized(df)
        if features.empty:
            return None
        
        # Ø§Ù„Ù†ØªØ§Ø¦Ø¬
        results = {
            'symbol': symbol,
            'timeframe': timeframe,
            'data_points': len(df),
            'strategies': {}
        }
        
        # ØªØ¯Ø±ÙŠØ¨ ÙƒÙ„ Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ©
        for strategy_name, strategy in self.strategies.items():
            try:
                logger.info(f"\nğŸ“Š Strategy: {strategy_name}")
                
                # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ø£Ù‡Ø¯Ø§Ù
                targets, confidences = self.create_targets_optimized(df, strategy)
                
                # ÙÙ„ØªØ±Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¹Ø§Ù„ÙŠØ© Ø§Ù„Ø«Ù‚Ø©
                high_confidence = confidences > 0.6
                X_filtered = features[high_confidence].values
                y_filtered = targets[high_confidence]
                
                if len(X_filtered) < 100:
                    logger.warning(f"âš ï¸ Not enough high confidence samples: {len(X_filtered)}")
                    continue
                
                # ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
                X_train, X_temp, y_train, y_temp = train_test_split(
                    X_filtered, y_filtered, 
                    test_size=0.3, 
                    random_state=42,
                    stratify=y_filtered
                )
                
                X_val, X_test, y_val, y_test = train_test_split(
                    X_temp, y_temp,
                    test_size=0.5,
                    random_state=42,
                    stratify=y_temp
                )
                
                # Ù…Ø¹Ø§ÙŠØ±Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
                scaler = RobustScaler()
                X_train_scaled = scaler.fit_transform(X_train)
                X_val_scaled = scaler.transform(X_val)
                X_test_scaled = scaler.transform(X_test)
                
                # ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
                model_results = self.train_model_optimized(
                    X_train_scaled, y_train,
                    X_val_scaled, y_val,
                    model_type='lightgbm'
                )
                
                if model_results:
                    # ØªÙ‚ÙŠÙŠÙ… Ø¹Ù„Ù‰ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±
                    model = model_results['model']
                    y_pred = model.predict(X_test_scaled)
                    test_accuracy = accuracy_score(y_test, y_pred)
                    
                    # Ø­ÙØ¸ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
                    strategy_results = {
                        'accuracy': test_accuracy,
                        'precision': model_results['precision'],
                        'recall': model_results['recall'],
                        'f1': model_results['f1'],
                        'samples': len(X_filtered)
                    }
                    
                    results['strategies'][strategy_name] = strategy_results
                    
                    # Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¥Ø°Ø§ ÙƒØ§Ù† Ø¬ÙŠØ¯Ø§Ù‹
                    if test_accuracy >= 0.60:
                        self.save_model(model, scaler, symbol, timeframe, strategy_name, strategy_results)
                    
                    logger.info(f"âœ… {strategy_name} completed - Accuracy: {test_accuracy:.4f}")
                
                # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø°Ø§ÙƒØ±Ø©
                gc.collect()
                
            except Exception as e:
                logger.error(f"âŒ Error in {strategy_name}: {e}")
                import traceback
                traceback.print_exc()
        
        # Ø§Ø®ØªÙŠØ§Ø± Ø£ÙØ¶Ù„ Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ©
        if results['strategies']:
            best_strategy = max(results['strategies'].items(), key=lambda x: x[1]['accuracy'])
            results['best_strategy'] = best_strategy[0]
            results['best_accuracy'] = best_strategy[1]['accuracy']
        else:
            results['best_strategy'] = None
            results['best_accuracy'] = 0
        
        return results
    
    def save_model(self, model, scaler, symbol, timeframe, strategy_name, results):
        """Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬"""
        try:
            model_dir = Path(f"models/{symbol}_{timeframe}/{strategy_name}")
            model_dir.mkdir(parents=True, exist_ok=True)
            
            # Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙˆØ§Ù„Ù…Ø¹Ø§ÙŠØ±
            model_data = {
                'model': model,
                'scaler': scaler,
                'results': results,
                'training_date': datetime.now().isoformat(),
                'symbol': symbol,
                'timeframe': timeframe,
                'strategy': strategy_name
            }
            
            model_path = model_dir / 'model_optimized.pkl'
            joblib.dump(model_data, model_path)
            
            # Ø­ÙØ¸ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
            info_path = model_dir / 'model_info.json'
            info_data = {
                'symbol': symbol,
                'timeframe': timeframe,
                'strategy': strategy_name,
                'accuracy': results['accuracy'],
                'precision': results['precision'],
                'recall': results['recall'],
                'f1': results['f1'],
                'samples': results['samples'],
                'training_date': datetime.now().isoformat()
            }
            
            with open(info_path, 'w') as f:
                json.dump(info_data, f, indent=2)
            
            logger.info(f"ğŸ’¾ Model saved to: {model_path}")
            
        except Exception as e:
            logger.error(f"âŒ Error saving model: {e}")

def main():
    """Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©"""
    trainer = OptimizedTrainer()
    
    # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ø±Ù…ÙˆØ² Ù…Ù† Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
    try:
        conn = sqlite3.connect("data/forex_ml.db")
        query = """
            SELECT symbol, timeframe, COUNT(*) as count
            FROM price_data
            GROUP BY symbol, timeframe
            HAVING count >= 5000
            ORDER BY count DESC
            LIMIT 10
        """
        available = pd.read_sql_query(query, conn)
        conn.close()
        
        logger.info(f"\nğŸ“Š Found {len(available)} symbols with sufficient data")
        
        # ØªØ¯Ø±ÙŠØ¨ Ø£ÙØ¶Ù„ 5 Ø±Ù…ÙˆØ²
        results = {}
        for idx, row in available.head(5).iterrows():
            symbol = row['symbol']
            timeframe = row['timeframe']
            
            result = trainer.train_symbol(symbol, timeframe)
            if result:
                results[f"{symbol}_{timeframe}"] = result
        
        # Ø¹Ø±Ø¶ Ø§Ù„Ù…Ù„Ø®Øµ
        logger.info("\n" + "="*80)
        logger.info("ğŸ“Š TRAINING SUMMARY")
        logger.info("="*80)
        
        for key, result in results.items():
            if result['best_strategy']:
                logger.info(f"{key}: {result['best_accuracy']:.2%} ({result['best_strategy']})")
        
    except Exception as e:
        logger.error(f"âŒ Error in main: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()