#!/usr/bin/env python3
"""
Model Validator - Ensures all models follow unified standards
Ù…ÙØ­Ù‚Ù‚ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ - ÙŠØ¶Ù…Ù† Ø§ØªØ¨Ø§Ø¹ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ù„Ù„Ù…Ø¹Ø§ÙŠÙŠØ± Ø§Ù„Ù…ÙˆØ­Ø¯Ø©
"""

import joblib
import numpy as np
from pathlib import Path
from datetime import datetime
from loguru import logger
import json

from unified_standards import (
    STANDARD_FEATURES,
    get_model_filename,
    validate_features
)

class ModelValidator:
    """Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ØµØ­Ø© Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ù‚Ø¨Ù„ Ø§Ù„Ø­ÙØ¸ Ø£Ùˆ Ø§Ù„ØªØ­Ù…ÙŠÙ„"""
    
    def __init__(self):
        self.validation_log = []
        self.models_dir = Path("models/unified")
        
    def validate_model_file(self, filepath):
        """Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ØµØ­Ø© Ù…Ù„Ù Ù†Ù…ÙˆØ°Ø¬"""
        try:
            # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
            model_data = joblib.load(filepath)
            
            # Ø§Ù„ØªØ­Ù‚Ù‚Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
            checks = {
                'has_model': 'model' in model_data,
                'has_scaler': 'scaler' in model_data,
                'has_features': 'feature_names' in model_data or 'n_features' in model_data,
                'has_metrics': 'metrics' in model_data
            }
            
            # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø¹Ø¯Ø¯ Ø§Ù„Ù…ÙŠØ²Ø§Øª
            n_features = None
            if 'n_features' in model_data:
                n_features = model_data['n_features']
            elif 'feature_names' in model_data:
                n_features = len(model_data['feature_names'])
            
            checks['correct_features'] = n_features == STANDARD_FEATURES
            
            # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ù‚Ø¯Ø±Ø© Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ Ø§Ù„ØªÙ†Ø¨Ø¤
            if all(checks.values()):
                try:
                    # Ø§Ø®ØªØ¨Ø§Ø± Ø¨Ø³ÙŠØ·
                    X_test = np.random.randn(1, STANDARD_FEATURES)
                    X_scaled = model_data['scaler'].transform(X_test)
                    prediction = model_data['model'].predict(X_scaled)
                    proba = model_data['model'].predict_proba(X_scaled)
                    checks['can_predict'] = True
                except:
                    checks['can_predict'] = False
            else:
                checks['can_predict'] = False
            
            # Ø§Ù„Ù†ØªÙŠØ¬Ø©
            is_valid = all(checks.values())
            
            result = {
                'filepath': str(filepath),
                'valid': is_valid,
                'checks': checks,
                'n_features': n_features,
                'timestamp': datetime.now().isoformat()
            }
            
            self.validation_log.append(result)
            
            if not is_valid:
                logger.warning(f"âŒ Invalid model: {filepath.name}")
                for check, passed in checks.items():
                    if not passed:
                        logger.warning(f"   Failed: {check}")
            else:
                logger.info(f"âœ… Valid model: {filepath.name}")
            
            return is_valid, result
            
        except Exception as e:
            logger.error(f"Error validating {filepath}: {e}")
            return False, {'error': str(e)}
    
    def fix_model_features(self, filepath, output_path=None):
        """Ø¥ØµÙ„Ø§Ø­ Ù†Ù…ÙˆØ°Ø¬ Ù„Ø§ ÙŠØªÙˆØ§ÙÙ‚ Ù…Ø¹ Ø§Ù„Ù…Ø¹Ø§ÙŠÙŠØ±"""
        try:
            model_data = joblib.load(filepath)
            
            # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø¹Ø¯Ø¯ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ø­Ø§Ù„ÙŠ
            if 'scaler' in model_data:
                current_features = model_data['scaler'].n_features_in_
            else:
                logger.error("No scaler found in model")
                return False
            
            if current_features == STANDARD_FEATURES:
                logger.info("Model already has correct number of features")
                return True
            
            logger.info(f"Fixing model: {current_features} -> {STANDARD_FEATURES} features")
            
            # Ø¥Ù†Ø´Ø§Ø¡ scaler Ø¬Ø¯ÙŠØ¯ ÙŠØªÙˆØ§ÙÙ‚ Ù…Ø¹ Ø§Ù„Ù…Ø¹Ø§ÙŠÙŠØ±
            from sklearn.preprocessing import RobustScaler
            
            if current_features < STANDARD_FEATURES:
                # Ù†Ø­ØªØ§Ø¬ padding
                logger.info(f"Adding {STANDARD_FEATURES - current_features} padding features")
                
                # Ø¥Ù†Ø´Ø§Ø¡ wrapper Ù„Ù„Ù€ scaler
                class PaddedScaler:
                    def __init__(self, original_scaler, target_features):
                        self.original_scaler = original_scaler
                        self.target_features = target_features
                        self.original_features = original_scaler.n_features_in_
                        self.padding_features = target_features - self.original_features
                    
                    def transform(self, X):
                        # ØªØ·Ø¨ÙŠØ¹ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ø£ØµÙ„ÙŠØ©
                        X_scaled = self.original_scaler.transform(X[:, :self.original_features])
                        
                        # Ø¥Ø¶Ø§ÙØ© padding
                        if self.padding_features > 0:
                            padding = np.zeros((X.shape[0], self.padding_features))
                            X_scaled = np.hstack([X_scaled, padding])
                        
                        return X_scaled
                    
                    def fit_transform(self, X, y=None):
                        self.original_scaler.fit(X[:, :self.original_features])
                        return self.transform(X)
                    
                    @property
                    def n_features_in_(self):
                        return self.target_features
                
                # Ø§Ø³ØªØ¨Ø¯Ø§Ù„ Ø§Ù„Ù€ scaler
                model_data['scaler'] = PaddedScaler(model_data['scaler'], STANDARD_FEATURES)
                
            else:
                # Ù†Ø­ØªØ§Ø¬ Ù‚Øµ Ø§Ù„Ù…ÙŠØ²Ø§Øª
                logger.warning(f"Trimming {current_features - STANDARD_FEATURES} extra features")
                logger.warning("This may affect model performance!")
            
            # ØªØ­Ø¯ÙŠØ« metadata
            model_data['n_features'] = STANDARD_FEATURES
            model_data['fixed_by_validator'] = True
            model_data['fix_date'] = datetime.now().isoformat()
            
            # Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…ÙØµÙ„Ø­
            if output_path is None:
                output_path = filepath.parent / f"fixed_{filepath.name}"
            
            joblib.dump(model_data, output_path)
            logger.info(f"âœ… Fixed model saved: {output_path}")
            
            return True
            
        except Exception as e:
            logger.error(f"Error fixing model: {e}")
            return False
    
    def validate_all_models(self):
        """Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ ÙÙŠ Ø§Ù„Ù…Ø¬Ù„Ø¯"""
        logger.info(f"ğŸ” Validating all models in {self.models_dir}")
        
        if not self.models_dir.exists():
            logger.error("Models directory not found")
            return
        
        model_files = list(self.models_dir.glob("*.pkl"))
        logger.info(f"Found {len(model_files)} model files")
        
        valid_count = 0
        invalid_count = 0
        
        for model_file in model_files:
            is_valid, result = self.validate_model_file(model_file)
            if is_valid:
                valid_count += 1
            else:
                invalid_count += 1
        
        # Ø­ÙØ¸ ØªÙ‚Ø±ÙŠØ± Ø§Ù„ØªØ­Ù‚Ù‚
        report = {
            'validation_date': datetime.now().isoformat(),
            'total_models': len(model_files),
            'valid_models': valid_count,
            'invalid_models': invalid_count,
            'details': self.validation_log
        }
        
        report_file = self.models_dir / "validation_report.json"
        with open(report_file, 'w') as f:
            json.dump(report, f, indent=2)
        
        logger.info(f"\nğŸ“Š Validation Summary:")
        logger.info(f"   Total: {len(model_files)}")
        logger.info(f"   âœ… Valid: {valid_count}")
        logger.info(f"   âŒ Invalid: {invalid_count}")
        logger.info(f"   Report saved: {report_file}")
        
        return report
    
    def create_compatibility_wrapper(self):
        """Ø¥Ù†Ø´Ø§Ø¡ wrapper Ù„Ù„ØªÙˆØ§ÙÙ‚ Ù…Ø¹ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø©"""
        wrapper_code = '''#!/usr/bin/env python3
"""
Compatibility Wrapper for Legacy Models
ØºÙ„Ø§Ù Ø§Ù„ØªÙˆØ§ÙÙ‚ Ù„Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø©
"""

import joblib
import numpy as np
from pathlib import Path

class CompatibilityWrapper:
    """ÙŠØ³Ù…Ø­ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø© Ù…Ø¹ Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ø¬Ø¯ÙŠØ¯"""
    
    def __init__(self, model_path):
        self.model_data = joblib.load(model_path)
        self.model = self.model_data['model']
        self.scaler = self.model_data['scaler']
        
        # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø¹Ø¯Ø¯ Ø§Ù„Ù…ÙŠØ²Ø§Øª
        if hasattr(self.scaler, 'n_features_in_'):
            self.original_features = self.scaler.n_features_in_
        else:
            self.original_features = 49  # Ø§ÙØªØ±Ø§Ø¶ÙŠ
        
        self.target_features = 70
    
    def predict(self, X):
        """Ø§Ù„ØªÙ†Ø¨Ø¤ Ù…Ø¹ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ø®ØªÙ„Ø§Ù Ø¹Ø¯Ø¯ Ø§Ù„Ù…ÙŠØ²Ø§Øª"""
        # Ø¶Ø¨Ø· Ø¹Ø¯Ø¯ Ø§Ù„Ù…ÙŠØ²Ø§Øª
        if X.shape[1] < self.original_features:
            # Ø¥Ø¶Ø§ÙØ© padding Ù„Ù„ÙˆØµÙˆÙ„ Ù„Ù„Ø¹Ø¯Ø¯ Ø§Ù„Ø£ØµÙ„ÙŠ
            padding = np.zeros((X.shape[0], self.original_features - X.shape[1]))
            X = np.hstack([X, padding])
        elif X.shape[1] > self.original_features:
            # Ù‚Øµ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ø²Ø§Ø¦Ø¯Ø©
            X = X[:, :self.original_features]
        
        # ØªØ·Ø¨ÙŠØ¹ ÙˆØªÙ†Ø¨Ø¤
        X_scaled = self.scaler.transform(X)
        return self.model.predict(X_scaled)
    
    def predict_proba(self, X):
        """Ø§Ø­ØªÙ…Ø§Ù„ÙŠØ© Ø§Ù„ØªÙ†Ø¨Ø¤ Ù…Ø¹ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ø®ØªÙ„Ø§Ù Ø¹Ø¯Ø¯ Ø§Ù„Ù…ÙŠØ²Ø§Øª"""
        # Ù†ÙØ³ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©
        if X.shape[1] < self.original_features:
            padding = np.zeros((X.shape[0], self.original_features - X.shape[1]))
            X = np.hstack([X, padding])
        elif X.shape[1] > self.original_features:
            X = X[:, :self.original_features]
        
        X_scaled = self.scaler.transform(X)
        return self.model.predict_proba(X_scaled)

# Ø§Ø³ØªØ®Ø¯Ø§Ù…
# wrapper = CompatibilityWrapper('path/to/old/model.pkl')
# predictions = wrapper.predict(X_with_70_features)
'''
        
        wrapper_file = Path("compatibility_wrapper.py")
        with open(wrapper_file, 'w') as f:
            f.write(wrapper_code)
        
        logger.info(f"âœ… Created compatibility wrapper: {wrapper_file}")

if __name__ == "__main__":
    validator = ModelValidator()
    
    # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬
    validator.validate_all_models()
    
    # Ø¥Ù†Ø´Ø§Ø¡ wrapper Ù„Ù„ØªÙˆØ§ÙÙ‚
    validator.create_compatibility_wrapper()