#!/usr/bin/env python3
"""
üîÑ Direct Data Merger - ÿØŸÖÿ¨ ŸÖÿ®ÿßÿ¥ÿ± ŸÑŸÑÿ®ŸäÿßŸÜÿßÿ™
üìä ŸäŸÜÿ≥ÿÆ ÿßŸÑÿ®ŸäÿßŸÜÿßÿ™ ŸÖŸÜ ÿßŸÑÿ¨ÿØÿßŸàŸÑ ÿßŸÑŸÖÿπÿßŸÑÿ¨ÿ© ŸÖÿ®ÿßÿ¥ÿ±ÿ©
"""

import sqlite3
import pandas as pd
import logging
from datetime import datetime

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s'
)
logger = logging.getLogger(__name__)

def merge_direct_data():
    """ÿØŸÖÿ¨ ÿßŸÑÿ®ŸäÿßŸÜÿßÿ™ ÿßŸÑŸÖÿ®ÿßÿ¥ÿ±ÿ© ŸÖŸÜ ÿßŸÑÿ¨ÿØÿßŸàŸÑ ÿßŸÑŸÖÿπÿßŸÑÿ¨ÿ©"""
    db_path = './data/forex_ml.db'
    
    logger.info("="*60)
    logger.info("üîÑ Direct Data Merger")
    logger.info("="*60)
    
    try:
        conn = sqlite3.connect(db_path)
        cursor = conn.cursor()
        
        # 1. ÿ≠ÿ∞ŸÅ ÿ¨ÿØŸàŸÑ price_data ÿßŸÑŸÇÿØŸäŸÖ ÿ•ÿ∞ÿß ŸÉÿßŸÜ ŸÖŸàÿ¨ŸàÿØÿßŸã
        logger.info("üóëÔ∏è Dropping old price_data table if exists...")
        cursor.execute("DROP TABLE IF EXISTS price_data")
        
        # 2. ÿ•ŸÜÿ¥ÿßÿ° ÿ¨ÿØŸàŸÑ price_data ÿ¨ÿØŸäÿØ
        logger.info("üìä Creating new price_data table...")
        cursor.execute("""
        CREATE TABLE price_data (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            symbol TEXT NOT NULL,
            timeframe TEXT NOT NULL,
            time DATETIME NOT NULL,
            open REAL NOT NULL,
            high REAL NOT NULL,
            low REAL NOT NULL,
            close REAL NOT NULL,
            volume REAL DEFAULT 0,
            spread INTEGER DEFAULT 0,
            UNIQUE(symbol, timeframe, time)
        )
        """)
        
        # 3. ÿ¨ŸÑÿ® ŸÇÿßÿ¶ŸÖÿ© ÿßŸÑÿ¨ÿØÿßŸàŸÑ ÿßŸÑŸÖÿπÿßŸÑÿ¨ÿ©
        cursor.execute("""
        SELECT name FROM sqlite_master 
        WHERE type='table' AND name LIKE '%_processed'
        """)
        processed_tables = cursor.fetchall()
        
        logger.info(f"\nüìÅ Found {len(processed_tables)} processed tables")
        
        total_records = 0
        
        # 4. ŸÜÿ≥ÿÆ ÿßŸÑÿ®ŸäÿßŸÜÿßÿ™ ŸÖŸÜ ŸÉŸÑ ÿ¨ÿØŸàŸÑ
        for table_tuple in processed_tables:
            table_name = table_tuple[0]
            logger.info(f"\nüîÑ Processing {table_name}...")
            
            try:
                # ŸÜÿ≥ÿÆ ÿßŸÑÿ®ŸäÿßŸÜÿßÿ™ ŸÖÿ®ÿßÿ¥ÿ±ÿ© ŸÖÿπ ÿ™ÿ≠ŸàŸäŸÑ ÿßŸÑŸàŸÇÿ™
                insert_query = f"""
                INSERT INTO price_data (symbol, timeframe, time, open, high, low, close, volume, spread)
                SELECT 
                    symbol,
                    CASE 
                        WHEN timeframe = 'PERIOD_M5' THEN 'M5'
                        WHEN timeframe = 'PERIOD_M15' THEN 'M15'
                        WHEN timeframe = 'PERIOD_M30' THEN 'M30'
                        WHEN timeframe = 'PERIOD_H1' THEN 'H1'
                        WHEN timeframe = 'PERIOD_H4' THEN 'H4'
                        WHEN timeframe = 'PERIOD_D1' THEN 'D1'
                        ELSE timeframe
                    END as timeframe,
                    datetime(time, 'unixepoch') as time,
                    open, high, low, close, volume, spread
                FROM {table_name}
                WHERE open IS NOT NULL AND high IS NOT NULL AND low IS NOT NULL AND close IS NOT NULL
                """
                
                cursor.execute(insert_query)
                records_added = cursor.rowcount
                
                if records_added > 0:
                    logger.info(f"   ‚úÖ Added {records_added:,} records")
                    total_records += records_added
                else:
                    logger.warning(f"   ‚ö†Ô∏è No valid records found")
                
            except Exception as e:
                logger.error(f"   ‚ùå Error: {e}")
                # ŸÖÿ≠ÿßŸàŸÑÿ© ÿ®ÿØŸäŸÑÿ© - ŸÜÿ≥ÿÆ ÿ≥ÿ¨ŸÑ ÿ®ÿ≥ÿ¨ŸÑ
                try:
                    logger.info(f"   üîÑ Trying record-by-record copy...")
                    cursor.execute(f"SELECT * FROM {table_name} LIMIT 5")
                    sample = cursor.fetchall()
                    
                    if sample:
                        # ÿ™ÿ≠ÿØŸäÿØ ŸÖŸàÿßŸÇÿπ ÿßŸÑÿ£ÿπŸÖÿØÿ©
                        cursor.execute(f"PRAGMA table_info({table_name})")
                        columns = cursor.fetchall()
                        col_names = [col[1] for col in columns]
                        
                        # ÿßŸÑÿ®ÿ≠ÿ´ ÿπŸÜ ŸÖŸàÿßŸÇÿπ ÿßŸÑÿ£ÿπŸÖÿØÿ© ÿßŸÑŸÖÿ∑ŸÑŸàÿ®ÿ©
                        idx_map = {}
                        for i, name in enumerate(col_names):
                            idx_map[name] = i
                        
                        # ŸÜÿ≥ÿÆ ÿßŸÑÿ®ŸäÿßŸÜÿßÿ™
                        cursor.execute(f"SELECT * FROM {table_name}")
                        rows = cursor.fetchall()
                        
                        records_added = 0
                        for row in rows:
                            try:
                                symbol = row[idx_map.get('symbol', 2)]
                                timeframe = row[idx_map.get('timeframe', 3)]
                                time_val = row[idx_map.get('time', 0)]
                                open_val = row[idx_map.get('open', 4)]
                                high_val = row[idx_map.get('high', 5)]
                                low_val = row[idx_map.get('low', 6)]
                                close_val = row[idx_map.get('close', 7)]
                                volume = row[idx_map.get('volume', 8)] if 'volume' in idx_map else 0
                                spread = row[idx_map.get('spread', 9)] if 'spread' in idx_map else 0
                                
                                # ÿ™ÿ≠ŸàŸäŸÑ ÿßŸÑÿ•ÿ∑ÿßÿ± ÿßŸÑÿ≤ŸÖŸÜŸä
                                tf_map = {
                                    'PERIOD_M5': 'M5',
                                    'PERIOD_M15': 'M15',
                                    'PERIOD_M30': 'M30',
                                    'PERIOD_H1': 'H1',
                                    'PERIOD_H4': 'H4',
                                    'PERIOD_D1': 'D1'
                                }
                                timeframe = tf_map.get(timeframe, timeframe)
                                
                                # ÿ™ÿ≠ŸàŸäŸÑ ÿßŸÑŸàŸÇÿ™
                                if isinstance(time_val, (int, float)):
                                    time_str = datetime.fromtimestamp(time_val).strftime('%Y-%m-%d %H:%M:%S')
                                else:
                                    time_str = str(time_val)
                                
                                # ÿ•ÿØÿ±ÿßÿ¨ ÿßŸÑÿ≥ÿ¨ŸÑ
                                cursor.execute("""
                                INSERT OR IGNORE INTO price_data 
                                (symbol, timeframe, time, open, high, low, close, volume, spread)
                                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
                                """, (symbol, timeframe, time_str, open_val, high_val, 
                                      low_val, close_val, volume, spread))
                                
                                records_added += 1
                                
                            except Exception as row_error:
                                continue
                        
                        if records_added > 0:
                            logger.info(f"   ‚úÖ Added {records_added:,} records (record-by-record)")
                            total_records += records_added
                        
                except Exception as e2:
                    logger.error(f"   ‚ùå Alternative method also failed: {e2}")
            
            # ÿ≠ŸÅÿ∏ ÿßŸÑÿ™ŸÇÿØŸÖ
            conn.commit()
        
        # 5. ÿ•ŸÜÿ¥ÿßÿ° ÿßŸÑŸÅŸáÿßÿ±ÿ≥
        logger.info("\nüîß Creating indexes...")
        cursor.execute("CREATE INDEX IF NOT EXISTS idx_symbol_timeframe ON price_data(symbol, timeframe)")
        cursor.execute("CREATE INDEX IF NOT EXISTS idx_time ON price_data(time)")
        
        # 6. ÿßŸÑÿ•ÿ≠ÿµÿßÿ¶Ÿäÿßÿ™ ÿßŸÑŸÜŸáÿßÿ¶Ÿäÿ©
        logger.info("\n" + "="*60)
        logger.info("üìä Final Statistics:")
        
        cursor.execute("SELECT COUNT(*) FROM price_data")
        final_count = cursor.fetchone()[0]
        logger.info(f"   Total records: {final_count:,}")
        
        # ÿßŸÑÿ®ŸäÿßŸÜÿßÿ™ ÿ≠ÿ≥ÿ® ÿßŸÑÿ•ÿ∑ÿßÿ± ÿßŸÑÿ≤ŸÖŸÜŸä
        logger.info("\nüìà Data by timeframe:")
        cursor.execute("""
        SELECT timeframe, COUNT(*) as count, COUNT(DISTINCT symbol) as symbols
        FROM price_data
        GROUP BY timeframe
        ORDER BY timeframe
        """)
        
        for tf, count, symbols in cursor.fetchall():
            logger.info(f"   {tf}: {count:,} records ({symbols} symbols)")
        
        # ÿßŸÑÿ£ÿ≤Ÿàÿßÿ¨ ÿßŸÑÿ¨ÿßŸáÿ≤ÿ© ŸÑŸÑÿ™ÿØÿ±Ÿäÿ®
        logger.info("\nüéØ Training-ready pairs (M15 >2000):")
        cursor.execute("""
        SELECT symbol, COUNT(*) as count
        FROM price_data
        WHERE timeframe = 'M15'
        GROUP BY symbol
        HAVING count > 2000
        ORDER BY count DESC
        """)
        
        ready_pairs = cursor.fetchall()
        if ready_pairs:
            for symbol, count in ready_pairs:
                logger.info(f"   ‚úÖ {symbol}: {count:,} candles")
            logger.info(f"\n‚úÖ {len(ready_pairs)} pairs ready for training!")
            logger.info("\nüöÄ You can now run: python3 train_all_pairs_enhanced.py")
        else:
            logger.info("   ‚ö†Ô∏è No pairs with sufficient M15 data")
            
            # ÿπÿ±ÿ∂ ŸÖÿß ŸáŸà ŸÖÿ™ÿßÿ≠
            cursor.execute("""
            SELECT symbol, COUNT(*) as count
            FROM price_data
            WHERE timeframe = 'M15'
            GROUP BY symbol
            ORDER BY count DESC
            LIMIT 10
            """)
            
            available = cursor.fetchall()
            if available:
                logger.info("\nüìä Available M15 data:")
                for symbol, count in available:
                    status = "‚úÖ" if count > 2000 else "‚ö†Ô∏è"
                    logger.info(f"   {status} {symbol}: {count:,} candles (need 2000+)")
        
        conn.close()
        
        logger.info("\n‚úÖ Data merge completed!")
        
    except Exception as e:
        logger.error(f"‚ùå Fatal error: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    merge_direct_data()